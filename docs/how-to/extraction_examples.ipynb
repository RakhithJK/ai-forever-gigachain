{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "70403d4f-50c1-43f8-a7ea-a211167649a5",
            "metadata": {},
            "source": [
                "# Извлечение данных с помощью образов\n",
                "\n",
                "Качество извлечения информации можно повысить если дать модели образцы данных, которые требуется получить.\n",
                "\n",
                "Извлечение данных - это попытка создать структурированное представление информации, содержащейся в тексте и других неструктурированных или полуструктурированных источниках. Для решения этой здачи часто используется [работа с функциями](/docs/concepts#functiontool-calling) LLM. В этом руководстве показано, как создать few-shot примеры вызова инструментов, чтобы помочь управлять поведением извлечения и подобных приложений.\n",
                "\n",
                ":::note\n",
                "\n",
                "В этом разделе показано как использовать примеры с моделями GigaChat, которые поддерживают работу с инструментами / функциями.\n",
                "Но вы также можете использовать показанный подход в других методиках: генерации JSON или работе с промптами.\n",
                "\n",
                ":::\n",
                "\n",
                "GigaChain реализует атрибут [tool-call](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage.tool_calls) для сообщений от LLM, включающих вызовы функций. Подробнее — в разделе [Вызов инструментов с помощью модели](/docs/how_to/tool_calling).\n",
                "Для создания образцов извлекаемых данных создается история чата, которая содержит последовательность из сообщений:\n",
                "\n",
                "- [HumanMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html), с примерами входных данных;\n",
                "- [AIMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html), с примерами вызова инструментов;\n",
                "- [ToolMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.tool.ToolMessage.html), с примерами выводов инструментов.\n",
                "\n",
                "Такая последовательность используется в GigaChain для единообразия при работе с разными LLM.\n",
                "\n",
                "Сначала создадим шаблон подсказки с объектом `MessagesPlaceholder`, в котором будут передоваться сообщения:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "89579144-bcb3-490a-8036-86a0a6bcd56b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Optional\n",
                "\n",
                "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
                "from langchain_core.pydantic_v1 import BaseModel, Field\n",
                "\n",
                "# Определяем промпт: добавляем инструкции и дополнительный контекст\n",
                "# На этом этапе можно:\n",
                "# * Добавить примеры работы функций, для улучшения качества извлечения информации\n",
                "# * Предоставить дополнительную информацию о том какие данные и откуда будут извлекаться\n",
                "prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\n",
                "            \"system\",\n",
                "            \"Ты эксперт в извлечении информации из текста. \"\n",
                "            \"Извлекай только релевантную информацию из текста. \"\n",
                "            \"Если ты не знаешь значение аттрибута, \"\n",
                "            \"который нужно извлечь, поставь аттрибуту null.\",\n",
                "        ),\n",
                "        MessagesPlaceholder(\"examples\"),  # <---- Примеры работы\n",
                "        (\"human\", \"{text}\"),\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2484008c-ba1a-42a5-87a1-628a900de7fd",
            "metadata": {},
            "source": [
                "Проверка шаблона."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "610c3025-ea63-4cd7-88bd-c8cbcb4d8a3f",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ChatPromptValue(messages=[SystemMessage(content='Ты эксперт в извлечении информации из текста. Извлекай только релевантную информацию из текста. Если ты не знаешь значение аттрибута, который нужно извлечь, поставь аттрибуту null.'), HumanMessage(content='testing 1 2 3'), HumanMessage(content='this is some text')])"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_core.messages import (\n",
                "    HumanMessage,\n",
                ")\n",
                "\n",
                "prompt.invoke(\n",
                "    {\"text\": \"this is some text\", \"examples\": [HumanMessage(content=\"testing 1 2 3\")]}\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "368abd80-0cf0-41a7-8224-acf90dd6830d",
            "metadata": {
                "vscode": {
                    "languageId": "plaintext"
                }
            },
            "source": [
                "## Определение схемы\n",
                "\n",
                "Используем схему данных о человеке, определенную в разделе [Разработка цепочки для извлечения данных](/docs/tutorials/extraction)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "d875a49a-d2cb-4b9e-b5bf-41073bc3905c",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List, Optional\n",
                "\n",
                "from langchain_core.pydantic_v1 import BaseModel, Field\n",
                "\n",
                "\n",
                "class Person(BaseModel):\n",
                "    \"\"\"Информация о человеке.\"\"\"\n",
                "\n",
                "    # Док-строка выше, передается в описании функции\n",
                "    # и помогает улучшить результаты работы LLM\n",
                "\n",
                "    # Обратите внимание:\n",
                "    # * Все поля — необязательные (`optional`). Это позволяет модели не извлекать неописанные поля.\n",
                "    # * У каждого поля есть описание (`description`), которое передается в модель, в описании аргументов функции.\n",
                "    # Хорошее пописание помогает повысить качество извлечения.\n",
                "    name: Optional[str] = Field(..., description=\"Имя человека\")\n",
                "    hair_color: Optional[str] = Field(\n",
                "        ..., description=\"Цвет волос человека, заполни если известен\"\n",
                "    )\n",
                "    height_in_meters: Optional[float] = Field(\n",
                "        ..., description=\"Высота человека в метрах.\"\n",
                "    )\n",
                "\n",
                "\n",
                "class Data(BaseModel):\n",
                "    \"\"\"Информация о людях.\"\"\"\n",
                "\n",
                "    # Создание модели, для извлечения информации о нескольких людях\n",
                "    people: List[Person]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "96c42162-e4f6-4461-88fd-c76f5aab7e32",
            "metadata": {},
            "source": [
                "## Определение образцов\n",
                "\n",
                "Образцы можно определить в виде списка пар значений ввода-вывода.\n",
                "\n",
                "Каждая пара содержит пример входного текста (`input`) и пример вывода (`output`), демонстрирующий, что нужно извлечьиз текста.\n",
                "\n",
                ":::important\n",
                "\n",
                "Формат примеров долже соответствовать используемой методике извлечения информации: работа с инструментами/функциями, генерация JSON и другими.\n",
                "\n",
                "Приведенные примеры соответствуют формату, который ожидается при вызове инструментов.\n",
                "\n",
                ":::"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "08356810-77ce-4e68-99d9-faa0326f2cee",
            "metadata": {},
            "outputs": [],
            "source": [
                "import itertools\n",
                "from typing import List, TypedDict\n",
                "\n",
                "from langchain_core.messages import (\n",
                "    AIMessage,\n",
                "    BaseMessage,\n",
                "    FunctionMessage,\n",
                "    HumanMessage,\n",
                ")\n",
                "from langchain_core.pydantic_v1 import BaseModel\n",
                "\n",
                "\n",
                "class Example(TypedDict):\n",
                "    \"\"\"Пример работы функций.\"\"\"\n",
                "\n",
                "    input: str  # Пример вызова\n",
                "    function_calls: List[BaseModel]  # Pydantic модель, с примером извлечения\n",
                "    function_outputs: List[str]\n",
                "\n",
                "\n",
                "def tool_example_to_messages(example: Example) -> List[BaseMessage]:\n",
                "    \"\"\"Превращаем примеры вызовов функций в историю сообщений\"\"\"\n",
                "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
                "    for function_call, function_output in itertools.zip_longest(\n",
                "        example[\"function_calls\"], example.get(\"function_outputs\", [])\n",
                "    ):\n",
                "        messages.append(\n",
                "            AIMessage(\n",
                "                content=\"\",\n",
                "                additional_kwargs={\n",
                "                    \"function_call\": {\n",
                "                        # Сейчас название модели соответствует pydantic-модели\n",
                "                        # В текущий момент в API это неочевидно и будет улучшено.\n",
                "                        \"name\": function_call.__class__.__name__,\n",
                "                        \"arguments\": function_call.dict(),\n",
                "                    },\n",
                "                },\n",
                "            )\n",
                "        )\n",
                "        output = \"You have correctly called this tool.\"\n",
                "        if function_output:\n",
                "            output = function_output\n",
                "        messages.append(\n",
                "            FunctionMessage(name=function_call.__class__.__name__, content=output)\n",
                "        )\n",
                "    return messages"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "463aa282-51c4-42bf-9463-6ca3b2c08de6",
            "metadata": {},
            "source": [
                "Определение образцов и преобразование их в формат сообщений."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "7f59a745-5c81-4011-a4c5-a33ec1eca7ef",
            "metadata": {},
            "outputs": [],
            "source": [
                "examples = [\n",
                "    (\n",
                "        \"Океан огромный и синий. Глубина более 20 000 футов. В нем много рыбы.\",\n",
                "        Data(people=[]),\n",
                "    ),\n",
                "    (\n",
                "        \"Фиона отправилась в путешествие из Испании во Францию\",\n",
                "        Data(people=[Person(name=\"Фиона\", height_in_meters=None, hair_color=None)]),\n",
                "    ),\n",
                "]\n",
                "\n",
                "\n",
                "messages = []\n",
                "\n",
                "for text, function_call in examples:\n",
                "    messages.extend(\n",
                "        tool_example_to_messages({\"input\": text, \"function_calls\": [function_call]})\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6fdbda30-e7e3-46b5-a54a-1769c580af93",
            "metadata": {},
            "source": [
                "Вызов промпта."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "e61fa3a5-3d15-46a2-a23b-788f9a3ede52",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ChatPromptValue(messages=[SystemMessage(content='Ты эксперт в извлечении информации из текста. Извлекай только релевантную информацию из текста. Если ты не знаешь значение аттрибута, который нужно извлечь, поставь аттрибуту null.'), HumanMessage(content='Океан огромный и синий. Глубина более 20 000 футов. В нем много рыбы.'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'Data', 'arguments': {'people': []}}}), FunctionMessage(content='You have correctly called this tool.', name='Data'), HumanMessage(content='Фиона отправилась в путешествие из Испании во Францию'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'Data', 'arguments': {'people': [{'name': 'Фиона', 'hair_color': None, 'height_in_meters': None}]}}}), FunctionMessage(content='You have correctly called this tool.', name='Data'), HumanMessage(content='this is some text')])"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "example_prompt = prompt.invoke({\"text\": \"this is some text\", \"examples\": messages})\n",
                "\n",
                "for message in example_prompt.messages:\n",
                "    print(f\"{message.type}: {message}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "47b0bbef-bc6b-4535-a8e2-5c84f09d5637",
            "metadata": {},
            "source": [
                "## Создание экстрактора\n",
                "\n",
                "Пример экстрактора, созданного с помощью GigaChat-Pro."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "dbfea43d-769b-42e9-a76f-ce722f7d6f93",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.chat_models.gigachat import GigaChat\n",
                "\n",
                "llm = GigaChat(\n",
                "    verify_ssl_certs=False,\n",
                "    timeout=6000,\n",
                "    model=\"GigaChat-Pro\",\n",
                "    temperature=0.01,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef21e8cb-c4df-4e12-9be7-37ac9d291d42",
            "metadata": {},
            "source": [
                "Вызов метода `.with_structured_output` для структурирования вывода модели в соответствии с заданной схемой:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "dbfea43d-769b-42e9-a76f-ce722f7d6f93",
            "metadata": {},
            "outputs": [],
            "source": [
                "runnable = prompt | llm.with_structured_output(\n",
                "    schema=Data,\n",
                "    include_raw=False,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "58a8139e-f201-4b8e-baf0-16a83e5fa987",
            "metadata": {},
            "source": [
                "## Без образцов\n",
                "\n",
                "Пример работы модели без образцов."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "8b1d6273-5ec5-4970-af8a-0da1f1efa293",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[Person(name='Земля', hair_color='синий', height_in_meters=None)]\n"
                    ]
                }
            ],
            "source": [
                "for _ in range(5):\n",
                "    text = \"Если бы у Земли были бы волосы, они были бы синими\"\n",
                "    print(runnable.invoke({\"text\": text, \"examples\": []}))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "09840f17-ab26-4ea2-8a39-c747103804ec",
            "metadata": {},
            "source": [
                "## С образцами\n",
                "\n",
                "Образцы заметно повышают качество извлечения информации."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "9bdfa49e-0005-4c06-9598-2adfd882b014",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "people=[]\n"
                    ]
                }
            ],
            "source": [
                "for _ in range(5):\n",
                "    text = \"Если бы у Земли были бы волосы, они были бы синими\"\n",
                "    print(runnable.invoke({\"text\": text, \"examples\": messages}))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3855cad5-dfee-4b42-ad35-b28d4d98902e",
            "metadata": {},
            "source": [
                "<!--\n",
                "Note that we can see the few-shot examples as tool-calls in the [Langsmith trace](https://smith.langchain.com/public/4c436bc2-a1ce-440b-82f5-093947542e40/r).\n",
                "-->\n",
                "\n",
                "Сохранение работоспособности при подходящем примере:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "84413e17-608d-4f85-b70e-00b89b271927",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Giga generation stopped with reason: function_call\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Data(people=[Person(name='Джо', hair_color='синий', height_in_meters=None)])"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "runnable.invoke(\n",
                "    {\n",
                "        \"text\": \"Моё имя Джо, мои волосы синие\",\n",
                "        \"examples\": messages,\n",
                "    }\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}