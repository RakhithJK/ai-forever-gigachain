{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fbc4bf6e",
            "metadata": {},
            "source": [
                "# Запуск собственных функций\n",
                "\n",
                ":::note\n",
                "\n",
                "С этим руководством будет проще работать, если ознакомиться с разделами:\n",
                "\n",
                "- [LangChain Expression Language (LCEL)](/docs/concepts/#langchain-expression-language)\n",
                "- [Соединение Runnable в цепочку](/docs/how_to/sequence/)\n",
                "\n",
                ":::\n",
                "\n",
                "Вы можете использовать произвольные функции как [Runnable](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable).\n",
                "Это полезно для форматирования или для реализации функциональности, которой нет в других компонентах GigaChain.\n",
                "Собственные функции, которые используются как Runnable-объекты, называются [`RunnableLambdas`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html).\n",
                "\n",
                "\n",
                ":::note\n",
                "\n",
                "При работе с такими функциями все входные данные для них должны быть представлены одним аргументом. В случае если ваша функция, принимает несколько аргументов, используйте обертку, которая принимает один dict и распаковывает его в несколько аргументов.\n",
                "\n",
                ":::\n",
                "\n",
                "В этом руководстве вы узнаете:\n",
                "\n",
                "* как явно создать Runnable из своей функции с помощью конструктора `RunnableLambda` и декоратора `@chain`;\n",
                "* как принудительно преобразовать свою функций в Runnable при использовании в цепочках;\n",
                "* как принимать и использовать метаданные выполнения в своей функции;\n",
                "* как возвращая генераторы работать с потоковой генерацией в своих функциях.\n",
                "\n",
                "## Использование конструктора\n",
                "\n",
                "Пример ниже показывает как явно создать обертку для собственной логики с помощью конструктора `RunnableLambda`:"
            ]
        },
        {
            "cell_type": "raw",
            "id": "9a5fe916",
            "metadata": {
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "%pip install --upgrade --quiet gigachain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "6bb221b3",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "AIMessage(content='3 + 9 equals 12.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-73728de3-e483-49e3-ad54-51bd9570e71a-0')"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from operator import itemgetter\n",
                "\n",
                "from langchain_community.chat_models.gigachat import GigaChat\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.runnables import RunnableLambda\n",
                "from langchain_opnai.chat_models import ChatOpenAI\n",
                "\n",
                "\n",
                "def length_function(text):\n",
                "    return len(text)\n",
                "\n",
                "\n",
                "def _multiple_length_function(text1, text2):\n",
                "    return len(text1) * len(text2)\n",
                "\n",
                "\n",
                "def multiple_length_function(_dict):\n",
                "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
                "\n",
                "\n",
                "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
                "model = GigaChat(credentials=\"авторизационные_данные\", verify_ssl_certs=False)\n",
                "\n",
                "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
                "\n",
                "chain1 = prompt | model\n",
                "\n",
                "chain = (\n",
                "    {\n",
                "        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
                "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}\n",
                "        | RunnableLambda(multiple_length_function),\n",
                "    }\n",
                "    | prompt\n",
                "    | model\n",
                ")\n",
                "\n",
                "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "063d72ea",
            "metadata": {},
            "source": [
                ":::note\n",
                "\n",
                "Авторизационные данные — строка, полученная в результате кодирования в Base64 клиентского идентификатора (Client ID) и ключа (Client Secret) API. Вы можете использовать готовые данные из личного кабинета или самостоятельно закодировать идентификатор и ключ.\n",
                "\n",
                "Пример строки авторизационных данных:\n",
                "\n",
                "```text\n",
                "MjIzODA0YTktMDU3OC00MTZmLWI4MWYtYzUwNjg3Njk4MzMzOjljMTI2MGQyLTFkNTEtNGRkOS05ZGVhLTBhNjAzZTdjZjQ3Mw==\n",
                "```\n",
                "\n",
                "Идентификатор, ключ и авторизационные данные вы можете получить после создания проекта GigaChat API:\n",
                "\n",
                "* [для физических лиц](https://developers.sber.ru/docs/ru/gigachat/individuals-quickstart#shag-1-sozdayte-proekt-giga-chat-api);\n",
                "* [для ИП и юридических лиц](https://developers.sber.ru/docs/ru/gigachat/legal-quickstart#shag-1-otpravte-zayavku-na-dostup-k-proektu-giga-chat-api).\n",
                "\n",
                ":::"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b7926002",
            "metadata": {},
            "source": [
                "## Использование декоратора `@chain`\n",
                "\n",
                "С помощью декоратор `@chain` вы можете преобразовать произвольную функцию в цепочку.\n",
                "Действие декоратора эквивалентно созданию обертки для функции с помощью конструктор `RunnableLambda`, как показано в примере выше.\n",
                "Пример:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "3142a516",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'The subject of the joke is the bear and his girlfriend.'"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.runnables import chain\n",
                "\n",
                "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
                "prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n",
                "\n",
                "\n",
                "@chain\n",
                "def custom_chain(text):\n",
                "    prompt_val1 = prompt1.invoke({\"topic\": text})\n",
                "    output1 = GigaChat(\n",
                "        credentials=\"авторизационные_данные\", verify_ssl_certs=False\n",
                "    ).invoke(prompt_val1)\n",
                "    parsed_output1 = StrOutputParser().invoke(output1)\n",
                "    chain2 = (\n",
                "        prompt2\n",
                "        | GigaChat(credentials=\"авторизационные_данные\", verify_ssl_certs=False)\n",
                "        | StrOutputParser()\n",
                "    )\n",
                "    return chain2.invoke({\"joke\": parsed_output1})\n",
                "\n",
                "\n",
                "custom_chain.invoke(\"bears\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9d632fc7",
            "metadata": {},
            "source": [
                "В примере декоратор `@chain` используется для преобразования `custom_chain` в Runnable, для вызова которого используется метод `.invoke()`.\n",
                "\n",
                "<!--\n",
                "Если вы используете трассировку с [LangSmith](https://docs.smith.langchain.com/), вы должны увидеть там трассировку `custom_chain`, с вложенными в нее вызовами OpenAI.\n",
                "-->\n",
                "\n",
                "## Принудительное преобразование в цепочках\n",
                "\n",
                "При запуске своих функций в цепочках с оператором конвейера (`|`) вы можете не использовать конструктор `RunnableLambda` или декоратор `@chain`, а положиться на принудительное преобразование функции в Runnable.\n",
                "\n",
                "Пример:\n",
                "\n",
                "<!--\n",
                "Вот простой пример с функцией, которая принимает вывод от модели и возвращает первые пять букв из него:\n",
                "-->"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "5ab39a87",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'Once '"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "prompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")\n",
                "\n",
                "model = GigaChat(credentials=\"авторизационные_данные\", verify_ssl_certs=False)\n",
                "\n",
                "chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])\n",
                "\n",
                "chain_with_coerced_function.invoke({\"topic\": \"bears\"})"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9a481d1",
            "metadata": {},
            "source": [
                "В примере для функции `(lambda x: x.content[:5])` не понадобилось создавать обертку с помощью конструктора `RunnableLambda`, так как слева от оператора конвейера задан Runnable-объект `model`.\n",
                "В этом случае функция принудительно преобразуется в Runnable.\n",
                "Подробнее — в разделе [Соединение Runnable в цепочку](/docs/how_to/sequence/#coercion).\n",
                "\n",
                "## Передача метаданных выполнения\n",
                "\n",
                "Runnable-лямбды могут принимать необязательный параметр [RunnableConfig](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig), который они могут использовать для передачи вложенным запускам обратных вызовов, тегов и других данных конфигурации."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "ff0daf0c-49dd-4d21-9772-e5fa133c5f36",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'foo': 'bar'}\n",
                        "Tokens Used: 62\n",
                        "\tPrompt Tokens: 56\n",
                        "\tCompletion Tokens: 6\n",
                        "Successful Requests: 1\n",
                        "Total Cost (USD): $9.6e-05\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "\n",
                "from langchain_core.runnables import RunnableConfig\n",
                "\n",
                "\n",
                "def parse_or_fix(text: str, config: RunnableConfig):\n",
                "    fixing_chain = (\n",
                "        ChatPromptTemplate.from_template(\n",
                "            \"Fix the following text:\\n\\n```text\\n{input}\\n```\\nError: {error}\"\n",
                "            \" Don't narrate, just respond with the fixed data.\"\n",
                "        )\n",
                "        | GigaChat(credentials=\"авторизационные_данные\", verify_ssl_certs=False)\n",
                "        | StrOutputParser()\n",
                "    )\n",
                "    for _ in range(3):\n",
                "        try:\n",
                "            return json.loads(text)\n",
                "        except Exception as e:\n",
                "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
                "    return \"Failed to parse\"\n",
                "\n",
                "\n",
                "from langchain_community.callbacks import get_openai_callback\n",
                "\n",
                "with get_openai_callback() as cb:\n",
                "    output = RunnableLambda(parse_or_fix).invoke(\n",
                "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
                "    )\n",
                "    print(output)\n",
                "    print(cb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "1a5e709e-9d75-48c7-bb9c-503251990505",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'foo': 'bar'}\n",
                        "Tokens Used: 62\n",
                        "\tPrompt Tokens: 56\n",
                        "\tCompletion Tokens: 6\n",
                        "Successful Requests: 1\n",
                        "Total Cost (USD): $9.6e-05\n"
                    ]
                }
            ],
            "source": [
                "from langchain_community.callbacks import get_openai_callback\n",
                "\n",
                "with get_openai_callback() as cb:\n",
                "    output = RunnableLambda(parse_or_fix).invoke(\n",
                "        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}\n",
                "    )\n",
                "    print(output)\n",
                "    print(cb)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "922b48bd",
            "metadata": {},
            "source": [
                "## Потоковая передача токенов\n",
                "\n",
                "В цепочках можно использовать функции-генераторы: работающие как итерароры функции, которые использут ключевое слово `yield`.\n",
                "\n",
                "Сигнатура таких функций должна быть `Iterator[Input] -> Iterator[Output]` или `AsyncIterator[Input] -> AsyncIterator[Output]` при работе в асинхронном режиме.\n",
                "\n",
                "Используйте функции-генераторы, когда:\n",
                "\n",
                "* добавляете собственный парсер выходных данных;\n",
                "* нужно сохранить возможность работать в режиме потоковой передачи токенов при преобразовании выходных данных, полученных на последнем шаге.\n",
                "\n",
                "Ниже представлены примеры синхронной и асинхронной версии собственного парсера выходных данных для разделенных запятой перечислений.\n",
                "\n",
                "### Синхронная версия\n",
                "\n",
                "Сначала создадим цепочку, которая создает список данных."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "29f55c38",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "lion, tiger, wolf, gorilla, panda"
                    ]
                }
            ],
            "source": [
                "from typing import Iterator, List\n",
                "\n",
                "prompt = ChatPromptTemplate.from_template(\n",
                "    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\"\n",
                ")\n",
                "\n",
                "str_chain = prompt | model | StrOutputParser()\n",
                "\n",
                "for chunk in str_chain.stream({\"animal\": \"bear\"}):\n",
                "    print(chunk, end=\"\", flush=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "46345323",
            "metadata": {},
            "source": [
                "Затем добавим собственную функцию, которая будет агрегировать текущий потоковый вывод и выдавать его, когда модель сгенерирует следующую запятую в списке:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "f08b8a5b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['lion']\n",
                        "['tiger']\n",
                        "['wolf']\n",
                        "['gorilla']\n",
                        "['raccoon']\n"
                    ]
                }
            ],
            "source": [
                "# Собственный парсер, который делит итератор llm-токенов\n",
                "# на список строк, разделенных запятыми\n",
                "def split_into_list(input: Iterator[str]) -> Iterator[List[str]]:\n",
                "    # сохранение части ввода до получения запятой\n",
                "    buffer = \"\"\n",
                "    for chunk in input:\n",
                "        # добавление текущего фрагмента в буфер\n",
                "        buffer += chunk\n",
                "        # пока в буфере есть запятые\n",
                "        while \",\" in buffer:\n",
                "            # деление буфера при обнаружении запятой\n",
                "            comma_index = buffer.index(\",\")\n",
                "            # получение данных перед запятой\n",
                "            yield [buffer[:comma_index].strip()]\n",
                "            # сохранение оставшихся данных для следующей итерации\n",
                "            buffer = buffer[comma_index + 1 :]\n",
                "    # получение последнего фрагмента\n",
                "    yield [buffer.strip()]\n",
                "\n",
                "\n",
                "list_chain = str_chain | split_into_list\n",
                "\n",
                "for chunk in list_chain.stream({\"animal\": \"bear\"}):\n",
                "    print(chunk, flush=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0a5adb69",
            "metadata": {},
            "source": [
                "Вызов цепочки возвращает массив со всеми значениями:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "9ea4ddc6",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['lion', 'tiger', 'wolf', 'gorilla', 'raccoon']"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "list_chain.invoke({\"animal\": \"bear\"})"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "96e320ed",
            "metadata": {},
            "source": [
                "### Асинхронная версия\n",
                "\n",
                "Асинхронная версия описанного примера:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "569dbbef",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['lion']\n",
                        "['tiger']\n",
                        "['wolf']\n",
                        "['gorilla']\n",
                        "['panda']\n"
                    ]
                }
            ],
            "source": [
                "from typing import AsyncIterator\n",
                "\n",
                "\n",
                "async def asplit_into_list(\n",
                "    input: AsyncIterator[str],\n",
                ") -> AsyncIterator[List[str]]:  # async def\n",
                "    buffer = \"\"\n",
                "    async for chunk in (\n",
                "        input\n",
                "    ):  # `input` — экземпляр `async_generator`, поэтому нужно использовать `async for`\n",
                "        buffer += chunk\n",
                "        while \",\" in buffer:\n",
                "            comma_index = buffer.index(\",\")\n",
                "            yield [buffer[:comma_index].strip()]\n",
                "            buffer = buffer[comma_index + 1 :]\n",
                "    yield [buffer.strip()]\n",
                "\n",
                "\n",
                "list_chain = str_chain | asplit_into_list\n",
                "\n",
                "async for chunk in list_chain.astream({\"animal\": \"bear\"}):\n",
                "    print(chunk, flush=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "3a650482",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['lion', 'tiger', 'wolf', 'gorilla', 'panda']"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await list_chain.ainvoke({\"animal\": \"bear\"})"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
