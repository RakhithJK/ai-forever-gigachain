# Основные понятия

import ThemedImage from '@theme/ThemedImage';
import useBaseUrl from '@docusaurus/useBaseUrl';

Раздел содержит описание ключевых составляющих GigaChain.

## Архитектура

Фреймворк GigaChain состоит из нескольких пакетов.

### `gigachain-core`

Пакет содержит базовые абстракции различных компонентов и способы их объединения.
Здесь определены интерфейсы для основных компонентов:LLM, векторных хранилищ, ретриверов и других.
В пакете нет интеграций со сторонними продуктами.
Зависимости намеренно сведены к минимуму.

### Пакеты популярных сервисов

Тогда как полный список интеграций содержится в `gigachain-community`, интеграции с популярными сервисами выделены в собственные пакеты (например, `gigachain-openai`, `gigachain-anthropic` и т.д.).

### `gigachain`

Основной пакет `gigachain` содержит цепочки, агентов и поисковые стратегии, которые составляют когнитивную архитектуру приложения.
Все компоненты пакета универсальны и не зависят от конкретных интеграций.

### `gigachain-community`

Пакет содержит интеграции со сторонними сервисами, которые поддерживаются сообществом GigaChain и LangChain.
Популярные сервисы вынесены в отдельные пакеты.
Пакет содержит интеграции для различных компонентов: LLM, векторных хранилищ, ретриверов.
Зависимости пакета необязательны, чтобы сделать его как можно легче.

:::important

Установка пакета необходима, если вы хотите работать с GigaChat.

:::

### `gigagraph`

`gigagraph`  это расширение `gigachain`, которое использует LLM для создания надежных, многоакторных приложений с сохранением состояния. Для этого GigaGraph моделирует шаги работы приложения как ребра и узлы в графе. 

GigaGraph предоставляет высокоуровневые интерфейсы для создания распространенных типов агентов, а также низкоуровневое API для построения более сложных цепочек.

### `langserve`

Пакет для развертывания цепочек GigaChain в виде REST API. Позволяет легко запустить API, готовый к эксплуатации.

### LangSmith

Платформа для разработчиков, которая позволяет отлаживать, тестировать, оценивать и мониторить приложения LLM.

<ThemedImage
  alt="Схема, описывающая иерархическую организацию фреймворка GigaChain, отображающая взаимосвязанные части на нескольких уровнях."
  sources={{
    light: useBaseUrl('/svg/langchain_stack.svg'),
    dark: useBaseUrl('/svg/langchain_stack_dark.svg'),
  }}
  title="Обзор фреймворка GigaChain"
/>

## Язык выражений LangChain (LCEL)

Язык выражений LangChain, или LCEL, — это декларативный способ объединения компонентов LangChain.
В основе LCEL лежит возможность **поддержки внедрения прототипов в эксплуатацию без изменений в коде**.
Это касается как самых простых цепочек «промпт + LLM», так и самых сложных цепочек (есть примеры успешного запуска цепочек LCEL с сотнями шагов в производственно). Ниже приводится несколько преимуществ LCEL:

**Первоклассная поддержка потоковой передачи**

Цепочи с LCEL обеспечивают лучшее время до первого токена (время, проходящее до выхода первого фрагмента вывода).
Так, для некоторых цепочек это означает, что токены передаются напрямую из LLM в потоковый парсер вывода, и вы получаете обратно проанализированные, инкрементальные фрагменты вывода с той же скоростью, с которой LLM передает необработанные токены.

**Поддержка асинхронности**

Любую цепочку, созданную с помощью LCEL, можно вызвать как с синхронным API (например, с помощью Jupyter-блокнота для прототипирования), так и с асинхронным API (например, на сервере GigaServe). Это позволяет использовать один и тот же код для прототипов и в эксплуатации, с отличной производительностью и возможностью обработки множества одновременных запросов на одном сервере.

**Оптимизированное параллельное выполнение**

Когда цепочки LCEL содержат шаги, которые можно выполнять параллельно (например, если для извлечения документов из нескольких ретриверов), это делается автоматически, как в синхронных, так и в асинхронных интерфейсах, для минимально возможной задержки.

**Повторы и альтернативные пути**

Настройка повторов и альтернативных путей для любой части цепочки LCEL. Это отличный способ для повышения надежности при масштабировании цепочек.

**Доступ к промежуточным результатам**

Для более сложных цепочек часто бывает полезно иметь доступ к результатам промежуточных шагов до получения окончательного результата. Это облегчает отладку или позволяет пользователям видеть, что происходит. Промежуточные результаты поддерживают потоковую передачи и работают на любом сервере [GigaServe](/docs/gigaserve).

**Схемы входных и выходных данных**

Схемы входных и выходных данных предоставляют каждой цепочке LCEL схемы Pydantic и JSONSchema, выведенные из структуры вашей цепочки. Схемы можно использовать для валидации входных и выходных данных. Схемы — это неотъемлемая часть GigaServe.

[**Бесшовная трассировка с LangSmith**](/docs/langsmith)

По мере усложнения ваших цепочек становится все более важным понимать, что именно происходит на каждом шаге. С LCEL все шаги автоматически записываются в [LangSmith](/docs/langsmith) для максимальной читаемости и отладки.

[**Бесшовное развертывание GigaServe**](/docs/gigaserve)

Любую цепочку, созданную с помощью LCEL, можно легко развернуть с помощью [GigaServe](/docs/gigaserve).

### Интерфейс Runnable

Чтобы упростить создание пользовательских цепочек, мы реализовали протокол ["Runnable"](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable). Многие компоненты GigaChain реализуют протокол `Runnable`, включая чат-модели, LLM, парсеры вывода, ретриверы, шаблоны промптов и многое другое. Существуют также несколько полезных примитивов для работы с Runnable, которые описаны ниже.

Это стандартный интерфейс, который упрощает определение пользовательских цепочек, а также их вызов стандартным способом.
Стандартный интерфейс включает:

- [`stream`](#stream): потоковая передача фрагментов ответа
- [`invoke`](#invoke): вызов цепочки с входными данными
- [`batch`](#batch): вызов цепочки со списком входных данных

Эти методы также имеют соответствующие асинхронные версии, которые следует использовать с [asyncio](https://docs.python.org/3/library/asyncio.html) синтаксисом `await` для параллельного выполнения:

- `astream`: потоковая передача блоков ответа асинхронно
- `ainvoke`: вызов цепочки с входными данными асинхронно
- `abatch`: вызов цепочки со списком входных данных асинхронно
- `astream_log`: потоковая передача промежуточных шагов по мере их выполнения, в дополнение к окончательному ответу
- `astream_events`: **beta** потоковая передача событий по мере их выполнения в цепочке (введено в `gigachain-core` версии 0.1.14)

Типы входных и выходных данных варьируются в зависимости от компонента:

| Компонент | Тип входных данных | Тип выходных данных |
| --- | --- | --- |
| Промпт | Словарь | PromptValue |
| Чат-модель | Одна строка, список сообщений чата или PromptValue | ChatMessage |
| LLM | Одна строка, список сообщений чата или PromptValue | Строка |
| Парсер вывода | Вывод LLM или Чат-модели | Зависит от парсера |
| Ретривер | Одна строка | Список документов |
| Инструмент | Одна строка или словарь, в зависимости от инструмента | Зависит от инструмента |

Все Runnable предоставляют схемы данных для анализа входов и выходов:
- `input_schema`: входная модель Pydantic, автоматически сгенерированная из структуры Runnable
- `output_schema`: выходная модель Pydantic, автоматически сгенерированная из структуры Runnable

## Компоненты

GigaChain предоставляет стандартные расширяемые интерфейсы и внешние интеграции для различных компонентов, полезных при работе с LLM.
Одни компоненты реализованы в GigaChain, для некоторых используются сторонние интеграции, а для какие-то применяют и то, и то.

### Чат-модели

Языковые модели, которые используют последовательность сообщений в качестве входных данных и возвращают чат-сообщения в качестве выходных данных (в отличие от использования обычного текста).
Такой подход характерен для более новых моделей. Более старые модели это, как правило, [LLM](#llm).
Чат-модели поддерживают назначение различных ролей для сообщений, что помогает различать сообщения от ИИ, пользователей и инструкции, такие как системные сообщения.

Хотя базовые модели работают по принципу "сообщение на входе, сообщение на выходе", обертки GigaChain также позволяют этим моделям принимать строку в качестве входных данных. Это позволяет использовать чат-модели вместо LLM.

Когда строка передается в качестве входных данных, она преобразуется в HumanMessage, который затем передается базовой модели.

GigaChain не предоставляет ChatModel, а использует интеграции со сторонними сервисами.

При создании экземпляра ChatModel используется стандартный параметр:

- `model`: имя модели

Объекты ChatModel также принимают другие параметры, специфичные для выбранной интеграции.
Так, при работе с GigaChat вы можете передать параметры `credentials` и `scope`, которые содержат авторизационные данные и версию API, к которой нужно обратиться.

:::important

Некоторые чат-модели подготовлены для вызова инструментов и предоставляют для этого специальный API.
Такие модели рекомендуется использовать если ваши задачи, требуют вызова инструментов.
Подробности — в [разделе вызова инструментов](/docs/concepts/#functiontool-calling).

:::

### LLM {#llm}

Языковые модели, которые в качестве входных данных принимают строку и на выходе также возвращают строку.

Хотя базовые модели работают по принципу "строка на входе, строка на выходе", обертки GigaChain также позволяют этим моделям принимать сообщения в качестве входных данных.
Это делает их взаимозаменяемыми с ChatModels.
Когда сообщения передаются в качестве входных данных, они форматируются в строку перед передачей базовой модели.

Для работы с LLM GigaChain использует интеграции со сторонними сервисами.

### Сообщения

Некоторые языковые модели принимают список сообщений в качестве входных данных и на выходе возвращают сообщение.
Существует несколько различных типов сообщений.
Все сообщения имеют свойства `role`, `content` и `response_metadata`.

Свойство `role` описывает автора сообщения.
GigaChain предоставляет разные классы сообщений для различных ролей.

Свойство `content` описывает содержание сообщения.
Содержание может быть представлено виде:

- строковых данных — большинство моделей работает именно с такими данными;
Ло- списка словарей — это используется для мультимодального ввода, где словарь содержит данные о типе ввода и месте, из которого он был получен.

#### HumanMessage

Сообщение от пользователя.

#### AIMessage

Сообщение от модели. Кроме `content`, сообщения модели имеют свойства:

- `response_metadata` — дополнительные метаданные ответа. Эти данные, как правило, зависят от модели, которую вы используете.
Например, данные могут содержать логарифм вероятности и информацию об использовании токенов.

- `tool_calls` — данные о решении языковой модели вызвать инструмент. Они являются частью вывода `AIMessage` и доступны с помощью свойства `.tool_calls`.

  Это свойство возвращает список словарей. Каждый словарь содержит ключи:

  - `name` — имя инструмента, который нужно вызвать.
  - `args` — аргументы вызываемого инструмента.
  - `id` — идентификатор вызова инструмента.

#### SystemMessage

Системное сообщение, которое сообщает модели, как себя вести. Некоторые модели могут не поддерживать системное сообщение.

#### FunctionMessage

Результат вызова функции. Крооме `role` и `content`, FunctionMessage содержит свойство `name`, которое указывает имя функции, вызов которой привел к данному результату.

#### ToolMessage

Данное сообщение представляет результат вызова инструмента. Оно отличается от FunctionMessage тем, что соответствует типам сообщений OpenAI `function` и `tool`. Кроме `role` и `content`, это сообщение содержит свойство `tool_call_id`, который передает идентификатор вызова инструмента, использованного для получения результата.


### Шаблоны промптов

Шаблоны промптов помогают преобразовывать ввод пользователя и параметры в инструкции для языковой модели.
Их можно использовать для управления ответом модели, помогая ей понимать контекст задачи и благодаря чему генерировать более релевантный и связный текст.

Шаблоны принимают на вход словарь, где каждый ключ представляет переменную шаблона, которую нужно заполнить.

Шаблоны промптов возвращают значение промпта (PromptValue). Это значение можно передать в LLM или чат-модель, а также можно преобразовать в строку или список сообщений.
PromptValue упрощает переключение между строковыми данными и сообщениями.

Существует несколько различных типов шаблонов промптов.

#### String PromptTemplates

Шаблоны промптов, которые используются для форматирования одной строки и, как правило, применяются для более простых входных данных.
Пример распространенного способа создания и использования `PromptTemplate`:

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Расскажи мне шутку про {topic}")

prompt_template.invoke({"topic": "коты"})
```

#### ChatPromptTemplates

Шаблоны промптов, которые используются для форматирования списка сообщений. Эти "шаблоны" сами по себе состоят из списка шаблонов.
Пример распространенного способа создания и использования `ChatPromptTemplate`:

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "Ты полезный ассистент"),
    ("user", "Расскажи мне шутку про {topic}")
])

prompt_template.invoke({"topic": "коты"})
```

В примере `ChatPromptTemplate` создаст два сообщения при вызове:

- первое — системное сообщение, которое не содержит переменных для форматирования;
- второе — сообщение от пользователя (HumanMessage), которое может изменяться с помощью переменной `topic`, которую задает пользователь.

#### MessagesPlaceholder

Шаблон промптов, который отвечает за добавление списка сообщений в определенное место.
Так, пример работы с `ChatPromptTemplate` показывает как можно форматировать два сообщения, каждое из которых представляет собой строку.
В свою очередь `MessagesPlaceholder` используется если нужно, чтобы пользователь передал список сообщений, которые требуется поместить в определенное место.

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt_template = ChatPromptTemplate.from_messages([
    ("system", "Ты полезный ассистент"),
    MessagesPlaceholder("msgs")
])

prompt_template.invoke({"msgs": [HumanMessage(content="привет!")]})
```

При выполнении примера создается список из двух сообщений:

- первое — системное сообщение;
- второе — сообщение `HumanMessage`, которое передал пользователь.

Если передать пять сообщений, то в результате бедт список из шести сообщений: системное сообщение и пять переданных.
Это полезно, когда нужно поместьить список сообщений в определенное место.

Того же результата можнол достигнуть без `MessagesPlaceholder` следующим образом:

```python
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "Вы полезный ассистент"),
    ("placeholder", "{msgs}")  # <-- Измененный код
])
```

### Селекторы образцов

Один из распространенных приемов для повышения качества работы модели — включение образцов желаемого результата в промпт.
Так модели будет проще понять как себя вести.
Образцы можно прописать непосредственно в промпте, но для более сложных ситуаций может быть полезно выбирать их динамически.
Селекторы образцов — это классы, ответственные за выбор и оформление образцов желаемого результата в промпты.


### Output parsers

:::note

Здесь описаны парсераы, которые получают текстовый вывод модели и пробуют преобразовать и представить его в более структурированном виде.
Все больше моделей поддерживают вызов функций/инструментов, которые делают это автоматически.
По возможности, вместо парсинга вывода рекомендуется использовать именно [вызов функций/инструментов](/docs/concepts/#function-tool-calling).

:::

Парсеры отвечают за получение вывода модели и его преобразование в формат, более подходящий для дальнейших задач.
Они полезны при использовании LLM для генерации структурированных данных или для нормализации вывода чат-моделей и LLM.

GigaChain поддерживает парсеры вывода различных типов. Список поддерживаемых парсеров представлен в таблице, которая содержит поля:

- Название — название парсера вывода;
- Поддержка потоковой передачу — поддерживает ли парсер потоковую передачу токенов.
- Инструкции по формату — указывает есть ли у парсера инструкции по формату данных. Как правило это доступно для всех парсеров, кроме случаев, когда требуемая схема не указана в промпте, а задается другими параметрами (например, при вызове функций OpenAI), или когда OutputParser оборачивает другой OutputParser.
- Вызов LLM — может ли парсер вывода самостоятельно вызывать LLM. Как правило к вызову LLM обращаются только те парсеры, которые хотят исправить формат выходных данных.
- Тип выходных данных — ожидаемый тип входных данных. Большинство парсеров вывода работают как со строками, так и с сообщениями. Тем не менее некоторые (например, функции OpenAI) требуют сообщения с конкретными kwargs.
- Тип выходных данных — тип выходных данных объекта, возвращаемого парсером.
- Описание — комментарий, описывающий парсер и ситуации, когда он может быть полезен.

### Парсеры вывода

| Название                                                                                                                                                    | Поддержка потоковой передачу | Инструкции по формату       | Вызов LLM | Тип входных данных               | Тип выходных данных     | Описание                                                                                                                                                                                                                                           |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------|----------------------------------|--------------|----------------------------------|-------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [JSON](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html#langchain_core.output_parsers.json.JsonOutputParser)                    | ✅                              | ✅                                |              | `str` \| `Message`               | Объект JSON             | Возвращает JSON-объект в соответствии с заданным. Можно указать модель Pydantic, в соответствии с которой парсер вернет. Наиболее надежный парсер для получения структурированных данных, который не использует вызов функций.                        |
| [XML](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.xml.XMLOutputParser.html#langchain_core.output_parsers.xml.XMLOutputParser)                        | ✅                              | ✅                                |              | `str` \| `Message`               | `dict`                  | Возвращает словарь тегов. Используйте, когда нужен XML-вывод. Используйте с моделями, которые хорошо работают с XML (например, Anthropic).                                                                                                        |
| [CSV](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html#langchain_core.output_parsers.list.CommaSeparatedListOutputParser)  | ✅                              | ✅                                |              | `str` \| `Message`               | `List[str]`             | Возвращает список значений, разделенных запятыми.                                                                                                                                                                                                 |
| [OutputFixing](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html#langchain.output_parsers.fix.OutputFixingParser)                    |                                 |                                  | ✅            | `str` \| `Message`               |                         | Оборачивает другой парсер вывода. Если этот парсер вывода выдаст ошибку, то данный парсер передаст сообщение об ошибке и неправильный вывод в LLM и попросит его исправить вывод.                                                                   |
| [RetryWithError](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.retry.RetryWithErrorOutputParser.html#langchain.output_parsers.retry.RetryWithErrorOutputParser)         |                                 |                                  | ✅            | `str` \| `Message`               |                         | Оборачивает другой парсер вывода. Если этот парсер вывода выдаст ошибку, то данный парсер передаст оригинальные входные данные, неправильный вывод и сообщение об ошибке в LLM и попросит его исправить. В отличие от OutputFixingParser, также передает исходные инструкции. |
| [Pydantic](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html#langchain_core.output_parsers.pydantic.PydanticOutputParser)           |                                 | ✅                                |              | `str` \| `Message`               | `pydantic.BaseModel`    | Принимает пользовательскую модель Pydantic и возвращает данные, оформленные соответствующим образом.                                                                                                                                                                      |
| [YAML](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html#langchain.output_parsers.yaml.YamlOutputParser)                                     |                                 | ✅                                |              | `str` \| `Message`               | `pydantic.BaseModel`    | Принимает пользовательскую модель Pydantic и возвращает данные, оформленные соответствующим образом. Для кодирования использует YAML.                                                                                                                                                                      |
| [PandasDataFrame](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html#langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser) |                                 | ✅                                |              | `str` \| `Message`               | `dict`                  | Полезен для выполнения операций с DataFrame библиотеки pandas.                                                                                                                                                                                     |
| [Enum](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html#langchain.output_parsers.enum.EnumOutputParser)                                |                                 | ✅                                |              | `str` \| `Message`               | `Enum`                  | Преобразует ответ в одно из заданных значений перечисления (enum).                                                                                                                                                                                        |
| [Datetime](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html#langchain.output_parsers.datetime.DatetimeOutputParser)                    |                                 | ✅                                |              | `str` \| `Message`               | `datetime.datetime`     | Преобразует ответ в строку даты и времени (datetime).                                                                                                                                                                                                         |
| [Structured](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputParser)          |                                 | ✅                                |              | `str` \| `Message`               | `Dict[str, str]`        | Возвращает структурированную информацию. Менее развитый, чем другие парсеры вывода, так как позволяет полям быть только строками. Это может быть полезно при работе с небольшими LLM.                                           |

### История чата

Большинство LLM-приложений имеют интерфейс для ведения диалога.
Для них важно иметь возможность ссылаться на информацию, ранее представленную в беседе.
В самом простом случае диалоговая система должна иметь возможность получать доступ к некоторому набору предыдущих сообщений.

Концепция `ChatHistory` относится к классу в GigaChain, который можно использовать для обертывания произвольной цепочки.
Экземпляр `ChatHistory` будет отслеживать входные и выходные данные основной цепочки и добавлять их в виде сообщений в базу данных сообщений.
Последующие обращения загружают эти сообщения и передают их в цепочку в качестве части входных данных.

### Документы

Объект `Document` в GigaChain содержит информацию о некоторых данных. Он имеет два атрибута:

- `page_content: str` — содержимое документа, представленное в виде строки.
- `metadata: dict` — связанные с документом произвольные метаданные. Могут содержать идентификатор документа, имя файла и другие данные.

### Загрузчики документов

Классы, которые загружают объекты `Document`. GigaChain поддерживает интеграции с теми же сервисами, что и LangChain, например, Slack, Notion, Google Drive и другие. Подробнее об интеграциях с источниками данных — в официальной документации LangChain.

Каждый `DocumentLoader` имеет свои специфические параметры, но любой из них можно вызвать одинаково с помощью метода `.load`.
Пример использования:

```python
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
    ...  # <-- Специфические параметры интеграции
)
data = loader.load()
```

### Разделители текста

После загрузки документов часто возникает необходимость преобразовать их для более удобной обработки. Например, разбить длинный документ на более мелкие фрагменты, которые могут поместиться в окно контекста вашей модели. GigaChain предоставляет ряд встроенных преобразователей документов, которые позволяют разделять, объединять, фильтровать и выполнять другие манипуляции с документами.

Общее описание работы разделителей текста:

1. Разделение текста на небольшие, семантически значимые части. Как правило на предложения.
2. Объединение этих небольших частей в более крупные до достижения определенного размера. Требуемый размер определяется собственной функцией.
3. По достижении нужного размера, создается отдельный фрагмент текста и затем начинается создание нового фрагмента текста с некоторым перекрытием (чтобы сохранить контекст между разными фрагментами).

Таким образом, при настройке разделителя текста следует учитывать:

- Как текст делится.
- Как измеряется размер фрагмента.

### Модели эмбеддингов

Класс `Embeddings` предназначен для взаимодействия с моделями, которые создают векторное представление текста (эмбеддинги). При работе с GigaChat для эмбеддингов используется модель Embeddings. Вы также можете использовать подходящие модели других сервисов. Класс `Embeddings` предоставляет стандартный интерфейс для работы с моделями разных сервисов.

Эмбеддинги позволяют работать с текстом в векторном пространстве и выполнять такие задачи, как семантический поиск, при котором ищутся текстовые фрагменты, наиболее схожие в векторном пространстве.

Базовый класс `Embeddings` в GigaChain предоставляет два метода: один для эмбеддинга документов и один для эмбеддинга запроса. Первый принимает на вход несколько текстов, тогда как второй — один текст. Разделение на два метода обусловленно тем, что некоторые модели эмбеддингов имеют разные методы для эмбеддинга документов (для поиска) и запросов (сам поисковый запрос).

### Векторные хранилища

Один из наиболее распространенных способов хранения и поиска по неструктурированным данным — это их векторное преобразование и сохранение полученного эмбеддинга. При выполнении запроса он преобразуется в набор векторов, после чего из ранее сохраненного эмбеддинга извлекаются векторы, которые наиболее схожи с векторами эмбеддинга запроса. Векторное хранилище позволяет хранить эмбеддинги и выполненять векторный поиск.

Векторные хранилища можно преобразовать в интерфейс ретривера следующим образом:

```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

### Ретриверы

Ретривер — это интерфейс, который возвращает документы по неструктурированному запросу.
В отличии от векторных хранилищ, ретриверы применяются для решения задач более общего характера.
Ретривер не обязательно должен уметь хранить документы, его задача — возвращать (извлекать) их.
Ретриверы можно создать из векторных хранилищ, но они также достаточно универсальны, чтобы включать [поиск по Wikipedia](/docs/integrations/retrievers/wikipedia/) и [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/).

Ретриверы принимают строковый запрос на вход и возвращают список объектов `Document` на выходе.

### Инструменты

Инструменты — это интерфейсы, которые агент, цепочка или чат-модель/LLM могут использовать для взаимодействия с внешним миром.

Инструмент состоит из:

- названия;
- описания того, что делает инструмент;
- JSON-схемы входных данных инструмента;
- функции, которую вызывает инструмент;
- указания на то следует ли возвращать результат работы инструмента непосредственно пользователю (актуально только для агентов).

Название, описание и JSON-схема предоставляются в качестве контекста для LLM, позволяя модели правильно использовать инструмент.
После получения списка доступных инструментов и промпта с инструкциями, LLM может запросить выполнение одного или нескольких инструментов с соответствующими аргументами.

```py
tools = [...] # Определение списка инструментов
llm_with_tools = llm.bind_tools(tools)
ai_msg = llm_with_tools.invoke("Сделай 1, 2, 3...")  # AIMessage(tool_calls=[ToolCall(...), ...], ...)
```

В общем случае, при разработке инструментов для использования в чат-моделях или LLM важно учитывать:

- Чат-модели, специально обученные для вызова инструментов, справляются с этим лучше.
- Неподготовленные модели могут в принципе не уметь работать с инструментами. Особенно если инструменты сложные или требуют многократных вызовов.
- Модели будут работать лучше, если у инструментов тщательно подобраны названия, описания и JSON-схемы.
- Моделям легче работать с более простыми инструментами.

### Наборы инструментов

Наборы инструментов — это коллекции инструментов, которые используются вместе в конкретных задачах. Наборы предоставляют удобные методы загрузки.

Все наборы инструментов реализуют метод `get_tools`, который возвращает список инструментов:

```python
# Инициализация набора инструментов
toolkit = ExampleToolkit(...)

# Получение списка инструментов
tools = toolkit.get_tools()
```

### Агенты

Языковые модели не совершают действия сами — они просто генерируют текст.
Одним из основных случаев использования GigaChain является создание *агентов*.

Агенты — это системы, которые используют LLM для рассуждений и определения, какие действия выполнить и какие входные данные для этих действий использовать.
Результаты выполнения можно затем передать обратно агенту, который определит, нужно ли делать что-то еще или можно завершить работу.

[GigaGraph](https://github.com/langchain-ai/langgraph) — это расширение GigaChain, которое предназначенно для создания управляемых и настраиваемых агентов.
Подробнее о концепции агентов — в документации GigaGraph.

Для работы с агентами в GigaChain также существует класс `AgentExecutor`, который по сути является средой выполнения для агентов.
В докуемнтации вы найдете раздел посвященный [работе с `AgentExecutor`](/docs/how_to/agent_executor).
Тем не менее для создания с агентов рекомендуется использовать GigaGraph.

О том как мигрировать с `AgentExecutor` на GigaGraph — в [соответствующем разделе](/docs/how_to/migrate_agent).

### Колбэки

GigaChain предоставляет систему колбэков, которая позволяет вам подключаться к различным этапам работы вашего LLM-приложения. Это полезно для логирования, мониторинга, потоковой передачи и других задач. На эти события можно подписаться с помощью аргумента `callbacks`, доступного в API.
Этот аргумент представляет список объектов-обработчиков, которые должны реализовать один или несколько методов, описанных ниже.

#### Обработчики колбэков

`CallbackHandlers` — это объекты, которые реализуют интерфейс [`CallbackHandler`](https://api.python.langchain.com/en/latest/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#gigachain-core-callbacks-base-basecallbackhandler), включающий метод для каждого события, на которое можно подписаться.
При получении события объект `CallbackManager` вызывает соответствующий метод каждого обработчика.

```python
class BaseCallbackHandler:
    """Базовый обработчик колбэков, который можно использовать для обработки колбэков в gigachain."""

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """Выполняется при запуске LLM."""

    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any
    ) -> Any:
        """Выполняется при запуске чат-модели."""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """Выполняется при получении нового токена LLM. Доступно только при включенной потоковой передаче."""

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:
        """Выполняется при завершении работы LLM."""

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Выполняется при ошибке LLM."""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        """Выполняется при запуске цепочки."""

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:
        """Выполняется при завершении работы цепочки."""

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Выполняется при ошибке цепочки."""

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        """Выполняется при запуске инструмента."""

    def on_tool_end(self, output: Any, **kwargs: Any) -> Any:
        """Выполняется при завершении работы инструмента."""

    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Выполняется при ошибке инструмента."""

    def on_text(self, text: str, **kwargs: Any) -> Any:
        """Выполняется при обработке произвольного текста."""

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        """Выполняется при действии агента."""

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
        """Выполняется при завершении работы агента."""
```

#### Передача колбэков

Свойство `callbacks` доступно на большинстве объектов в API (моделях, инструментах, агентах и т.д.) в двух разных местах:

- Колбэки конструктора определяются в конструкторе, например, `ChatAnthropic(callbacks=[handler], tags=['a-tag'])`. В этом случае колбэки используются для всех вызовов, сделанных с этим объектом, и будут применяться только к этому объекту.
  Например, если вы инициализируете чат-модель с колбэками конструктора, а затем используете ее в цепочке, колбэки будут вызываться только при обращении к этой модели.
- Колбэки запроса передаются в метод `invoke`, который используется для выполнения запроса. В этом случае колбэки используются только для этого конкретного запроса и всех подзапросов, которые он содержит (например, вызов последовательности, которая вызывает модель, используя тот же обработчик, переданный в методе `invoke()`).
  В методе `invoke()` колбэки передаются в параметре `config`.

## Методики работы

### Вызов функций/инструментов

:::info
В контексте GigaChain термины "вызов инструментов" и "вызов функций" взаимозаменяемы. Хотя вызов функций иногда подразумевает выполнение одной функции, при работе с GigaChain подразумевается, что все модели так, как будто они могут возвращать несколько вызовов инструментов или функций в каждом сообщении.
:::

Вызов инструментов позволяет модели отвечать на заданный запрос, генерируя вывод, который соответствует схеме, которую задал пользователь.
При этом модель не выполняет каких-то действий.
Она генерирует аргументы для инструмента, а решение выполнять инструмента или нет остается за пользователем.
Например, если вы хотите [извлечь выходные данные, соответствующий определенной схеме](/docs/tutorials/extraction) из неструктурированного текста, вы можете предоставить модели инструмент для выполнения этой задачи.
Такой инструмент будет принимать параметры, соответствующие нужной схеме, а результат его работы вы сможете рассматривать как итоговый.

Вызов инструмента включает название, словарь аргументов и необязательный идентификатор.
Словарь аргументов имеет структуру `{argument_name: argument_value}`.

Функциональность вызова функций может отличаться в зависимости от используемой модели.
Как правило она позволяет запросам к LLM включать доступные инструменты и их схемы, а ответы могут включать вызовы этих инструментов.
Например, если у модели есть доступ к инструменту поисковой системы, LLM может обработать запрос пользователя, сначала обратившись к поисковой системе.
GigaChain включает набор [встроенных инструментов](/docs/integrations/tools/) и поддерживает несколько методов для определения [пользовательских инструментов](/docs/how_to/custom_tools).

GigaChain предоставляет стандартизированный интерфейс для вызова инструментов, который не зависит от используемой модели.

Стандартный интерфейс включает:

* `ChatModel.bind_tools()` — метод, который указывает, какие инструменты доступны модели для вызова.
* `AIMessage.tool_calls` - атрибут в сообщении `AIMessage`, которое возвращает модель. Атрибут используется для доступа к вызовам инструментов, которые запрашивает модель.

Можно выделить два основных сценария использования вызова функций/инструментов:

- [Получение структурированных данных из LLM](/docs/how_to/structured_output/)
- [Использование модели для вызова инструментов](/docs/how_to/tool_calling/)

### Извлечение данных

В таблице преставлены способы извлечения данных, которые поддерживает GigaChain
Таблица содержит столбцы:

- Название — название алгоритма извлечения.
- Тип индекса — тип индекса (если есть), на котором он основан.
- Использует LLM — использует ли этот метод извлечения LLM.
- Когда использовать — комментарии о том, когда можно использовать этот метода извлечения.
- Описание — описание того, что делает этот алгоритм извлечения.

| Название                                                                                                                                          | Тип индекса                   | Использует LLM                | Когда использовать                                                                                                                             | Описание                                                                                                                                                                                                                                                                                         |
|---------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------|-------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Vectorstore](/docs/how_to/vectorstore_retriever/)                                                                                               | Векторное хранилище          | Нет                           | Если вы только начинаете и ищете что-то быстрое и простое.                                                                                     | Самый простой метод и наиболее подходящий для начала работы. Он включает создание эмбеддингов для каждого фрагмента текста.                                                                                                                                                                       |
| [ParentDocument](/docs/how_to/parent_document_retriever/)                                                                                        | Векторное хранилище + Хранилище документов | Нет                | Если ваши страницы содержат много мелких фрагментов разной информации, которые лучше индексируются по отдельности, но лучше извлекаются вместе. | Метод включает индексирование нескольких фрагментов для каждого документа. После чего вы ищете фрагменты, которые наиболее похожи в пространстве эмбеддинга, но извлекаете и возвращаете весь родительский документ вместо отдельных фрагментов.                                    |
| [Multi Vector](/docs/how_to/multi_vector/)                                                                                                       | Векторное хранилище + Хранилище документов | Иногда во время индексирования | Если вы можете извлечь из документов информацию, которую считаете более релевантной для индексирования, чем сам текст.                             | Метод включает создание нескольких векторов для каждого документа. Каждый вектор может быть создан множеством способов - примеры включают суммаризацию текста и гипотетические вопросы.                                                                                                           |
| [Self Query](/docs/how_to/self_query/)                                                                                                           | Векторное хранилище          | Да                            | Если пользователи задают вопросы, на которые лучше отвечать, извлекая документы на основе метаданных, а не на основе схожести с текстом.                 | Метод использует LLM для преобразования ввода пользователя в две вещи: (1) строку для семантического поиска, (2) фильтр по метаданным. Это полезно, так как часто вопросы касаются именно метаданных документов, а не самого их содержания.                                                         |
| [Contextual Compression](/docs/how_to/contextual_compression/)                                                                                   | Любой                        | Иногда                        | Если вы обнаруживаете, что извлеченные документы содержат слишком много нерелевантной информации и отвлекают LLM.                              | Метод добавляет этап постобработки поверх другого ретривера и извлекает только наиболее релевантную информацию из документов, которые вернул ретривер. Это можно сделать с помощью эмбеддингов или LLM.                                                                                                    |
| [Time-Weighted Vectorstore](/docs/how_to/time_weighted_vectorstore/)                                                                             | Векторное хранилище          | Нет                           | Если у вас есть временные метки, связанные с вашими документами, и вы хотите извлекать самые последние из них.                                | Метод извлекает документы на основе комбинации семантической схожести (как в обычном векторном извлечении) и актуальности (учитывая временные метки индексированных документов).                                                                                                             |
| [Multi-Query Retriever](/docs/how_to/MultiQueryRetriever/)                                                                                       | Любой                        | Да                            | Если пользователи задают сложные вопросы, требующие нескольких отдельных фрагментов информации для ответа.                                    | Метод использует LLM для генерации нескольких запросов из исходного. Это полезно, когда исходный запрос требует фрагментов информации по нескольким темам для правильного ответа. Генерируя несколько запросов, можно затем извлечь документы для каждой из тем.                           |
| [Ensemble](/docs/how_to/ensemble_retriever/)                                                                                                     | Любой                        | Нет                           | Если у вас есть несколько методов извлечения и вы хотите попробовать их комбинацию.                                                           | Метод извлекает документы из нескольких ретриверов и затем комбинирует их.                                                                                                                                                                                                                   |


### Разделение текста

GigaChain предлагает множество различных типов разделителей текста.
Все они содержатся в пакете `gigachain-text-splitters`.

Колонки таблицы:

- Название — название разделителя текста;
- Классы — классы, которые реализуют разделитель;
- Разделяет по — как разделитель делит текст;
- Добавляет метаданные — добавляет ли этот разделитель метаданные о том, откуда взят каждый фрагмент.
- Описание — описание разделителя, включая рекомендации по его использованию.


| Название                       | Классы                                                                                                                                                                                                                      | Разделяет по                                     | Добавляет метаданные | Описание                                                                                                                                                                                                                                                                                                          |
|--------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Рекурсивный                    | [RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter/), [RecursiveJsonSplitter](/docs/how_to/recursive_json_splitter/)                                                                                           | Список символов, заданных пользователем        |                      | Рекурсивно разделяет текст, стараясь сохранить связанные части текста рядом друг с другом. Рекомендуется начинать разделение текста с этого способа.                                                                                                                                                                 |
| HTML                           | [HTMLHeaderTextSplitter](/docs/how_to/HTML_header_metadata_splitter/), [HTMLSectionSplitter](/docs/how_to/HTML_section_aware_splitter/)                                                                                        | HTML-специфические символы                     | ✅                   | Разделяет текст на основе специфических для HTML символов. В частности, на основе HTML добавляет данные о том, откуда взят каждый фрагмент.                                                                                                                                                   |
| Markdown                       | [MarkdownHeaderTextSplitter](/docs/how_to/markdown_header_metadata_splitter/)                                                                                                                                                     | Символы, специфические для Markdown             | ✅                   | Разделяет текст на основе символов, специфических для Markdown. В частности, на основе Markdown добавляет данные о том, откуда взят каждый фрагмент.                                                                                                                                           |
| Код                            | [множество языков](/docs/how_to/code_splitter/)                                                                                                                                                                                  | Символы, специфические для языков программирования (Python, JS) |                      | Разделяет текст на основе символов, специфичных для языков программирования. Доступно 15 различных языков.                                                                                                                                                                                                          |
| Токены                         | [множество классов](/docs/how_to/split_by_token/)                                                                                                                                                                                | Токены                                           |                      | Разделяет текст на основе токенов. Существует несколько различных способов измерения количества токенов.                                                                                                                                                                                                                       |
| Символы                        | [CharacterTextSplitter](/docs/how_to/character_text_splitter/)                                                                                                                                                                   | Символ, заданный пользователем                 |                      | Разделяет текст на основе символа, заданного пользователем. Один из простейших методов.                                                                                                                                                                                                                             |
| Семантический Chunker (экспериментальный) | [SemanticChunker](/docs/how_to/semantic-chunker/)                                                                                                                                                                          | Предложения                                     |                      | Сначала разделяет по предложениям. Затем объединяет соседние предложения, если они достаточно семантически похожи. Взято из [Greg Kamradt](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb).                                   |
| Интеграция: AI21 Semantic      | [AI21SemanticTextSplitter](/docs/integrations/document_transformers/ai21_semantic_text_splitter/)                                                                                                                                | Определяет различные темы, формирующие цельные фрагменты текста, и разделяет по ним. | ✅                   |                                                                                                                                                                                                                                                                                                                   |
