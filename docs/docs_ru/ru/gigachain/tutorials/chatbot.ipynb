{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "u-QHPUpGZHpZ",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "keywords: [conversationchain]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzG4MogMZHpc"
   },
   "source": [
    "# Разработка чат-бота\n",
    "\n",
    "Раздел содержит пример разработки чат-бота на основе LLM.\n",
    "Этот чат-бот может вести беседу и запоминать предыдущие действия пользователя.\n",
    "\n",
    "В примере рассмотрен чат-бот, который для ведения беседы использует только языковую модель.\n",
    "Также существуют другие способы разработки чат-ботов, которые могут вас заинтересовать:\n",
    "\n",
    "- [Создание разговорного приложения с RAG](/ru/gigachain/tutorials/qa_chat_history) — позволяет чат-боту использовать внешние источники данных.\n",
    "- [Агенты](/ru/gigachain/tutorials/agents) — чат-боты, которые могут выполнять действия.\n",
    "\n",
    "Здесь вы найдете базовую информацию о разработке чат-ботов, которая будет полезна при работе с приведенными выше разделами.\n",
    "Но если нужно вы можете сразу начать с более сложных чат-ботов.\n",
    "\n",
    "## Основные компоненты\n",
    "\n",
    "Пример чат-бота показывает как работать с такими компоненатми, как:\n",
    "\n",
    "- [`Чат-модели`](/ru/gigachain/concepts#chat-models). Чат-боты работают с данными, представленными в виде сообщений, а не в виде необработанного текста. Поэтому для разработки лучше использовать чат-модели, а не текстовые LLM, которые возвращают простой текст.\n",
    "- [`Шаблоны промптов`](/ru/gigachain/concepts#prompt-templates). Шаблоны упрощают создание промптов, которые объединяют стандартные сообщения, ввод пользователя, историю чатов и, если нужно, дополнительный контекст.\n",
    "- [`История чата`](/ru/gigachain/concepts#chat-history). История позволяет чат-боту сохранять прошлые взаимодействия с пользователем и учитывать их при ответе на последующие вопросы.\n",
    "\n",
    "<!--\n",
    "- Отладка и трассировка вашего приложения с помощью [LangSmith](/docs/concepts/#langsmith)\n",
    "-->\n",
    "\n",
    "## Подготовка к разработке\n",
    "\n",
    "### Jupyter-блокноты\n",
    "\n",
    "Это руководство, как и большинство других в документации, использует [Jupyter-блокноты](https://jupyter.org/). Они отлично подходят для изучения работы с LLM-системами, так как предоставляют интерактивную среду для работы с руководствами и позволяют работать с непредвиденными ситуациями: недоступностью API, нетипичным выводом и другими.\n",
    "\n",
    "Подробнее об установке jupyter — в [официальной документации](https://jupyter.org/install).\n",
    "\n",
    "### Установка\n",
    "\n",
    "Для установки GigaChain выполните команду:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjXjuyQFZHpf",
    "outputId": "88554ba0-2be5-4fa0-fbe0-935074492ef5",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install gigachain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZweTst4TZHph"
   },
   "source": [
    "Подробнее об установке — в разделе [Установка](https://developers.sber.ru/docs/ru/gigachain/get-started/installation).\n",
    "\n",
    "<!--\n",
    "### LangSmith\n",
    "\n",
    "Многие приложения, которые вы создаете с помощью LangChain, будут содержать несколько шагов с многократными вызовами LLM.\n",
    "По мере усложнения этих приложений становится важно иметь возможность инспектировать, что именно происходит внутри вашей цепочки или агента.\n",
    "Лучший способ сделать это — с помощью [LangSmith](https://smith.langchain.com).\n",
    "\n",
    "После регистрации по ссылке выше, убедитесь, что вы установили переменные среды для начала ведения журнала трассировок:\n",
    "\n",
    "```shell\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=\"...\"\n",
    "```\n",
    "\n",
    "Или, если вы работаете в блокноте, вы можете установить их с помощью:\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "```\n",
    "-->\n",
    "\n",
    "## Быстрый старт\n",
    "\n",
    "Сначала ознакомьтесь как использовать языковую модель отдельно.\n",
    "Хотя GigaChain поддерживает различные языковые модели, основным преимуществом библиотеки является возможность работы с моделями GigaChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n_rKUoVqZHph"
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_community.chat_models.gigachat import GigaChat\n",
    "\n",
    "model = GigaChat(\n",
    "    credentials=\"<авторизационные_данные>\",\n",
    "    scope=\"GIGACHAT_API_PERS\",\n",
    "    model=\"GigaChat-Pro\",\n",
    "    verify_ssl_certs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2w1JQBVZHpi"
   },
   "source": [
    ":::note\n",
    "\n",
    "Объект GigaChat принимает параметры:\n",
    "\n",
    "- `credentials` — авторизационные данные для обмена сообщениями с GigaChat API. О том как получить атворизационные данные — в разделе [Быстрый старт](/ru/gigachat/individuals-quickstart).\n",
    "- `scope` — необязательный параметры, в котором можно указать версию API, к которой нужно обратиться. Возможные значения:\n",
    "  \n",
    "  - `GIGACHAT_API_PERS` — версия API для физических лиц;\n",
    "  - `GIGACHAT_API_CORP` — версия API для ИП и юрлиц.\n",
    "\n",
    "  По умолчанию запросы передаются в версию для физических лиц.\n",
    "\n",
    "- `model` — необязательный параметр, в котором можно явно задать [модель GigaChat](/ru/gigachat/models).\n",
    "\n",
    "  В примере используется модель GigaChat-Pro. Ответы других моделей могут отличаться.\n",
    "\n",
    "- `verify_ssl_certs` — необязательный параметр, с помощью которого можно отключить проверку [сертификатов НУЦ Минцифры](/ru/gigachat/certificates).\n",
    "\n",
    "[Подробнее о параметрах GigaChat](https://github.com/ai-forever/gigachat).\n",
    "\n",
    ":::\n",
    "\n",
    "Попробуйте обратиться к модели напрямую.\n",
    "\n",
    "Объекты `ChatModel` — это экземпляры Runnable-интерфейса GigaChain.\n",
    "Все экземпляры Runnable предоставляют стандартный интерфейс для взаимподействия.\n",
    "\n",
    "Так, чтобы обратиться к модели достаточно вызвать метод `.invoke()` со списком сообщений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHoFJqq6ZHpi",
    "outputId": "39350d70-6956-4f5d-a0e1-394a094546cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Здравствуйте, Вася! Я генеративная языковая модель от Сбера. Готова ответить на ваши вопросы.', response_metadata={'token_usage': Usage(prompt_tokens=19, completion_tokens=28, total_tokens=47), 'model_name': 'GigaChat-Pro:2.2.25.3', 'finish_reason': 'stop'}, id='run-5e4e4ab8-8ed6-4004-90a5-fa0c8dfdb8ce-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Привет! Меня зовут Вася\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P61mG-IHZHpj"
   },
   "source": [
    "Сама модель не сохраняет информацию о состоянии.\n",
    "В этом можно убедиться, если задать ей дополнительный вопрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qh0zBeWMZHpk",
    "outputId": "647576af-5e6a-46fb-9f9b-bf82e06ae60c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Я не могу знать вашего имени, но вы можете сообщить его мне.', response_metadata={'token_usage': Usage(prompt_tokens=16, completion_tokens=16, total_tokens=32), 'model_name': 'GigaChat-Pro:2.2.25.3', 'finish_reason': 'stop'}, id='run-1ed92f73-e945-40ac-913b-bc0caf013d59-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"Как меня зовут?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWKwUQluZHpk"
   },
   "source": [
    "<!--\n",
    "Давайте взглянем на пример [трассировки LangSmith](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r)\n",
    "\n",
    "Мы видим, что модель не учитывает предыдущий ход разговора и не может ответить на вопрос. Это делает чат-бот крайне неудобным!\n",
    "-->\n",
    "\n",
    "Чтобы обойти это ограничение, передайте всю историю разговора в модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fleipeK7ZHpk",
    "outputId": "f187b70e-4a2d-40ab-cc33-f7404d9b31fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Вас зовут Вася.', response_metadata={'token_usage': Usage(prompt_tokens=60, completion_tokens=8, total_tokens=68), 'model_name': 'GigaChat-Pro:2.2.25.3', 'finish_reason': 'stop'}, id='run-bc7a1b1d-f6aa-455d-aa5b-f84c65394d07-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Привет! Меня зовут Вася\"),\n",
    "        AIMessage(\n",
    "            content=\"Здравствуйте, Вася! Я генеративная языковая модель от Сбера. Готова ответить на ваши вопросы.\"\n",
    "        ),\n",
    "        HumanMessage(content=\"Как меня зовут?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZicYwWY7ZHpl"
   },
   "source": [
    "Теперь модель может гораздо точнее отвечать на дополнительные вопросы.\n",
    "\n",
    "Работа с историей сообщений позволяет чат-боту вести разговор.\n",
    "Ниже показано как ее реализовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1gFCBgGZHpl"
   },
   "source": [
    "## История сообщений\n",
    "\n",
    "Чтобы модель сохраняла состояние, вы можете обернуть ее в класс Message History.\n",
    "Класс отслеживает входные и выходные данные модели и сохраняет их в хранилище данных.\n",
    "При повторных обращениях сообщения модели загружаются из хранилища и передаются в цепочку как часть входных данных.\n",
    "\n",
    "Пример ниже использует хранилище истории сообщений, доступное в пакете `gigachain-community`, который вы установили в начале.\n",
    "\n",
    "Импортируйте соответствующие классы и настройте цепочку, которая обернет модель и добавит историю сообщений.\n",
    "\n",
    "Самой важной частью здесь является функция `get_session_history`.\n",
    "Она должна принимать строковый идентификатор сессии `session_id` и возвращать историю разговора в объекте Message History.\n",
    "Параметр `session_id` используется, чтобы различать разговоры.\n",
    "Он передается как часть конфигурационной переменной при вызове новой цепочки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l-9L2V4UZHpl"
   },
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkSnHVa7ZHpm"
   },
   "source": [
    "Создайте переменную `config`, которая будет содержать дополнительные данные для вызова цепочки.\n",
    "В примере эта переменная содержит идентификатор сессии `session_id`.\n",
    "Передавайте переменную при каждом вызове runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bg-IOaGGZHpm"
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "nd-0Xq0uZHpm",
    "outputId": "4c73bcb0-55d6-4a41-b3e9-2eae36ae135a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Здравствуйте, Вася! Я генеративная языковая модель от Сбера. Готова ответить на ваши вопросы.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Привет! Меня зоут Вася\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "JV1mKlG5ZHpm",
    "outputId": "c1838cbb-62c0-4061-ff28-3791b9e6c382"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Вас зовут Вася.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Как меня зовут?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "go2qkT7RZHpn"
   },
   "source": [
    "Теперь чат-бот запоминает информацию.\n",
    "Если вы измените переменную `config`, чтобы сослаться на другую сессию (`session_id`), то увидите, что разговор начнется заново."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Drqhtt1pZHpn",
    "outputId": "c8c7c684-a4bb-40b5-a41c-4c5bf5c32bac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Я не могу знать вашего имени, но готова сгенерировать для вас текст по запросу.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Как меня зовут?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pU9d_eQMZHpn"
   },
   "source": [
    "При этом вы всегда можете вернуться к первоначальному разговору (так как он сохраняется в базе данных)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DBSpNmvEZHpn",
    "outputId": "8cb5d40f-66b3-4b8b-aaf9-c1ae7af7f52c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Вас зовут Вася.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJDtqFI9ZHpo"
   },
   "source": [
    "Таким образом, ваш чат-бот сможет разговаривать с разными пользователями.\n",
    "\n",
    "Примеры ниже показывают как использовать шаблон промпта, чтобы расширить и персонализировать данные, которые сохраняет чат-бот."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tvz6zR4ZHpo"
   },
   "source": [
    "## Шаблоны промптов\n",
    "\n",
    "Шаблоны промптов помогают преобразовать необработанные данные пользователя в формат, с которым может работать LLM.\n",
    "В данном случае необработанные данные - это просто сообщение, которое вы передаете в LLM.\n",
    "Попробуйте развить сообщение.\n",
    "\n",
    "Сначала добавьте системное сообщение со своими инструкциями (но все еще принимая сообщения в качестве входных данных).\n",
    "А затем добавьте больше входных данных, помимо сообщений.\n",
    "\n",
    "Для добавления системного сообщения создайте экземпляр `ChatPromptTemplate`.\n",
    "Чтобы передать все сообщения используйте `MessagesPlaceholder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ugTyecoQZHpo"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Ты личный помощник. Старайся как можно лучше помочь пользователю.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-URFvzljZHpo"
   },
   "source": [
    "В результате тип входных данных изменится — вместо простого списка сообщений вы теперь передаете словарь с ключом `messages`, который содержит список сообщений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fH2iyIMZHpo",
    "outputId": "2b53bde9-c590-4349-f1a9-485a8cd8f88c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Здравствуйте, Вася! Рада знакомству. Как я могу вам помочь?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"Привет! Меня зовут Вася\")]})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_PyrSFUZHpp"
   },
   "source": [
    "Теперь вы можете обернуть полученный код в объект истории сообщений `with_message_history`, созданный ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InCQsbhUZHpp"
   },
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BXK9x9TZHpp"
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf8LQbA4ZHpp",
    "outputId": "e58efc2f-ea97-41ac-d6cd-2a4aa4854791"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Здравствуйте, Кира! Рада знакомству с вами.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Привет! Меня зовут Кира\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1l0fgOBDZHpq",
    "outputId": "43002e17-df6d-4e81-f52f-ec40757377a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Вас зовут Кира.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Как меня зовут?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dE_TIbPZHpq"
   },
   "source": [
    "Усложните полученный промпт.\n",
    "Предположим, что шаблон промпта теперь выглядит примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GDz0PmvKZHpq"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Ты личный ассистент. Старайся как можно лучше помочь пользователю. Отвечай на все вопросы пользователя на следующем языке: {language}. Не называй своего имени.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEczoYqfZHpq"
   },
   "source": [
    "Выше в промпт добавлена новая переменная `language`.\n",
    "Теперь вы можете вызвать цепочку и задать язык, на котором должна ответить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtzFLZ_rZHpr",
    "outputId": "d7642a3d-d5bb-4559-a3b4-6407d616efc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am your personal assistant. How can I help you?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Привет! Меня зовут Вася\")],\n",
    "        \"language\": \"Английский\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8goysHGuZHpr"
   },
   "source": [
    "Оберните полученную цепочку в класс `with_message_history`.\n",
    "Поскольку входные данные содержат несколько ключей, вам нужно указать правильный ключ для сохранения истории чата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "R4dusLjyZHpr"
   },
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJDwMNIHZHpr"
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5P27VNiZHpx",
    "outputId": "4bff7298-50fb-4273-97a5-7f4a56b91c57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, I am your personal assistant. How can I help you?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Привет! Меня зовут Коля\")],\n",
    "        \"language\": \"Английский\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeVCmxK7ZHpx"
   },
   "source": [
    "<!--\n",
    "\n",
    "Блок кода\n",
    "\n",
    "```\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Как меня зовут?\")], \"language\": \"Китайский\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJSM5bREZHpy"
   },
   "source": [
    "<!--\n",
    "Чтобы лучше понять, что происходит внутри, ознакомьтесь с [этой трассировкой LangSmith](https://smith.langchain.com/public/f48fabb6-6502-43ec-8242-afc352b769ed/r).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW-SasN9ZHpy"
   },
   "source": [
    "## Управление историей разговоров\n",
    "\n",
    "При разработке чат-бота рано или поздно вам понадобится управлять историей разговоров.\n",
    "Как правило, это продиктовано размером контекстного окна модели, в котором перестанут помещаться все сообщения разговора.\n",
    "Чтобы избежать этого нужно добавить этап, на котором будет ограничиваться размер передаваемых сообщений.\n",
    "\n",
    ":::caution\n",
    "\n",
    "При этом, этот этап должен срабатывать до шаблона промпта, но после загрузки предыдущих сообщений из Message History.\n",
    "\n",
    ":::\n",
    "\n",
    "Для этого перед промптом вы можете добавить шаг, который изменяет содержимое `messages` соответствующим образом, а затем обернуть полученную цепочку в класс Message History.\n",
    "\n",
    "В GigaChain есть несколько встроенных инструментов для управления списком сообщений.\n",
    "В примере ниже для уменьшения количества сообщений, которые нужно передать в модель, используется функция [trim_messages](/ru/gigachain/how-to/trim_messages).\n",
    "Она позволяет указать, сколько токенов в истории сообщений нужно сохранить, а также задать другие параметры. Например, всегда ли нужно сохранять системное сообщение и разрешать ли частичные сообщения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "he1ZvM6hZHpy",
    "outputId": "0f7fbe12-da73-4a3d-f7d9-9ad672b1404f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Ты личный ассистент'),\n",
       " HumanMessage(content='Сколько будет 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='Спасибо'),\n",
       " AIMessage(content='Пожалуйста!'),\n",
       " HumanMessage(content='Тебе нравится наш разговор?'),\n",
       " AIMessage(content='Да!')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=35,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Ты личный ассистент\"),\n",
    "    HumanMessage(content=\"Привет! Меня зовут Вася\"),\n",
    "    AIMessage(content=\"Привет!\"),\n",
    "    HumanMessage(content=\"Я люблю шоколадное мороженое\"),\n",
    "    AIMessage(content=\"Здорово\"),\n",
    "    HumanMessage(content=\"Сколько будет 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"Спасибо\"),\n",
    "    AIMessage(content=\"Пожалуйста!\"),\n",
    "    HumanMessage(content=\"Тебе нравится наш разговор?\"),\n",
    "    AIMessage(content=\"Да!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\n",
    "\n",
    "Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history:\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "O-gvXbxUZHpz",
    "outputId": "d0d570ce-2c35-4618-dd3f-7873951a90c5"
   },
   "source": [
    "<!--\n",
    "```python\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"Как меня зовут?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXt7vqJHZHpz"
   },
   "source": [
    "<!--\n",
    "Но если вы спросите информацию, которая находится в последних десяти сообщениях, модель покажет, что все еще ее помнит.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idgpxg_lZHp0",
    "outputId": "1dedfaf2-dd54-465a-9f2a-f950bbec0019"
   },
   "source": [
    "<!--\n",
    "```py\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxK3KabiZHp0"
   },
   "source": [
    "<!--\n",
    "Теперь оберните полученный код в `with_message_history`.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbYzqoZtZHp0"
   },
   "source": [
    "<!--\n",
    "```py\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc20\"}}\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAqqM36AZHp0",
    "outputId": "9968833e-b594-486e-9d90-469eba1691d2"
   },
   "source": [
    "<!--\n",
    "```py\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSBLXBlvZHp1"
   },
   "source": [
    "<!--\n",
    "Теперь история чата содержит два новых сообщения.\n",
    "Это значит, что еще больше информации, которая ранее хранилась истории разговоров, теперь недоступна.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUjYcoreZHp1",
    "outputId": "607e2f1c-ccef-4291-e338-61a67dcac16f"
   },
   "source": [
    "<!--\n",
    "```py\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koruQEyMZHp1"
   },
   "source": [
    "<!--\n",
    "Если вы посмотрите на LangSmith, вы сможете увидеть, что происходит под капотом, в [трассировке LangSmith](https://smith.langchain.com/public/fa6b00da-bcd8-4c1c-a799-6b32a3d62964/r).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySLF_jNmZHp1"
   },
   "source": [
    "## Потоковая передача\n",
    "\n",
    "Потоковая передача ответа — важная составляющая хорошего пользовательского опыта при разработке чат-ботов.\n",
    "Языковые модели могут долго генерировать ответ, поэтому для повышения отзывчивости большинство приложений обрабатывает и отображает каждый токен по мере его генерации.\n",
    "Это позволяет пользователю видеть прогресс .\n",
    "\n",
    "Для работы с потоковой передачей все цепочки предоставляют метод `.stream`, в том числе и те, что используют историю сообщений.\n",
    "Используйте его, чтобы получить потоковый ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "wiA3XvewZHp2",
    "outputId": "b8bad13e-fb80-411e-fe88-2b03f6e0d939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little boy named Jack who lived in a small village with his family. One day, he decided to explore the nearby forest and stumbled upon an old oak tree. Upon closer inspection, he discovered a hidden door leading to a magical underground world filled with fairies and elves. They welcomed him warmly and taught him many things about their world, including how to fly on a magic carpet. When it was time for Jack to return home, they gave him a special gift - a golden amulet that would| protect| him| from| harm|.| From| that| day| forward|,| Jack| always| wore| the| am|u|let| and| never| forgot| his| advent|ure| in| the| mag|ical| under|ground| world|.||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc19\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Привет! Я Вася. Расскажи историю из 100 слов\")\n",
    "        ],\n",
    "        \"language\": \"Английский\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OijNp7EPZHp2"
   },
   "source": [
    "## Смотрите также\n",
    "\n",
    "- [Создание разговорного приложения с RAG](/ru/gigachain/tutorials/qa_chat_history) — позволяет чат-боту использовать внешние источники данных.\n",
    "- [Агенты](/docs/tutorials/agents) — чат-боты, которые могут выполнять действия.\n",
    "- [Работа с потоковой передачей](/ru/gigachain/how_to/streaming).\n",
    "- [Работа с историей сообщений](/ru/gigachain/how_to/message_history)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
