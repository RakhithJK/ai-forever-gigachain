{
   "cells": [
      {
         "cell_type": "raw",
         "metadata": {
            "vscode": {
               "languageId": "raw"
            }
         },
         "source": [
            "---\n",
            "sidebar_position: 1\n",
            "keywords: [conversationchain]\n",
            "---"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Разработка чат-бота\n",
            "\n",
            "Раздел содержит пример разработки чат-бота на основе LLM.\n",
            "Этот чат-бот сможет вести беседу и запоминать предыдущие действия пользователя.\n",
            "\n",
            "В примере рассмотрен чат-бот, который для ведения беседы использует только языковую модель.\n",
            "Также существуют способы создания чат-ботов, которые могут вас заинтересовать:\n",
            "\n",
            "- [Conversational RAG](/docs/tutorials/qa_chat_history) — позволяет чат-боту использовать внешние источники данных.\n",
            "- [Агенты](/docs/tutorials/agents) — чат-боты, которые может выполнять действия.\n",
            "\n",
            "В этом разделе вы найдете базовую информацию о разработке чат-ботов, которая будет полезна при работе с приведенными выше разделами.\n",
            "Но если нужно вы можете сразу начать с более сложных чат-ботов.\n",
            "\n",
            "## Основные понятия\n",
            "\n",
            "Основные компоненты, с которыми вы будете работать:\n",
            "\n",
            "- [`Чат-модели`](/docs/concepts/#chat-models). Чат-боты работают с сообщениями, а не необработанным текстом. Поэтому для разработки лучше подходят для чат-модели, а не текстовые LLM.\n",
            "- [`Шаблоны промптов`](/docs/concepts/#prompt-templates), которые упрощают процесс создания промптов, объединяющих стандартные сообщения, ввод пользователя, историю чатов и, при необходимости, дополнительный извлеченный контекст.\n",
            "- [`История чата`](/docs/concepts/#chat-history), которая позволяет чат-боту \"запоминать\" прошлые взаимодействия и учитывать их при ответе на последующие вопросы.\n",
            "\n",
            "<!--\n",
            "- Отладка и трассировка вашего приложения с помощью [LangSmith](/docs/concepts/#langsmith)\n",
            "-->\n",
            "\n",
            "На наглядном примере вы узнаете, как объединить вышеупомянутые компоненты для создания мощного развитого чат-бота.\n",
            "\n",
            "## Подготовка к разработке\n",
            "\n",
            "### Jupyter-блокноты\n",
            "\n",
            "Это руководство, как и большинство других в документации, использует [Jupyter-блокноты](https://jupyter.org/). Они отлично подходят для изучения работы с LLM-системами, так как предоставляют интерактивную среду для работы с руководствами и позволяют работать с непредвиденными ситуациями: недоступностью API, нетипичным выводом и другими.\n",
            "\n",
            "Подробнее об установке jupyter — в [официальной документации](https://jupyter.org/install).\n",
            "\n",
            "### Установка\n",
            "\n",
            "Для установки GigaChain выполните команды:\n",
            "\n",
            "```{=mdx}\n",
            "import Tabs from '@theme/Tabs';\n",
            "import TabItem from '@theme/TabItem';\n",
            "import CodeBlock from \"@theme/CodeBlock\";\n",
            "\n",
            "<Tabs>\n",
            "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
            "    <CodeBlock language=\"bash\">pip install langchain</CodeBlock>\n",
            "  </TabItem>\n",
            "  <TabItem value=\"conda\" label=\"Conda\">\n",
            "    <CodeBlock language=\"bash\">conda install langchain -c conda-forge</CodeBlock>\n",
            "  </TabItem>\n",
            "</Tabs>\n",
            "```\n",
            "\n",
            "\n",
            "Подробнее об установке — в разделе [Установка](https://developers.sber.ru/docs/ru/gigachain/get-started/installation).\n",
            "\n",
            "<!--\n",
            "### LangSmith\n",
            "\n",
            "Многие приложения, которые вы создаете с помощью LangChain, будут содержать несколько шагов с многократными вызовами LLM.\n",
            "По мере усложнения этих приложений становится важно иметь возможность инспектировать, что именно происходит внутри вашей цепочки или агента.\n",
            "Лучший способ сделать это — с помощью [LangSmith](https://smith.langchain.com).\n",
            "\n",
            "После регистрации по ссылке выше, убедитесь, что вы установили переменные среды для начала ведения журнала трассировок:\n",
            "\n",
            "```shell\n",
            "export LANGCHAIN_TRACING_V2=\"true\"\n",
            "export LANGCHAIN_API_KEY=\"...\"\n",
            "```\n",
            "\n",
            "Или, если вы работаете в блокноте, вы можете установить их с помощью:\n",
            "\n",
            "```python\n",
            "import getpass\n",
            "import os\n",
            "\n",
            "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
            "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
            "```\n",
            "-->\n",
            "\n",
            "## Быстрый старт\n",
            "\n",
            "Сначала ознакомьтесь как использовать языковую модель отдельно.\n",
            "GigaChain поддерживает различные языковые модели, которые могут заменять друг друга.\n",
            "\n",
            "```{=mdx}\n",
            "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
            "\n",
            "<ChatModelTabs openaiParams={`model=\"gpt-3.5-turbo\"`} />\n",
            "```"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "# | output: false\n",
            "# | echo: false\n",
            "\n",
            "from langchain_openai import ChatOpenAI\n",
            "\n",
            "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Сначала попробуйте использовать модель напрямую.\n",
            "`ChatModel` — это экземпляры Runnable-интерфейса GigaChain, что означает, что для работы они они предоставляют стандартный интерфейс.\n",
            "Для простого вызова модели, вы можете передать список сообщений в метод `.invoke`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "AIMessage(content='Hello Bob! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 12, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d939617f-0c3b-45e9-a93f-13dafecbd4b5-0', usage_metadata={'input_tokens': 12, 'output_tokens': 10, 'total_tokens': 22})"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from langchain_core.messages import HumanMessage\n",
            "\n",
            "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Сама по себе модель не имеет понятия состояния.\n",
            "Это можно увидеть, если задать модели дополнительный вопрос:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "AIMessage(content=\"I'm sorry, I don't have access to personal information unless you provide it to me. How may I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 12, 'total_tokens': 38}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-47bc8c20-af7b-4fd2-9345-f0e9fdf18ce3-0', usage_metadata={'input_tokens': 12, 'output_tokens': 26, 'total_tokens': 38})"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "model.invoke([HumanMessage(content=\"What's my name?\")])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<!--\n",
            "Давайте взглянем на пример [трассировки LangSmith](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r)\n",
            "\n",
            "Мы видим, что модель не учитывает предыдущий ход разговора и не может ответить на вопрос. Это делает чат-бот крайне неудобным!\n",
            "-->\n",
            "\n",
            "Чтобы обойти это ограничение, передайте всю историю разговора в модель:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "AIMessage(content='Your name is Bob. How can I help you, Bob?', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 35, 'total_tokens': 48}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9f90291b-4df9-41dc-9ecf-1ee1081f4490-0', usage_metadata={'input_tokens': 35, 'output_tokens': 13, 'total_tokens': 48})"
                  ]
               },
               "execution_count": 4,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from langchain_core.messages import AIMessage\n",
            "\n",
            "model.invoke(\n",
            "    [\n",
            "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
            "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
            "        HumanMessage(content=\"What's my name?\"),\n",
            "    ]\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Качество ответа модели заметно возросло.\n",
            "\n",
            "Работа с историей сообщений — это техника, которая лежит в основе способности чат-бота вести разговор.\n",
            "Ниже вы узнаете как лучше ее реализовать."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## История сообщений\n",
            "\n",
            "Чтобы модель сохраняла состояние, вы можете обернуть ее в класс Message History.\n",
            "Класс будет отслеживать входные и выходные данные модели и сохранять их в хранилище данных.\n",
            "При повторных обращениях сообщения модели будут загружаться из хранилища и передаваться в цепочку в качестве части входных данных.\n",
            "\n",
            "Пример ниже использует хранилище истории сообщений, доступное в пакете `gigachain-community`.\n",
            "Убедитесь, что установили его:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "# ! pip install langchain_community"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "После этого вы можем импортировать соответствующие классы и настроить цепочку, которая обернет модель и добавит историю сообщений.\n",
            "Самой важной частью здесь является функция, которую мы передаем в качестве `get_session_history`.\n",
            "Эта функция должна принимать `session_id` и возвращать объект Message History.\n",
            "Параметр `session_id` используется для различения отдельных разговоров и передается как часть конфигурационной переменной при вызове новой цепочки."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_community.chat_message_histories import ChatMessageHistory\n",
            "from langchain_core.chat_history import BaseChatMessageHistory\n",
            "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
            "\n",
            "store = {}\n",
            "\n",
            "\n",
            "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
            "    if session_id not in store:\n",
            "        store[session_id] = ChatMessageHistory()\n",
            "    return store[session_id]\n",
            "\n",
            "\n",
            "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Создайте переменную `config`, которая будет содержать дополнительные данные, полезные для вызова цепочки.\n",
            "В приведенном примере вам нужно передавать в этой переменной `session_id`.\n",
            "Переменную нужно передавать при каждом вызове runnable."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Hi Bob! How can I assist you today?'"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = with_message_history.invoke(\n",
            "    [HumanMessage(content=\"Hi! I'm Bob\")],\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Your name is Bob. How can I help you today, Bob?'"
                  ]
               },
               "execution_count": 9,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = with_message_history.invoke(\n",
            "    [HumanMessage(content=\"What's my name?\")],\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Теперь ваш чат-бот запоминает информацию.\n",
            "Если вы измените переменную config, чтобы сослаться на другой `session_id`, то увидите, что разговор начнется заново."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "\"I'm sorry, I cannot determine your name as I am an AI assistant and do not have access to that information.\""
                  ]
               },
               "execution_count": 10,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
            "\n",
            "response = with_message_history.invoke(\n",
            "    [HumanMessage(content=\"What's my name?\")],\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "При этом вы всегда можете вернуться к первоначальному разговору (так как он сохраняется его в базе данных)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Your name is Bob. How can I assist you today, Bob?'"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
            "\n",
            "response = with_message_history.invoke(\n",
            "    [HumanMessage(content=\"What's my name?\")],\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Таким образом, ваш чат-бот сможет вести беседы с множеством пользователей.\n",
            "\n",
            "Ниже вы узнаете как развить и персонализировать данные, которые сохраняет чат-бот, с помощью шаблона промпта."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Шаблоны промптов\n",
            "\n",
            "Шаблоны промптов помогают преобразовать необработанную информацию от пользователя в формат, с которым может работать LLM.\n",
            "В данном случае необработанный ввод представляет собой сообщение, которое передается в LLM.\n",
            "Это сообщение можно усложнить.\n",
            "Сначала добавьте в него системное сообщение с набором собственных инструкций (но все еще принимая сообщения в качестве ввода).\n",
            "Затем дополните сообщения вспомогательными входными данными.\n",
            "\n",
            "Для добавления системного сообщения создайте экземпляр `ChatPromptTemplate`.\n",
            "Чтобы передать все сообщения используйте `MessagesPlaceholder`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
            "\n",
            "prompt = ChatPromptTemplate.from_messages(\n",
            "    [\n",
            "        (\n",
            "            \"system\",\n",
            "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
            "        ),\n",
            "        MessagesPlaceholder(variable_name=\"messages\"),\n",
            "    ]\n",
            ")\n",
            "\n",
            "chain = prompt | model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "В результате тип входных данных изменится — вместо передачи списка сообщений вы теперь передаете словарь с ключом `messages`, который содержит список сообщений."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Hello Bob! How can I assist you today?'"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! I'm bob\")]})\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Теперь вы можете обернуть полученный код в объект истории сообщений `with_message_history`, созданный ранее."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [],
         "source": [
            "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [],
         "source": [
            "config = {\"configurable\": {\"session_id\": \"abc5\"}}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Hello, Jim! How can I assist you today?'"
                  ]
               },
               "execution_count": 16,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = with_message_history.invoke(\n",
            "    [HumanMessage(content=\"Hi! I'm Jim\")],\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Your name is Jim.'"
                  ]
               },
               "execution_count": 17,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = with_message_history.invoke(\n",
            "    [HumanMessage(content=\"What's my name?\")],\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Усложните полученный промпт.\n",
            "Предположим, что шаблон промпта теперь выглядит примерно так:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [],
         "source": [
            "prompt = ChatPromptTemplate.from_messages(\n",
            "    [\n",
            "        (\n",
            "            \"system\",\n",
            "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
            "        ),\n",
            "        MessagesPlaceholder(variable_name=\"messages\"),\n",
            "    ]\n",
            ")\n",
            "\n",
            "chain = prompt | model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "В примере выше в промпт добавлена новая переменная `language`.\n",
            "Теперь вы можем вызвать цепочку и передать язык на свой выбор выбор."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'¡Hola, Bob! ¿En qué puedo ayudarte hoy?'"
                  ]
               },
               "execution_count": 19,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = chain.invoke(\n",
            "    {\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"language\": \"Spanish\"}\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Оберните полученную цепочку в класс `with_message_history`.\n",
            "Теперь, поскольку входные данные содержать несколько ключей, вам нужно указать правильный ключ для сохранения истории чата."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [],
         "source": [
            "with_message_history = RunnableWithMessageHistory(\n",
            "    chain,\n",
            "    get_session_history,\n",
            "    input_messages_key=\"messages\",\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [],
         "source": [
            "config = {\"configurable\": {\"session_id\": \"abc11\"}}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'¡Hola Todd! ¿En qué puedo ayudarte hoy?'"
                  ]
               },
               "execution_count": 22,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = with_message_history.invoke(\n",
            "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"Spanish\"},\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Tu nombre es Todd.'"
                  ]
               },
               "execution_count": 23,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = with_message_history.invoke(\n",
            "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Spanish\"},\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<!--\n",
            "Чтобы лучше понять, что происходит внутри, ознакомьтесь с [этой трассировкой LangSmith](https://smith.langchain.com/public/f48fabb6-6502-43ec-8242-afc352b769ed/r).\n",
            "-->"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Управление историей разговоров\n",
            "\n",
            "При разработке чат-бота рано или поздно вам понадобится управлять историей разговоров.\n",
            "Это связанно с тем, что в определенный момент количество и содержимое сообщений превзойдут размер контекстного окна LLM.\n",
            "Поэтому вам нужно добавить этап, на котором будет ограничиваться размер передаваемых сообщений.\n",
            "\n",
            ":::caution\n",
            "\n",
            "При этом, этот этап должен срабатывать до шаблона промпта, но после загрузки предыдущих сообщений из Message History.\n",
            "\n",
            ":::\n",
            "\n",
            "Для этого перед промптом вы можете добавить простой шаг, который изменяет ключ `messages` соответствующим образом, а затем обернуть полученную цепочку в класс Message History.\n",
            "Сначала определите функцию, которая будет изменять передаваемые сообщения.\n",
            "Пусть она выбирает `k` самых последних сообщений.\n",
            "Затем вы можете создать новую цепочку, добавив их в начало."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[SystemMessage(content=\"you're a good assistant\"),\n",
                     " HumanMessage(content='whats 2 + 2'),\n",
                     " AIMessage(content='4'),\n",
                     " HumanMessage(content='thanks'),\n",
                     " AIMessage(content='no problem!'),\n",
                     " HumanMessage(content='having fun?'),\n",
                     " AIMessage(content='yes!')]"
                  ]
               },
               "execution_count": 24,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from langchain_core.messages import SystemMessage, trim_messages\n",
            "\n",
            "trimmer = trim_messages(\n",
            "    max_tokens=65,\n",
            "    strategy=\"last\",\n",
            "    token_counter=model,\n",
            "    include_system=True,\n",
            "    allow_partial=False,\n",
            "    start_on=\"human\",\n",
            ")\n",
            "\n",
            "messages = [\n",
            "    SystemMessage(content=\"you're a good assistant\"),\n",
            "    HumanMessage(content=\"hi! I'm bob\"),\n",
            "    AIMessage(content=\"hi!\"),\n",
            "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
            "    AIMessage(content=\"nice\"),\n",
            "    HumanMessage(content=\"whats 2 + 2\"),\n",
            "    AIMessage(content=\"4\"),\n",
            "    HumanMessage(content=\"thanks\"),\n",
            "    AIMessage(content=\"no problem!\"),\n",
            "    HumanMessage(content=\"having fun?\"),\n",
            "    AIMessage(content=\"yes!\"),\n",
            "]\n",
            "\n",
            "trimmer.invoke(messages)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Теперь если вы создадите список сообщений длиной более 10 сообщений, вы увидите, что модель больше не запоминает информацию из первых сообщений."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "\"I'm sorry, but I don't have access to your personal information. How can I assist you today?\""
                  ]
               },
               "execution_count": 25,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "from operator import itemgetter\n",
            "\n",
            "from langchain_core.runnables import RunnablePassthrough\n",
            "\n",
            "chain = (\n",
            "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
            "    | prompt\n",
            "    | model\n",
            ")\n",
            "\n",
            "response = chain.invoke(\n",
            "    {\n",
            "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
            "        \"language\": \"English\",\n",
            "    }\n",
            ")\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Но если вы спросите информацию, которая находится в последних десяти сообщениях, модель покажет, что все еще ее помнит."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 26,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'You asked \"what\\'s 2 + 2?\"'"
                  ]
               },
               "execution_count": 26,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = chain.invoke(\n",
            "    {\n",
            "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
            "        \"language\": \"English\",\n",
            "    }\n",
            ")\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Теперь оберните полученный код в `with_message_history`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 27,
         "metadata": {},
         "outputs": [],
         "source": [
            "with_message_history = RunnableWithMessageHistory(\n",
            "    chain,\n",
            "    get_session_history,\n",
            "    input_messages_key=\"messages\",\n",
            ")\n",
            "\n",
            "config = {\"configurable\": {\"session_id\": \"abc20\"}}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "\"I'm sorry, I don't have access to that information. How can I assist you today?\""
                  ]
               },
               "execution_count": 28,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = with_message_history.invoke(\n",
            "    {\n",
            "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
            "        \"language\": \"English\",\n",
            "    },\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Теперь история чата содержит два новых сообщения.\n",
            "Это значит, что еще больше информации, которая ранее хранилась истории разговоров, теперь недоступна."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 29,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "\"You haven't asked a math problem yet. Feel free to ask any math-related question you have, and I'll be happy to help you with it.\""
                  ]
               },
               "execution_count": 29,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = with_message_history.invoke(\n",
            "    {\n",
            "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
            "        \"language\": \"English\",\n",
            "    },\n",
            "    config=config,\n",
            ")\n",
            "\n",
            "response.content"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<!--\n",
            "Если вы посмотрите на LangSmith, вы сможете увидеть, что происходит под капотом, в [трассировке LangSmith](https://smith.langchain.com/public/fa6b00da-bcd8-4c1c-a799-6b32a3d62964/r).\n",
            "-->"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Потоковая передача\n",
            "\n",
            "Потоковая передача — одна из важных составляющих пользовательского опыта для чат-ботов.\n",
            "Большие языковые модели могут долго отвечать, поэтому для повышения отзывчивости большинство приложений обрабатывает и отображает каждый токен по мере его генерации.\n",
            "Это позволяет пользователю видеть прогресс.\n",
            "\n",
            "Для работы с потоковой передачей все цепочки предоставляют метод `.stream`, и те, что используют историю сообщений, не исключение.\n",
            "Используйте этот метод, чтобы получить потоковый ответ."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 30,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "|Hi| Todd|!| Sure|,| here|'s| a| joke| for| you|:| Why| couldn|'t| the| bicycle| find| its| way| home|?| Because| it| lost| its| bearings|!| 😄||"
               ]
            }
         ],
         "source": [
            "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
            "for r in with_message_history.stream(\n",
            "    {\n",
            "        \"messages\": [HumanMessage(content=\"hi! I'm todd. tell me a joke\")],\n",
            "        \"language\": \"English\",\n",
            "    },\n",
            "    config=config,\n",
            "):\n",
            "    print(r.content, end=\"|\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Смотрите также\n",
            "\n",
            "- [Conversational RAG](/docs/tutorials/qa_chat_history) — позволяет чат-боту использовать внешние источники данных.\n",
            "- [Агенты](/docs/tutorials/agents) — чат-боты, которые может выполнять действия.\n",
            "- [Работа с потоковой передача](/docs/how_to/streaming) — потоковая передача очень важна для чат-приложений.\n",
            "- [Работа с историей сообщений](/docs/how_to/message_history) — раздел с подробной информацией о работе с историей сообщений."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.6"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
