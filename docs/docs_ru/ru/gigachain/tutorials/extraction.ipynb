{
 "cells": [
  {
   "cell_type": "raw",
   "id": "df29b30a-fd27-4e08-8269-870df5631f9e",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28530a6-ddfd-49c0-85dc-b723551f6614",
   "metadata": {},
   "source": [
    "# Создание цепочки извлечения данных\n",
    "\n",
    "Раздел содержит руководствопо разработке цепочки для извлечения структурированной информации из неструктурированного текста.\n",
    "\n",
    ":::important\n",
    "\n",
    "Примеры в разделе работают только с моделями, которые поддерживают вызов функций/инструментов.\n",
    "\n",
    ":::\n",
    "\n",
    "## В этом руководстве\n",
    "\n",
    "Примеры в разделе показывают как:\n",
    "- использовать [чат-модели](/docs/concepts/#chat-models)\n",
    "- вызывать [функции/инструменты](/docs/concepts/#function-tool-calling)\n",
    "<!--\n",
    "- Отладка и трассировка вашего приложения с помощью [LangSmith](/docs/concepts/#langsmith)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412def2-38e3-4bd0-bbf0-fb09ff9e5985",
   "metadata": {},
   "source": [
    "## Подготовка\n",
    "\n",
    "### Jupyter\n",
    "\n",
    "Как и многие другие руководства в документации GigaChain это руководство основано на [блокноте Jupyter](https://jupyter.org/).\n",
    "Блокноты предоставляют интерактивность, которая значительно упрощает изучение работы с LLM.\n",
    "\n",
    "Об установке Jupyter читайте в [официальной документации](https://jupyter.org/install).\n",
    "\n",
    "### Установка Зависимостей\n",
    "\n",
    "В примере используются модели GigaChat (чат-модель и эмбеддинги), а также векторное хранилище Chroma, но в своем приложении вы можете использовать любой из доступных компонентов.\n",
    "\n",
    "Для работы с примером нужно установить пакеты:\n",
    "\n",
    "```python\n",
    "%pip install --upgrade --quiet  gigachain langchainhub chromadb bs4\n",
    "```\n",
    "\n",
    "\n",
    "For more details, see our [Installation guide](/docs/how_to/installation).\n",
    "\n",
    "<!--\n",
    "### LangSmith\n",
    "\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\n",
    "As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\n",
    "The best way to do this is with [LangSmith](https://smith.langchain.com).\n",
    "\n",
    "After you sign up at the link above, make sure to set your environment variables to start logging traces:\n",
    "\n",
    "```shell\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=\"...\"\n",
    "```\n",
    "\n",
    "Or, if in a notebook, you can set them with:\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6b970-2ea3-4192-951e-21237212b359",
   "metadata": {},
   "source": [
    "## Определение схемы данных\n",
    "\n",
    "В первую очередь вам нужно описать, какую информацию вы хотите извлечь из текста.\n",
    "\n",
    "В примере ниже с помощью Pydantic-модели задана схема образца персональных данных, которые нужно извлечь с помощью модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c141084c-fb94-4093-8d6a-81175d688e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Док-строка для сущности Person.\n",
    "    # Эта док-строка передается в модель в качестве описания схемы Person.\n",
    "    # Она может помочь улучшить результаты извлечения.\n",
    "\n",
    "    # Обратите внимание:\n",
    "    # 1. Каждое поле является необязательным (`optional`). Это позволяет модели отказаться от его извлечения.\n",
    "    # 2. Каждое поле имеет описание (`description`). Это описание использует модель.\n",
    "    # Хорошее описание может помочь улучшить результаты извлечения данных.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        default=None, description=\"The color of the person's hair if known\"\n",
    "    )\n",
    "    height_in_meters: Optional[str] = Field(\n",
    "        default=None, description=\"Height measured in meters\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248dd54-e36d-435a-b154-394ab4ed6792",
   "metadata": {},
   "source": [
    "При определении схемы придерживайтесь следующих рекомендаций:\n",
    "\n",
    "- Документируйте как атрибуты, так и саму схему. Эта информация передается в модель и используется для повышения качества извлечения данных.\n",
    "- Не вынуждайте модель придумывать данные. В приведенном выше примере все атрибуты необязательны (параметр `Optional`). Таким образом модель сможет вернуть `None`, если в тексте нет подходящих для извлечения данных.\n",
    "\n",
    "## Экстрактор\n",
    "\n",
    "Создайте экстрактор информации, используя заданную выше схему данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e490f6-35ad-455e-8ae4-2bae021583ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Определите собственный промпта, чтобы предоставить модели инструкции и другой дополнительный контекст.\n",
    "# 1) Вы можете повысить качесво извлечения, если укажете в шаблоне промпа образцы ожидаемых результатов\n",
    "# 2) Задайте дополнительные параметры, чтобы учитывать контекст\n",
    "# (например, добавьте метаданные о документе, из которого был извлечен текст.)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        # О том как использовать примеры для повышения качетсва результатов\n",
    "        # вы можете узнать в соответсвующем руководстве.\n",
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832bf6a1-8e0c-4b6a-aa37-12fe9c42a6d9",
   "metadata": {},
   "source": [
    "Для запуска примера используйте модель, которая поддерживает вызов функций/инструментов.\n",
    "\n",
    "Вам подойдет любая модель GigaChat, предназначенная для генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d846a6-d5cb-4009-ac19-61e3aac0177e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonchase/workplace/langchain/libs/core/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The method `ChatMistralAI.with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "\n",
    "runnable = prompt | llm.with_structured_output(schema=Person)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23582c0b-00ed-403f-a10e-3aeabf921f12",
   "metadata": {},
   "source": [
    "Проверьте работоспособность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13165ac8-a1dc-44ce-a6ed-f52b577473e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
    "runnable.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c493d-f9dc-4236-8da9-50f6919f5710",
   "metadata": {},
   "source": [
    "<!--\n",
    ":::important\n",
    "\n",
    "Извлечение является генеративным 🤯\n",
    "\n",
    "LLMs являются генеративными моделями, поэтому они могут делать некоторые довольно крутые вещи, например, правильно извлекать рост человека в метрах, даже если он был предоставлен в футах!\n",
    ":::\n",
    "\n",
    "Мы можем увидеть трассировку LangSmith здесь: https://smith.langchain.com/public/44b69a63-3b3b-47b8-8a6d-61b46533f015/r\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c5ef0c-b8d1-4e12-bd0e-e2528de87fcc",
   "metadata": {},
   "source": [
    "## Извлечение нескольких сущностей\n",
    "\n",
    "Как правило вам будет нужно извлекать список сущностей, а не одну сущность.\n",
    "\n",
    "Для этого вы можете вкалдывать Pydantic-модели друг в друга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591a0c16-7a17-4883-91ee-0d6d2fdb265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Док-строка для сущности Person.\n",
    "    # Эта док-строка передается в модель в качестве описания схемы Person.\n",
    "    # Она может помочь улучшить результаты извлечения.\n",
    "\n",
    "    # Обратите внимание:\n",
    "    # 1. Каждое поле является необязательным (`optional`). Это позволяет модели отказаться от его извлечения.\n",
    "    # 2. Каждое поле имеет описание (`description`). Это описание использует модель.\n",
    "    # Хорошее описание может помочь улучшить результаты извлечения данных.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        default=None, description=\"The color of the person's hair if known\"\n",
    "    )\n",
    "    height_in_meters: Optional[str] = Field(\n",
    "        default=None, description=\"Height measured in meters\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "\n",
    "    # Создает схему, чтобы мы могли извлекать несколько сущностей.\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5cda33-fd7b-481e-956a-703f45e40e1d",
   "metadata": {},
   "source": [
    "<!--\n",
    ":::{.callout-important}\n",
    "Извлечение здесь может быть не идеальным. Пожалуйста, продолжайте читать, чтобы узнать, как использовать **Референсные примеры** для улучшения качества извлечения, и смотрите раздел **Руководства**!\n",
    ":::\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7062cc-1d1d-4a37-9122-509d1b87f0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Jeff', hair_color=None, height_in_meters=None), Person(name='Anna', hair_color=None, height_in_meters=None)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = prompt | llm.with_structured_output(schema=Data)\n",
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "runnable.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba1d770-bf4d-4de4-9e4f-7384872ef0dc",
   "metadata": {},
   "source": [
    ":::tip\n",
    "\n",
    "Схема, которая поддерживает извлечение нескольких сущностей, также позволяет модели не извлекать ни одной сущности, если в тексте нет соответствующей информации. В таком случае модель вернет список.\n",
    "\n",
    "Как правило это хорошо, так как позволяет указывать обязательные атрибуты для сущности без необходимости вынуждать модель обнаруживать эту сущность.\n",
    ":::\n",
    "\n",
    "<!--\n",
    "Трассировку LangSmith можно увидеть здесь: https://smith.langchain.com/public/7173764d-5e76-45fe-8496-84460bd9cdef/r\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a7455-7de6-4a6f-9772-0477ef65e3dc",
   "metadata": {},
   "source": [
    "## Смотрите также\n",
    "\n",
    "- [Добавление образцов извлеченных данных](/docs/how_to/extraction_examples)\n",
    "- [Обработка длинных текстов](/docs/how_to/extraction_long_text)\n",
    "- [Извлечение данных на основе парсинга](/docs/how_to/extraction_parse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
