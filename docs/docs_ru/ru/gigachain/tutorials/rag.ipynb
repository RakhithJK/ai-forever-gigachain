{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5630b0ca",
   "metadata": {
    "id": "5630b0ca"
   },
   "source": [
    "# Создание RAG-приложения (Retrieval Augmented Generation)\n",
    "\n",
    "Большие языковые модели (LLM) позволяют разрабатывать развитые вопросно-ответные приложения (или *Q&A приложения*), которые могут использовать для ответов заданные данные.\n",
    "В основе таких приложений лежит методика известная как «генерация, дополненная извлечением данных» (retrieval-augmented generation или RAG).\n",
    "\n",
    "В этом разделе приведен пример создания простого Q&A приложения на основе текстового источника данных.\n",
    "Здесь вы также найдете пример типичной Q&A-архитектуры и сможете ознакомиться с дополнительными материалами, которые рассматривают более продвинутые методики, применяемые при разработке вопросно-ответных приложений.\n",
    "\n",
    "## Что такое RAG?\n",
    "\n",
    "RAG — это методика, которая позволяет расширить знания LLM дополнительными данными.\n",
    "\n",
    "Большие языковые модели имеют представление о различных темах, но их знания ограничены общими данными, доступными на момент обучения.\n",
    "Если вам нужно, чтобы модель знала о какой-то специфической информации или информации, которая появилась после ее обучения, вам понадобится предоставить ей соответствующие данные.\n",
    "Предоставление таких данных и применение их в промпте это и есть RAG — генерация, дополненная извлечением данных.\n",
    "\n",
    "GigaChain предоставляет компоненты для разработки вопросно-ответных приложений и поддержки RAG-методики в целом.\n",
    "\n",
    ":::note\n",
    "\n",
    "В этом разделе приводится пример разработки Q&A приложения, которое использует неструктурированные данные.\n",
    "Пример использования RAG со структурированными данными — в разделе [Разработка Q&A приложения на основе SQL-данных](/docs/tutorials/sql_qa).\n",
    "\n",
    ":::\n",
    "\n",
    "## Архитектура RAG\n",
    "\n",
    "В общем случае RAG приложение включает два компонента:\n",
    "\n",
    "* Конвейер для загрузки данных из источника и их индексирование. Как правило, индексирование работает в автономном режиме.\n",
    "* Цепочка RAG, которая в реальном времени обрабатывает запрос пользователя, извлекает соответствующие данные из индекса и передает их модели.\n",
    "\n",
    "Процесс генерации ответа на основе необработанных данных можно представить следующим образом:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A(Индексирование)\n",
    "    B(Извлечение данных и генерация)\n",
    "    A -->  B\n",
    "```\n",
    "\n",
    "Индексирование включает этапы:\n",
    "\n",
    "1. Загрузка документов с помощью [DocumentLoaders](/docs/concepts/#document-loaders).\n",
    "2. Использование [разделителей текста](/docs/concepts/#text-splitters) для разбивки `Documents` на мелкие фрагменты. Это удобно как для индексации данных, так и для передачи их в модель, поскольку большие фрагменты труднее искать и использовать в рамках контекста модели.\n",
    "3. Сохранение данных с помощью [векторных хранилищ](/docs/concepts/#vectorstores) и создание [эмбеддингов](/docs/concepts/#embedding-models) для поиска.\n",
    "\n",
    "![index_diagram](../../../../static/img/rag_indexing.png)\n",
    "\n",
    "Извлечение данных и генерация включает этапы:\n",
    "\n",
    "1. Извлечение данных, которые соответствуют запросу пользователя, с помощью [ретривера](/docs/concepts/#retrievers).\n",
    "2. Передача промпта, включающего вопрос пользователя и извлеченные данные, в [чат-модель](/docs/concepts/#chat-models) / [LLM](/docs/concepts/#llms) для генерации ответа.\n",
    "\n",
    "![retrieval_diagram](../../../../static/img/rag_retrieval_generation.png)\n",
    "\n",
    "\n",
    "## Подготовка\n",
    "\n",
    "### Jupyter\n",
    "\n",
    "Как и многие другие руководства в документации GigaChain это руководство основано на [блокноте Jupyter](https://jupyter.org/).\n",
    "Блокноты предоставляют интерактивность, которая значительно упрощает изучение работы с LLM.\n",
    "\n",
    "Об установке Jupyter читайте в [официальной документации](https://jupyter.org/install).\n",
    "\n",
    "### Установка зависимостей\n",
    "\n",
    "В примере используются модели GigaChat (чат-модель и эмбеддинги), а также векторное хранилище Chroma, но в своем приложении вы можете использовать любой из доступных компонентов.\n",
    "\n",
    "Для работы с примером нужно установить пакеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e615b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade --quiet  gigachain-community gigachain-chroma bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896fcfa",
   "metadata": {},
   "source": [
    "Подробнее — в разделе [Установка](/ru/gigachain/get-started/installation).\n",
    "\n",
    "<!--\n",
    "### LangSmith\n",
    "\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\n",
    "As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\n",
    "The best way to do this is with [LangSmith](https://smith.langchain.com).\n",
    "\n",
    "After you sign up at the link above, make sure to set your environment variables to start logging traces:\n",
    "\n",
    "```shell\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=\"...\"\n",
    "```\n",
    "\n",
    "Or, if in a notebook, you can set them with:\n",
    "\n",
    "```python\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "```\n",
    "-->\n",
    "\n",
    "## Обзор решения\n",
    "\n",
    "Пример демонстрирует вопросно-ответное приложение, которое генерирует ответы на основе [первой страницы документации GigaChain](https://developers.sber.ru/docs/ru/gigachain/overview).\n",
    "\n",
    "Итоговое решение, которое включает конвейер индексации и цепочку RAG, может занимать всего около 20 строк кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6281ec7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "6281ec7b",
    "outputId": "5453b891-9e11-4cb7-ed68-ed81f7774d4a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'GigaChain — это библиотека Python, предназначенная для упрощения и автоматизации работы с нейросетевой моделью GigaChat и другими большими языковыми моделями (LLM). Она является адаптированной версией библиотеки LangChain, специально разработанной для работы с русским языком.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_models.gigachat import GigaChat\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = GigaChat(\n",
    "    credentials=\"<авторизационные_данные>\",\n",
    "    scope=\"GIGACHAT_API_PERS\",\n",
    "    model=\"GigaChat-Pro\",\n",
    "    verify_ssl_certs=False,\n",
    ")\n",
    "\n",
    "# Загрузка, разделение на части и индексация содержимого страницы.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://developers.sber.ru/docs/ru/gigachain/overview\",),\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=(\"theme-doc-markdown markdown\"))),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GigaChatEmbeddings(\n",
    "        credentials=\"<авторизационные_данные>\",\n",
    "        scope=\"GIGACHAT_API_PERS\",\n",
    "        verify_ssl_certs=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Извлечение данных и генерация с помощью релевантных фрагментов данных.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = load_prompt(\"lc://prompts/retrievers/rag.yaml\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Что такое GigaChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3d56d203",
   "metadata": {
    "id": "3d56d203"
   },
   "outputs": [],
   "source": [
    "# Очистка хранилища\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d51135",
   "metadata": {
    "id": "c9d51135"
   },
   "source": [
    "## Разбор решения\n",
    "\n",
    "Ниже описаны этапы разработки приведенного примера.\n",
    "\n",
    "## 1. Индексирование. Загрузка данных {#indexing-load}\n",
    "\n",
    "Cначала загружается содержимое страницы.\n",
    "Для этого используются загрузчики [DocumentLoaders](/docs/concepts#document-loaders), которые загружают данные из источника и возвращают список документов [`Documents`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html).\n",
    "Экземпляр `Document` — объект с полями:\n",
    "\n",
    "* `page_content` (str) — содержимое страницы;\n",
    "* `metadata` (dict) — метаданные.\n",
    "\n",
    "Код в примере загружает HTML по заданному адресу и преобразовывает его в текст с помощью компонента [WebBaseLoader](/docs/integrations/document_loaders/web_base), который использует библиотеки `urllib` и `BeautifulSoup`.\n",
    "Правила преобразования можно задать с помощью параметров парсера `BeautifulSoup` переданных в `bs_kwargs`.\n",
    "Подробнее об изменении параметров — в [документации BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup).\n",
    "Для представленного примера актуальны только HTML-теги с классом `theme-doc-markdown markdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f5ba0122-8c92-4895-b5ef-f03a634e3fdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5ba0122-8c92-4895-b5ef-f03a634e3fdf",
    "outputId": "5100dcf6-3530-400e-f26e-777ae523921a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3764"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Сохраняются только содержимое страницы документации.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=\"theme-doc-markdown markdown\")\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://developers.sber.ru/docs/ru/gigachain/overview\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5cf74be6-5f40-4f6d-8689-b6b42ced8b70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cf74be6-5f40-4f6d-8689-b6b42ced8b70",
    "outputId": "7f356b02-34a5-4c1e-937b-7a2eb79e3fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaChain (GigaChat SDK)Обновлено 24 мая 2024GigaChain — это библиотека Python, которая позволяет упростить и автоматизировать работу с нейросетевой моделью GigaChat и другими большими языковыми моделями (LLM). GigaChain является версией библиотеки LangChain, которая адаптирована для работы с русским языком. Библиотека GigaChain обратно совместима с LangChain, что позволяет использовать ее не только для работы с GigaChat, но и при работе с другими LLM в различных комбинациях.Подробная документац\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07845e7a",
   "metadata": {
    "id": "07845e7a"
   },
   "source": [
    "### Дополнительная информация\n",
    "\n",
    "`DocumentLoader` — объект, который загружает данные в виде списка документов `Documents`.\n",
    "\n",
    "* [Справка API](https://api.python.langchain.com/en/latest/document_loaders/langchain_core.document_loaders.base.BaseLoader.html) базового интерфейса.\n",
    "\n",
    "<!--\n",
    "* [Руководства по работе с `DocumentLoaders`](/docs/how_to#document-loaders).\n",
    "[Integrations](../../../docs/integrations/document_loaders/): 160+\n",
    "-->\n",
    "\n",
    "## 2. Индексирование. Разделение данных {#indexing-split}\n",
    "\n",
    "Размер загруженного документа превышает три тысячи символов. Текст такой длины может не умещаться в контекст модели.\n",
    "Даже если модель уместит в контексте всю страницу, у нее могут быть проблемы с поиском информации в большом объеме входных данных.\n",
    "\n",
    "Чтобы создать эмбеддинг из данных `Document` и сохранить его в векторном хранилище, документы нужно разделить на фрагменты.\n",
    "Это позволит упростить извлечение релевантных частей страницы на этапе выполнения.\n",
    "\n",
    "В примере документы делятся на фрагменты по 1000 символов с перекрытием в 200 символов между каждым фрагментом.\n",
    "Перекрытие помогает избежать разделения утверждения и важного контекста, связанного с ним.\n",
    "Для разделения текста используется [`RecursiveCharacterTextSplitter`](/docs/how_to/recursive_text_splitter), который рекурсивно делит документ с помощью общепринятых разделителей, таких как переводы строк `\\n`, до тех пор, пока каждый фрагмент не достигнет нужного размера.\n",
    "Используйте `RecursiveCharacterTextSplitter` для решения типичных задач, связанных с обработкой текста.\n",
    "\n",
    "Атрибут `add_start_index=True` используется для сохранения индекса символа, с которого начинается каждый фрагмент начального документа `Document`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6aa3f8c0-5113-4c36-9706-ee702407173a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aa3f8c0-5113-4c36-9706-ee702407173a",
    "outputId": "ae27f781-097a-43eb-cd52-63028177e777"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2257752c-bed2-4d57-be8e-d275bfe70ace",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2257752c-bed2-4d57-be8e-d275bfe70ace",
    "outputId": "138fff7d-f05e-4c52-f09a-86a17ff5ece4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "325fdc48-4a24-4645-9d08-0d22f5be5e13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "325fdc48-4a24-4645-9d08-0d22f5be5e13",
    "outputId": "909cc5e5-2574-4953-e433-5f13b44323b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://developers.sber.ru/docs/ru/gigachain/overview',\n",
       " 'start_index': 2404}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[3].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046d580",
   "metadata": {
    "id": "7046d580"
   },
   "source": [
    "### Дополнительная информация\n",
    "\n",
    "`TextSplitter` — объект, который разделяет список документов `Document` на более мелкие фрагменты. Подкласс `DocumentTransformer`.\n",
    "\n",
    "* Ознакомьтесь с [«контекстно-зависимыми разделителями»](/docs/how_to#text-splitters), которые сохраняют положение (контекст) каждого деления в исходном документе `Document`:\n",
    "  * [Код (python или js)](/docs/integrations/document_loaders/source_code)\n",
    "  * [Научные публикации](/docs/integrations/document_loaders/grobid)\n",
    "* [Справка API](https://api.python.langchain.com/en/latest/base/langchain_text_splitters.base.TextSplitter.html) базового интерфейса.\n",
    "\n",
    "`DocumentTransformer` — объект, который выполняет преобразование списка документов `Document`.\n",
    "* [Руководства по работе с `DocumentTransformers`](/docs/how_to#text-splitters).\n",
    "* [Справка API](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.transformers.BaseDocumentTransformer.html) базового интерфейса.\n",
    "\n",
    "<!--\n",
    "- [Integrations](../../../docs/integrations/document_transformers/)\n",
    "-->\n",
    "\n",
    "## 3. Индексирование. Хранение данных {#indexing-store}\n",
    "\n",
    "Теперь полученные фрагменты текста нужно проиндексировать, чтобы иметь возможность искать данные в процессе выполнения.\n",
    "Распространенным решением такой задачи является создание эмбеддингов для фрагментов и сохранение их в векторной базе данных (*векторном хранилище*).\n",
    "Поиск по полученным фрагментам выполняется на основе сходства эмбеддинга запроса и эмбеддингов в хранилище.\n",
    "Самая простая мера сходства — это косинусное сходство, при котором измеряется косинус угла между каждой парой эмбеддингов, представляющих собой высокоразмерные вектора.\n",
    "\n",
    "Для создания эмбеддингов с помощью `GigaChatEmbeddings` и сохранения их в векторном хранилище Chroma достаточно одной команды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0b44b41a-8b25-42ad-9e37-7baf82a058cd",
   "metadata": {
    "id": "0b44b41a-8b25-42ad-9e37-7baf82a058cd"
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=GigaChatEmbeddings(\n",
    "        credentials=\"<авторизационные_данные>\",\n",
    "        scope=\"GIGACHAT_API_PERS\",\n",
    "        verify_ssl_certs=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddc12e",
   "metadata": {
    "id": "dbddc12e"
   },
   "source": [
    "### Дополнительная информация\n",
    "\n",
    "`Embeddings` — обертка для работы с моделью, которая преобразует текст в эмбеддинги.\n",
    "\n",
    "* [Руководство по работе с эмбеддингами](/docs/how_to/embed_text).\n",
    "* [Справка API](https://api.python.langchain.com/en/latest/embeddings/langchain_core.embeddings.Embeddings.html) базового интерфейса.\n",
    "\n",
    "<!--\n",
    "- [Integrations](../../../docs/integrations/text_embedding/): 30+ integrations to choose from.\n",
    "-->\n",
    "\n",
    "`VectorStore` — обертка для работы с векторной базой данных, которая используется для хранения эмбеддингов и выполнения запросов.\n",
    "\n",
    "* [Руководства по работе с векторными хранилищами](/docs/how_to/vectorstores).\n",
    "* [Справка API](https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html) для базового интерфейса.\n",
    "\n",
    "<!--\n",
    "- [Integrations](../../../docs/integrations/vectorstores/): 40+ integrations to choose from.\n",
    "-->\n",
    "\n",
    "На этом завершается индексирование.\n",
    "К этому моменту в примере реализовано векторное хранилище, которое содержит фрагменты поста и может возвращать их в соответствии с запросом пользователя.\n",
    "\n",
    "## 4. Извлечение данных и генерация. Извлечение данных{#retrieval-and-generation-retrieve}\n",
    "\n",
    "Теперь добавьте логику приложения.\n",
    "Приложение должно уметь:\n",
    "\n",
    "1. Принимать вопрос пользователя;\n",
    "2. Искать документы, соответствующие этому вопросу;\n",
    "3. Передавать в модель вопрос и полученные документы;\n",
    "4. Возвращать ответ.\n",
    "\n",
    "Сначала определите логику поиска по документам.\n",
    "Для этого GigaChain предоставляет интерфейс [Retriever](/docs/concepts#retrievers/) — обертку для работы с индексом.\n",
    "Ретривер может возвращать подходящие документы `Documents` в ответ на строковый запрос.\n",
    "\n",
    "Наиболее распространенным типом `Retriever` является [VectorStoreRetriever](/docs/how_to/vectorstore_retriever) — объект, который использует возможности поиска по сходству в векторном хранилище для облегчения получения данных.\n",
    "Любой экземпляр `VectorStore` можно преобразовать в `Retriever` с помощью метода `VectorStore.as_retriever()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1a0d25f8-8a45-4ec7-b419-c36e231fde13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a0d25f8-8a45-4ec7-b419-c36e231fde13",
    "outputId": "f7f8d389-f18c-4fac-a14f-a491c5b100f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Что такое GigaChain?\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "58db0a6a-f1ad-4d28-acf8-98be9ed3c968",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58db0a6a-f1ad-4d28-acf8-98be9ed3c968",
    "outputId": "a0f82301-4a86-462f-c456-0dbfccc54f45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaChain (GigaChat SDK)Обновлено 24 мая 2024GigaChain — это библиотека Python, которая позволяет упростить и автоматизировать работу с нейросетевой моделью GigaChat и другими большими языковыми моделями (LLM). GigaChain является версией библиотеки LangChain, которая адаптирована для работы с русским языком. Библиотека GigaChain обратно совместима с LangChain, что позволяет использовать ее не только для работы с GigaChat, но и при работе с другими LLM в различных комбинациях.Подробная документация для GigaChain доступна в репозитории.GigaChain находится на ранней стадии разработки. Будьте осторожны при использовании SDK в своих проектах, так как не все компоненты оригинальной библиотеки проверены на совместимость с GigaChat.Большая часть документации представлена на английском языке и находится в процессе локализации.Назначение﻿SDK упростит интеграцию вашего приложения с нейросетевой моделью GigaChat и поможет в таких задачах, как:Работа с промптами.Включая управление промптами и их\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb602b0",
   "metadata": {
    "id": "8bb602b0"
   },
   "source": [
    "### Дополнительная информация\n",
    "\n",
    "Как правило, для извлечения данных используются векторные хранилища, но существуют и другие подходы.\n",
    "\n",
    "`Retriever` — объект, который возвращает документы `Document` на основе текстового запроса.\n",
    "\n",
    "* [Дополнительная документация](/docs/how_to#retrievers) по интерфейсу и встроенным методикам получения данных. Которые, среди прочего, включают:\n",
    "  * [`MultiQueryRetriever`](/docs/how_to/MultiQueryRetriever) генерирует вариации запроса, что позволяет повысить точность получаемых данных.\n",
    "  * [`MultiVectorRetriever`](/docs/how_to/multi_vector) генерирует вариации эмбеддингов, для повышения точности получаемых данных.\n",
    "  * [Self Query Retriever](/docs/how_to/self_queryy) фильтрует документы на основе метаданных при их получении из векторного хранилища.\n",
    "* [Справка API](https://api.python.langchain.com/en/latest/retrievers/langchain_core.retrievers.BaseRetriever.html) для базового интерфейса.\n",
    "\n",
    "<!--\n",
    "    -   `Max marginal relevance` selects for [relevance and diversity](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf) among the retrieved documents to avoid passing in duplicate context.\n",
    "-->\n",
    "<!--\n",
    "-   [Integrations](../../../docs/integrations/retrievers/): Integrations\n",
    "-->\n",
    "\n",
    "## 5. Извлечение данных и генерация. Генерация {#retrieval-and-generation-generate}\n",
    "\n",
    "Объедините полученный код в цепочку, которая:\n",
    "\n",
    "* принимает вопрос;\n",
    "* извлекает соответствующие документы;\n",
    "* создает промпт;\n",
    "* передает промпт в модель;\n",
    "* анализирует выходные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "iX1ww1umBgaR",
   "metadata": {
    "id": "iX1ww1umBgaR"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.gigachat import GigaChat\n",
    "\n",
    "llm = GigaChat(\n",
    "    credentials=\"<авторизационные_данные>\",\n",
    "    scope=\"GIGACHAT_API_PERS\",\n",
    "    verify_ssl_certs=False,\n",
    "    model=\"GigaChat-Pro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nmUndJCiBXWB",
   "metadata": {
    "id": "nmUndJCiBXWB"
   },
   "source": [
    "Используйте промпт для RAG, который хранится хабе промптов [GigaChain](https://github.com/ai-forever/gigachain/tree/master/hub/prompts/retrievers/rag.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ff01d415-7b0f-469d-bfda-b9cb672da611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff01d415-7b0f-469d-bfda-b9cb672da611",
    "outputId": "69751bd7-e934-4bd9-9d97-483890494a0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Твоя задача — отвечать на вопросы, чтобы помочь пользователю.\\nДля ответов используй данные, извлеченные из источника.\\nЕсли ты не знаешь ответа, просто скажи пользователю об этом.\\nОтвечай коротко и ясно.\\nТвой ответ должен быть не длиннее трех предложений.\\n\\nКонтекст: образец контекста\\n\\nВопрос: образец вопроса\\n\\nТвой ответ:')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"lc://prompts/retrievers/rag.yaml\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"образец контекста\", \"question\": \"образец вопроса\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2885ed99-31a0-4d7e-b9b0-af49c462caf4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2885ed99-31a0-4d7e-b9b0-af49c462caf4",
    "outputId": "593ded46-f79d-41b8-9d68-44c5ea64d468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Твоя задача — отвечать на вопросы, чтобы помочь пользователю.\n",
      "Для ответов используй данные, извлеченные из источника.\n",
      "Если ты не знаешь ответа, просто скажи пользователю об этом.\n",
      "Отвечай коротко и ясно.\n",
      "Твой ответ должен быть не длиннее трех предложений.\n",
      "\n",
      "Контекст: образец контекста\n",
      "\n",
      "Вопрос: образец вопроса\n",
      "\n",
      "Твой ответ:\n"
     ]
    }
   ],
   "source": [
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516200c",
   "metadata": {
    "id": "4516200c"
   },
   "source": [
    "Для создания цепочки используйте [Runnable-интерфейс LCEL](/docs/concepts#langchain-expression-language), который позволяет объединять компоненты и функции, получать потоковую, асинхронную и пакетную передачу данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d6820cf3-e14d-4275-bd00-aa1b8262b1ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6820cf3-e14d-4275-bd00-aa1b8262b1ae",
    "outputId": "3da52ec1-a6b1-4f69-9166-f760ec506ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaChain - это библиотека Python, разработанная для упрощения и автоматизации работы с нейросетевой моделью GigaChat и другими большими языковыми моделями (LLM). Она является адаптированной версией библиотеки LangChain, предназначенной для работы с русским языком. GigaChain предоставляет стандартные интерфейсы и интеграции для работы с различными компонентами, а также позволяет создавать цепочки и агентов."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"Что такое GigaChain?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dacf214-0803-46f1-960d-42336a545e39",
   "metadata": {
    "id": "3dacf214-0803-46f1-960d-42336a545e39"
   },
   "source": [
    "Разберем реализацию на LCEL, чтобы понять, что происходит.\n",
    "\n",
    "Каждый из компонентов (`retriever`, `prompt`, `llm` и других) является экземпляром [Runnable](/docs/concepts#langchain-expression-language).\n",
    "То есть они реализуют одни и те же методы, такие как `sync` и `async`, `.invoke`, `.stream` или `.batch`.\n",
    "Это позволяет соединять их с помощью оператора конвейера `|` в [RunnableSequence](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableSequence.html) — последовательность, которая также является Runnable-объектом.\n",
    "\n",
    "При обнаружении оператора `|` GigaChain автоматически приводит определенные объекты к Runnable.\n",
    "Здесь `format_docs` приводится к [RunnableLambda](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html), а словарь dict с `«context»` и `«question»` приводится к [RunnableParallel](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableParallel.html).\n",
    "Самое важное на этом этапе то, что каждый объект является Runnable\n",
    "\n",
    "На входе в `prompt` ожидается `dict` с ключами `«context»` и `«question»`.\n",
    "Поэтому первый элемент этой цепочки создает Runnable-объекты, которые вычисляют оба ключа из входного вопроса:\n",
    "\n",
    "* `retriever | format_docs` передает вопрос через ретривер, генерирующий объекты [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html), а затем в `format_docs` для генерации строк;\n",
    "* `RunnablePassthrough()` передает входной вопрос без изменений.\n",
    "\n",
    "Таким образом, если собрать цепочку вида:\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    ")\n",
    "```\n",
    "\n",
    "То вызов метода `chain.invoke(question)` создаст отформатированный промпт, готовый к выводу.\n",
    "\n",
    ":::tip\n",
    "\n",
    "Тестирование с помощью таких подцепочек упрощает разработку на LCEL.\n",
    "\n",
    ":::\n",
    "\n",
    "Работа цепочки завершается обращение к `llm`, которое запускает вывод.\n",
    "После чего вызывается `StrOutputParser()`, который извлекает строковое содержимое из выходного сообщения модели.\n",
    "\n",
    "### Встроенные цепочки\n",
    "\n",
    "При желании вы можете воспользоваться встроенными в GigaChain функциями, которые делают то же самое, что и описанная реализация на LCEL:\n",
    "\n",
    "* [`create_stuff_documents_chain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) определяет, как извлеченный контекст будет подаваться в промпт и LLM. Цепочка помещает в промпт весь найденный контекст без суммаризации или другой обработки. По большому счету она реализует описанную цепочку `rag_chain`, с входными ключами `context` и `input`. Функция генерирует ответ, используя извлеченный контекст и запрос.\n",
    "* [`create_retrieval_chain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html) добавляет шаг извлечения и распространяет извлеченный контекст по цепочке, предоставляя его вместе с окончательным ответом. Цепочка имеет входной ключ `input` и включает `input`, `context` и `answer` в свой вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e75bfe98-d9e4-4868-bae1-5811437d859b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e75bfe98-d9e4-4868-bae1-5811437d859b",
    "outputId": "39e19a69-a408-4efb-d8be-db0340c3481a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GigaChain — это библиотека Python, предназначенная для упрощения работы с нейросетевой моделью GigaChat и другими большими языковыми моделями. Она позволяет создавать цепочки и агентов, управлять промптами и работать с памятью.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"Твоя задача — отвечать на вопросы, чтобы помочь пользователю. \"\n",
    "    \"Для ответов используй данные, извлеченные из источника. \"\n",
    "    \"Если ты не знаешь ответа, просто скажи пользователю об этом. \"\n",
    "    \"Твой ответ должен быть не длиннее трех предложений.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"Что такое GigaChain?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe711ea-592b-44a1-89b3-cee33c81aca4",
   "metadata": {
    "id": "0fe711ea-592b-44a1-89b3-cee33c81aca4"
   },
   "source": [
    "#### Получение источников\n",
    "\n",
    "Зачастую в вопросно-ответных приложениях важно показать пользователю источники, использованные для генерации ответа.\n",
    "Встроенная цепочка `create_retrieval_chain` будет добавлять полученные источники в вывод, в ключе `«context»`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9d4cec1a-75d6-4479-929f-72cadb2dcde8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d4cec1a-75d6-4479-929f-72cadb2dcde8",
    "outputId": "70307aec-42e5-431e-f900-ca5a630fe112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='GigaChain (GigaChat SDK)Обновлено 24 мая 2024GigaChain — это библиотека Python, которая позволяет упростить и автоматизировать работу с нейросетевой моделью GigaChat и другими большими языковыми моделями (LLM). GigaChain является версией библиотеки LangChain, которая адаптирована для работы с русским языком. Библиотека GigaChain обратно совместима с LangChain, что позволяет использовать ее не только для работы с GigaChat, но и при работе с другими LLM в различных комбинациях.Подробная документация для GigaChain доступна в репозитории.GigaChain находится на ранней стадии разработки. Будьте осторожны при использовании SDK в своих проектах, так как не все компоненты оригинальной библиотеки проверены на совместимость с GigaChat.Большая часть документации представлена на английском языке и находится в процессе локализации.Назначение﻿SDK упростит интеграцию вашего приложения с нейросетевой моделью GigaChat и поможет в таких задачах, как:Работа с промптами.Включая управление промптами и их' metadata={'source': 'https://developers.sber.ru/docs/ru/gigachain/overview', 'start_index': 0}\n",
      "\n",
      "page_content='на основе дополненных данных включают в себя суммирование больших текстов и ответы на вопросы по заданным источникам.Пример — Ответы на вопросы по загруженным документамРабота с агентами.Агент — это LLM, которая принимает решение о дальнейшем действии, отслеживает его результат, и, с учетом результата, принимает новое решение. Процесс повторяется до завершения. GigaChain предоставляет стандартный интерфейс для работы с агентами, выбор агентов и примеры готовых агентов.Пример — Игра в стиле DnD с GPT-3.5 и GigaChat.Создание памяти.Память сохраняет состояние между вызовами цепочки или агента. GigaChain предоставляет стандартный интерфейс для создания памяти, коллекцию реализаций памяти и примеры цепочек и агентов, которые используют память.Состав﻿GigaChat SDK включает:Библиотеку GigaChain. Библиотека на Python содержит интерфейсы и интеграции для множества компонентов, базовую среду выполнения для объединения этих компонентов в цепочки и агенты, а также готовые реализации цепочек и' metadata={'source': 'https://developers.sber.ru/docs/ru/gigachain/overview', 'start_index': 1605}\n",
      "\n",
      "page_content='в процессе локализации.Назначение﻿SDK упростит интеграцию вашего приложения с нейросетевой моделью GigaChat и поможет в таких задачах, как:Работа с промптами.Включая управление промптами и их оптимизацию. GigaChain предоставляет универсальный интерфейс для всех LLM, а также стандартные инструменты для работы с ними.Создание цепочек.Цепочки — это последовательность вызовов к LLM или другим инструментам. GigaChain дает доступ к стандартному интерфейсу для создания цепочек, различным интеграциям с другими инструментами и готовым цепочкам для популярных приложений.Дополнение данных.Генерация с дополненными данными включает в себя специфические типы цепочек. Эти цепочки сначала получают данные от внешнего источника, а затем используют их в генерации ответа нейросети. Примеры генерации ответов на основе дополненных данных включают в себя суммирование больших текстов и ответы на вопросы по заданным источникам.Пример — Ответы на вопросы по загруженным документамРабота с агентами.Агент — это' metadata={'source': 'https://developers.sber.ru/docs/ru/gigachain/overview', 'start_index': 806}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in response[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd57618",
   "metadata": {
    "id": "7cd57618"
   },
   "source": [
    "### Дополнительная информация\n",
    "\n",
    "#### Выбор модели\n",
    "\n",
    "`ChatModel` — чат-модель, в основе которой лежит LLM. Объект принимает на вход последовательность сообщений и возвращает ответ модели в виде сообщения.\n",
    "\n",
    "* [Документация](/docs/how_to#chat-models);\n",
    "* [Справка API](https://api.python.langchain.com/en/latest/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) для базового интерфейса.\n",
    "\n",
    "<!--\n",
    "- [Integrations](../../../docs/integrations/chat/): 25+ integrations to choose from.\n",
    "-->\n",
    "\n",
    "`LLM` — LLM, которая принимает и возвращает строку.\n",
    "\n",
    "* [Документация](/docs/how_to#llms);\n",
    "* [Справка API](https://api.python.langchain.com/en/latest/language_models/langchain_core.language_models.llms.BaseLLM.html) для базового интерфейса.\n",
    "\n",
    "<!--\n",
    "- [Integrations](../../../docs/integrations/llms): 75+ integrations to choose from.\n",
    "-->\n",
    "\n",
    "Руководство по реализации [RAG с помощью локальных моделей](/docs/tutorials/local_rag).\n",
    "\n",
    "#### Настройка промпта\n",
    "\n",
    "В примере выше RAG-промпт загружается из [хаба промптов GigaChain](https://github.com/ai-forever/gigachain/blob/master/hub/prompts/retrievers/rag.yaml).\n",
    "Если нужно вы можете использовать собственный промпт:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2ac552b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "2ac552b6",
    "outputId": "280c4b3e-6325-428e-f5d7-37171414a1ad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'GigaChain - это библиотека Python, разработанная для упрощения и автоматизации работы с нейросетевой моделью GigaChat и другими большими языковыми моделями (LLM). Она является адаптированной версией библиотеки LangChain, предназначенной для работы с русским языком. GigaChain предоставляет стандартные интерфейсы и интеграции для работы с различными компонентами, а также инструменты для создания цепочек и агентов.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Твоя задача — отвечать на вопросы, чтобы помочь пользователю.\n",
    "Для ответов используй данные, извлеченные из источника.\n",
    "Если ты не знаешь ответа, просто скажи пользователю об этом.\n",
    "Отвечай коротко и ясно.\n",
    "Твой ответ должен быть не длиннее трех предложений.\n",
    "\n",
    "Контекст: {context}\n",
    "\n",
    "Вопрос: {question}\n",
    "\n",
    "Твой ответ:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Что такое GigaChain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4d779",
   "metadata": {
    "id": "82e4d779"
   },
   "source": [
    "## Смотрите также\n",
    "\n",
    "* [Получение исходных документов](/docs/how_to/qa_sources).\n",
    "* [Потоковая передача выходных данных и промежуточных шагов](/docs/how_to/streaming).\n",
    "* [Добавление истории чата](/docs/how_to/message_history)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
