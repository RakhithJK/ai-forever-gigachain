{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4becbd-238e-4c1d-a02d-08e61fbc3763",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Работа с историей сообщений\n",
    "\n",
    "Для добавления истории сообщений в некоторые цепочки можно использовать обертку `RunnableWithMessageHistory`.\n",
    "\n",
    "Так, обертку можно использовать при работе с Runnable-объектом, который принимает на вход:\n",
    "\n",
    "* последовательность экземпляров `BaseMessage`;\n",
    "* словар с полем, в котором можно передать последовательность экземпляров `BaseMessage`;\n",
    "* словарь полем, в котором можно передать последнее сообщение или несколько сообщений в формате строки, либо последовательность экземпляров `BaseMessage`. И отдельным полем, в котором можно историю сообщений.\n",
    "\n",
    "И возвращает на выходе:\n",
    "\n",
    "* строку, которая можно передать внутри экземпляра `AIMessage`;\n",
    "* последовательность экземпляров `BaseMessage`;\n",
    "* словарь с полем, содержащим последовательность экземпляров `BaseMessage`.\n",
    "\n",
    "Раздел содержит несколько примеров работы с истории сообщений.\n",
    "\n",
    "Первый пример демонстрирует экземпляр Runnable, который принимает на вход словарь и возвращает сообщение."
=======
    "# How to add message history\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "- [LangChain Expression Language (LCEL)](/docs/concepts/#langchain-expression-language)\n",
    "- [Chaining runnables](/docs/how_to/sequence/)\n",
    "- [Configuring chain parameters at runtime](/docs/how_to/configure)\n",
    "- [Prompt templates](/docs/concepts/#prompt-templates)\n",
    "- [Chat Messages](/docs/concepts/#message-types)\n",
    "\n",
    ":::\n",
    "\n",
    "Passing conversation state into and out a chain is vital when building a chatbot. The [`RunnableWithMessageHistory`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#langchain_core.runnables.history.RunnableWithMessageHistory) class lets us add message history to certain types of chains. It wraps another Runnable and manages the chat message history for it.\n",
    "\n",
    "Specifically, it can be used for any Runnable that takes as input one of:\n",
    "\n",
    "* a sequence of [`BaseMessages`](/docs/concepts/#message-types)\n",
    "* a dict with a key that takes a sequence of `BaseMessages`\n",
    "* a dict with a key that takes the latest message(s) as a string or sequence of `BaseMessages`, and a separate key that takes historical messages\n",
    "\n",
    "And returns as output one of\n",
    "\n",
    "* a string that can be treated as the contents of an `AIMessage`\n",
    "* a sequence of `BaseMessage`\n",
    "* a dict with a key that contains a sequence of `BaseMessage`\n",
    "\n",
    "Let's take a look at some examples to see how it works. First we construct a runnable (which here accepts a dict as input and returns a message as output):\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs\n",
    "  customVarName=\"llm\"\n",
    "/>\n",
    "```"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
   "id": "6489f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "%pip install -qU langchain langchain_anthropic\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass()\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
>>>>>>> langchan/master
   "id": "2ed413b4-33a1-48ee-89b0-2d4917ec101a",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "model = GigaChat(credentials=\"<авторизационные_данные>\", verify_ssl_certs=False)\n",
=======
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
>>>>>>> langchan/master
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
<<<<<<< HEAD
    "            \"Ты ассистент в сфере {ability}. Твой ответ должен быть не длиннее 20 слов.\",\n",
=======
    "            \"You're an assistant who's good at {ability}. Respond in 20 words or fewer\",\n",
>>>>>>> langchan/master
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd175e1-c7b8-4929-a57e-3331865fe7aa",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Для дальней работы с историей сообщений вам потребуется:\n",
    "\n",
    "* Runnable из примера;\n",
    "* вызываемый объект, который возвращает экземпляр `BaseChatMessageHistory`.\n",
    "\n",
    "Раздел содержит как примеры in-memory памяти, реализованной с помощью объектов `ChatMessageHistory`, так и более надежный способ хранения с помощью Redis — `RedisChatMessageHistory`.\n",
    "\n",
    ":::note\n",
    "\n",
    "В разделе интеграций вы найдете описание других провайдеров для реализации памяти.\n",
    "\n",
    ":::"
=======
    "To manage the message history, we will need:\n",
    "1. This runnable;\n",
    "2. A callable that returns an instance of `BaseChatMessageHistory`.\n",
    "\n",
    "Check out the [memory integrations](https://integrations.langchain.com/memory) page for implementations of chat message histories using Redis and other providers. Here we demonstrate using an in-memory `ChatMessageHistory` as well as more persistent storage using `RedisChatMessageHistory`."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83adad-9672-496d-9f25-5747e7b8c8bb",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Хранение in-memory\n",
    "\n",
    "В примере ниже история сообщений хранится в оперативной памяти внутри глобального словаря Python.\n",
    "\n",
    "Экземпляр `ChatMessageHistory` возвращается с помощью вызываемого объекта `get_session_history`, который ссылается на заданный словарь.\n",
    "Аргументы `get_session_history` можно передать в процессе выполнения с помощью в экземпляре `RunnableWithMessageHistory`.\n",
    "По умолчанию параметр конфигурации ожидается в виде строки `session_id`.\n",
    "Это можно изменить с помощью kwarg `history_factory_config`.\n",
    "\n",
    "Пример работы по умолчанию:"
=======
    "## In-memory\n",
    "\n",
    "Below we show a simple example in which the chat history lives in memory, in this case via a global Python dict.\n",
    "\n",
    "We construct a callable `get_session_history` that references this dict to return an instance of `ChatMessageHistory`. The arguments to the callable can be specified by passing a configuration to the `RunnableWithMessageHistory` at runtime. By default, the configuration parameter is expected to be a single string `session_id`. This can be adjusted via the `history_factory_config` kwarg.\n",
    "\n",
    "Using the single-parameter default:"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54348d02-d8ee-440c-bbf9-41bc0fbbc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acb505-3fd3-4ab4-9f04-5ea07e81542e",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "В примере задано два поля:\n",
    "\n",
    "* `input_messages_key` — содержимое поля должно обрабатыватья как последнее входное сообщение;\n",
    "* `history_messages_key` — поле в котором сохраняется история сообщений.\n",
    "\n",
    "При вызове этого Runnable соответствующую историю сообщений можно задать с помощью параметра конфигурации:"
=======
    ":::info\n",
    "\n",
    "Note that we've specified `input_messages_key` (the key to be treated as the latest input message) and `history_messages_key` (the key to add historical messages to).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35222c30",
   "metadata": {},
   "source": [
    "When invoking this new runnable, we specify the corresponding chat history via a configuration parameter:"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01384412-f08e-4634-9edb-3f46f475b582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "AIMessage(content='Косинус - это тригонометрическая функция, которая равна синусу угла в прямоугольном треугольнике, противолежащего данному катету.', response_metadata={'token_usage': Usage(prompt_tokens=42, completion_tokens=40, total_tokens=82), 'model_name': 'GigaChat:3.1.24.3', 'finish_reason': 'stop'}, id='run-21e71c2f-1e0b-442e-98c0-54d52991fbb3-0')"
=======
       "AIMessage(content='Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse of a right triangle.', response_metadata={'id': 'msg_01DH8iRBELVbF3sqM8U5sk8A', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 32, 'output_tokens': 31}}, id='run-e07fc012-a4f6-4e47-8ef8-250f296eba5b-0')"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
<<<<<<< HEAD
    "    {\"ability\": \"математика\", \"input\": \"Что такое косинус?\"},\n",
=======
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
>>>>>>> langchan/master
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "954688a2-9a3f-47ee-a9e8-fa0c83e69477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "AIMessage(content='Проще говоря, это отношение длины катета к гипотенузе.', response_metadata={'token_usage': Usage(prompt_tokens=91, completion_tokens=21, total_tokens=112), 'model_name': 'GigaChat:3.1.24.3', 'finish_reason': 'stop'}, id='run-49195640-4bf8-408f-ab87-81fcfba19b19-0')"
=======
       "AIMessage(content='The inverse of the cosine function is called the arccosine or inverse cosine.', response_metadata={'id': 'msg_015TeeRQBvTvc7XG1JxYqZyq', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 72, 'output_tokens': 22}}, id='run-32ae22ea-3b2f-4d38-8c8a-cb8702e2f3e7-0')"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Обращение к памяти\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"математика\", \"input\": \"Что?\"},\n",
=======
    "# Remembers\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What is its inverse called?\"},\n",
>>>>>>> langchan/master
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "markdown",
   "id": "e0c651e5",
   "metadata": {},
   "source": [
    ":::info\n",
    "\n",
    "Note that in this case the context is preserved via the chat history for the provided `session_id`, so the model knows that \"it\" refers to \"cosine\" in this case.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f8d5f",
   "metadata": {},
   "source": [
    "Now let's try a different `session_id`"
   ]
  },
  {
>>>>>>> langchan/master
   "cell_type": "code",
   "execution_count": 6,
   "id": "39350d7c-2641-4744-bc2a-fd6a57c4ea90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "AIMessage(content='Я не совсем понимаю ваш вопрос. Можете уточнить, пожалуйста?', response_metadata={'token_usage': Usage(prompt_tokens=37, completion_tokens=17, total_tokens=54), 'model_name': 'GigaChat:3.1.24.3', 'finish_reason': 'stop'}, id='run-82a1ae88-27e9-4a0b-970c-28d01f41a9bc-0')"
=======
       "AIMessage(content='The inverse of a function is the function that undoes the original function.', response_metadata={'id': 'msg_01M8WbHWg2sjWTz3m3NKqZuF', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 32, 'output_tokens': 18}}, id='run-b64c73d6-03ee-4b0a-85e0-34beb45408d4-0')"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Новая сессия session_id --> память отсутствует.\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"математика\", \"input\": \"Что?\"},\n",
=======
    "# New session_id --> does not remember.\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What is its inverse called?\"},\n",
>>>>>>> langchan/master
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "d29497be-3366-408d-bbb9-d4a8bf4ef37c",
   "metadata": {},
   "source": [
    "Конфигурационные параметры, которые используются для ведения историй сообщений, можно изменить, если передать в параметр `history_factory_config` список объектов `ConfigurableFieldSpec`.\n",
    "Пример ниже показывает как использовать два параметра — `user_id` и `conversation_id`."
=======
   "id": "5416e195",
   "metadata": {},
   "source": [
    "When we pass a different `session_id`, we start a new chat history, so the model does not know what \"it\" refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6710e65",
   "metadata": {},
   "source": [
    "### Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29497be-3366-408d-bbb9-d4a8bf4ef37c",
   "metadata": {},
   "source": [
    "The configuration parameters by which we track message histories can be customized by passing in a list of ``ConfigurableFieldSpec`` objects to the ``history_factory_config`` parameter. Below, we use two parameters: a `user_id` and `conversation_id`."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c89daee-deff-4fdf-86a3-178f7d8ef536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "AIMessage(content='Hello! How can I assist you with math today?', response_metadata={'id': 'msg_01UdhnwghuSE7oRM57STFhHL', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 27, 'output_tokens': 14}}, id='run-3d53f67a-4ea7-4d78-8e67-37db43d4af5d-0')"
=======
       "AIMessage(content=\"Why can't a bicycle stand up on its own? It's two-tired!\", response_metadata={'id': 'msg_011qHi8pvbNkKhRb9XYRm2kc', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 30, 'output_tokens': 20}}, id='run-5d1d5b5a-ccec-4c2c-b11a-f1953dbe85a3-0')"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
<<<<<<< HEAD
    "            name=\"ID пользоватея\",\n",
    "            description=\"Уникальный идентификатор пользователя.\",\n",
=======
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
>>>>>>> langchan/master
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
<<<<<<< HEAD
    "            name=\"ID диалого\",\n",
    "            description=\"Уникальный идентификатор диалога.\",\n",
=======
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
>>>>>>> langchan/master
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "with_message_history.invoke(\n",
<<<<<<< HEAD
    "    {\"ability\": \"математика\", \"input\": \"Привет\"},\n",
=======
    "    {\"ability\": \"jokes\", \"input\": \"Tell me a joke\"},\n",
>>>>>>> langchan/master
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f282883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The joke was about a bicycle not being able to stand up on its own because it\\'s \"two-tired\" (too tired).', response_metadata={'id': 'msg_01LbrkfidZgseBMxxRjQXJQH', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 59, 'output_tokens': 30}}, id='run-8b2ca810-77d7-44b8-b27b-677e0062b19a-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remembers\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"jokes\", \"input\": \"What was the joke about?\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc122c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm afraid I don't have enough context to provide a relevant joke. As an AI assistant, I don't actually have pre-programmed jokes. I'd be happy to try generating a humorous response if you provide more details about the context.\", response_metadata={'id': 'msg_01PgSp46hNJnKyNfNKPDauQ9', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 32, 'output_tokens': 54}}, id='run-ed202892-27e4-4da9-a26d-e0dc16b10940-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New user_id --> does not remember\n",
    "with_message_history.invoke(\n",
    "    {\"ability\": \"jokes\", \"input\": \"What was the joke about?\"},\n",
    "    config={\"configurable\": {\"user_id\": \"456\", \"conversation_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce37565",
   "metadata": {},
   "source": [
    "Note that in this case the context was preserved for the same `user_id`, but once we changed it, the new chat history was started, even though the `conversation_id` was the same."
   ]
  },
  {
>>>>>>> langchan/master
   "cell_type": "markdown",
   "id": "18f1a459-3f88-4ee6-8542-76a907070dd6",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Примеры реализации Runnable\n",
    "\n",
    "В предыдущем примере Runnable принимает на вход словарь и возвращает `BaseMessage`.\n",
    "\n",
    "Примеры ниже показывают как можно решить ту же задачу другими способами."
=======
    "### Examples with runnables of different signatures\n",
    "\n",
    "The above runnable takes a dict as input and returns a BaseMessage. Below we show some alternatives."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eae1bf-b59d-4a61-8e62-b6dbf667e866",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### Сообщения на входе, словарь на выходе"
=======
    "#### Messages input, dict output"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17733d4f-3a32-4055-9d44-5d58b9446a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "{'output_message': AIMessage(content='Симона де Бовуар считала, что свобода воли является ключевым аспектом человеческого существования. Она утверждала, что свобода воли позволяет людям принимать решения и действовать на основе своих собственных убеждений и ценностей. Однако, де Бовуар также подчеркивала, что свобода воли ограничена социальными и культурными условиями, в которых мы живем.', response_metadata={'token_usage': Usage(prompt_tokens=304, completion_tokens=80, total_tokens=384), 'model_name': 'GigaChat:3.1.24.3', 'finish_reason': 'stop'}, id='run-f6725c40-2f24-42c3-b26e-aef2d5b09176-0')}"
=======
       "{'output_message': AIMessage(content='Simone de Beauvoir was a prominent French existentialist philosopher who had some key beliefs about free will:\\n\\n1. Radical Freedom: De Beauvoir believed that humans have radical freedom - the ability to choose and define themselves through their actions. She rejected determinism and believed that we are not simply products of our biology, upbringing, or social circumstances.\\n\\n2. Ambiguity of the Human Condition: However, de Beauvoir also recognized the ambiguity of the human condition. While we have radical freedom, we are also situated beings constrained by our facticity (our given circumstances and limitations). This creates a tension and anguish in the human experience.\\n\\n3. Responsibility and Bad Faith: With radical freedom comes great responsibility. De Beauvoir criticized \"bad faith\" - the denial or avoidance of this responsibility by making excuses or pretending we lack free will. She believed we must courageously embrace our freedom and the burdens it entails.\\n\\n4. Ethical Engagement: For de Beauvoir, freedom is not just an abstract philosophical concept, but something that must be exercised through ethical engagement with the world and others. Our choices and actions have moral implications that we must grapple with.\\n\\nOverall, de Beauvoir\\'s perspective on free will was grounded in existentialist principles - the belief that we are fundamentally free, yet this freedom is fraught with difficulty and responsibility. Her views emphasized the centrality of human agency and the ethical dimensions of our choices.', response_metadata={'id': 'msg_01QFXHx74GSzcMWnWc8YxYSJ', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 20, 'output_tokens': 324}}, id='run-752513bc-2b4f-4cad-87f0-b96fee6ebe43-0')}"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
<<<<<<< HEAD
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"output_message\": GigaChat(\n",
    "            credentials=\"<авторизационные_данные>\", verify_ssl_certs=False\n",
    "        )\n",
    "    }\n",
    ")\n",
=======
    "chain = RunnableParallel({\"output_message\": model})\n",
>>>>>>> langchan/master
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    output_messages_key=\"output_message\",\n",
    ")\n",
    "\n",
    "with_message_history.invoke(\n",
<<<<<<< HEAD
    "    [HumanMessage(content=\"Что Симона де Бовуар думала о свободе воли\")],\n",
=======
    "    [HumanMessage(content=\"What did Simone de Beauvoir believe about free will\")],\n",
>>>>>>> langchan/master
    "    config={\"configurable\": {\"session_id\": \"baz\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efb57ef5-91f9-426b-84b9-b77f071a9dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "{'output_message': AIMessage(content='Идеи Симоны де Бовуар о свободе воли отличаются от идей Жан-Поля Сартра, другого известного французского философа.\\n\\nСартр утверждал, что человек изначально свободен, но эта свобода не является благом. Он считал, что свобода воли приводит к тому, что люди становятся ответственными за свои собственные действия и их последствия. Однако, он также утверждал, что эта ответственность может быть невыносимой, поскольку она требует от нас принятия решений и действий, которые мы не можем контролировать.\\n\\nВ отличие от Сартра, де Бовуар считала, что свобода воли является основой для самореализации и самоопределения. Она утверждала, что свобода воли позволяет нам принимать решения и действовать на основе наших собственных убеждений и ценностей.\\n\\nТаким образом, идеи де Бовуар и Сартра о свободе воли различаются в том, как они рассматривают эту свободу и ее последствия. Де Бовуар видит в свободе воли возможность для самореализации и самоопределения, в то время как Сартр считает, что она приводит к ответственности и невыносимой свободе.', response_metadata={'token_usage': Usage(prompt_tokens=403, completion_tokens=257, total_tokens=660), 'model_name': 'GigaChat:3.1.24.3', 'finish_reason': 'stop'}, id='run-47bcaac0-b807-49cc-a162-7ee948013814-0')}"
=======
       "{'output_message': AIMessage(content='Simone de Beauvoir\\'s views on free will were quite similar to those of her long-time partner and fellow existentialist philosopher, Jean-Paul Sartre. There are some key parallels and differences:\\n\\nSimilarities:\\n\\n1. Radical Freedom: Both de Beauvoir and Sartre believed that humans have radical, unconditioned freedom to choose and define themselves.\\n\\n2. Rejection of Determinism: They both rejected deterministic views that see humans as products of their circumstances or nature.\\n\\n3. Emphasis on Responsibility: They agreed that with radical freedom comes great responsibility for one\\'s choices and actions.\\n\\n4. Critique of \"Bad Faith\": Both philosophers criticized the tendency of people to deny or avoid their freedom through self-deception and making excuses.\\n\\nDifferences:\\n\\n1. Gendered Perspectives: While Sartre developed a more gender-neutral existentialist philosophy, de Beauvoir brought a distinctly feminist lens, exploring the unique challenges and experiences of women\\'s freedom.\\n\\n2. Ethical Engagement: De Beauvoir placed more emphasis on the importance of ethical engagement with the world and others, whereas Sartre\\'s focus was more individualistic.\\n\\n3. Ambiguity of the Human Condition: De Beauvoir was more attuned to the ambiguity and tensions inherent in the human condition, whereas Sartre\\'s views were sometimes seen as more absolutist.\\n\\n4. Influence of Phenomenology: De Beauvoir was more influenced by the phenomenological tradition, which shaped her understanding of embodied, situated freedom.\\n\\nOverall, while Sartre and de Beauvoir shared a core existentialist framework, de Beauvoir\\'s unique feminist perspective and emphasis on ethical engagement with others distinguished her views on free will and the human condition.', response_metadata={'id': 'msg_01BEANW4VX6cUWYjkv3CanLz', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 355, 'output_tokens': 388}}, id='run-e786ab3a-1a42-45f3-94a3-f0c591430df3-0')}"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
<<<<<<< HEAD
    "    [HumanMessage(content=\"Как эти идеи отличаются от того, что думал Сартр\")],\n",
=======
    "    [HumanMessage(content=\"How did this compare to Sartre\")],\n",
>>>>>>> langchan/master
    "    config={\"configurable\": {\"session_id\": \"baz\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39eac5f-a9d8-4729-be06-5e7faf0c424d",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### Сообщения на входе и сообщения на выходе"
=======
    "#### Messages input, messages output"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45bcd95-e31f-4a9a-967a-78f96e8da881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
<<<<<<< HEAD
       "| RunnableBinding(bound=ChatAnthropic(model='claude-3-haiku-20240307', temperature=0.0, anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), _client=<anthropic.Anthropic object at 0x1077ff5b0>, _async_client=<anthropic.AsyncAnthropic object at 0x1321c71f0>), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x1473dd000>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x1374c7be0>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
=======
       "| RunnableBinding(bound=ChatAnthropic(model='claude-3-haiku-20240307', temperature=0.0, anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), _client=<anthropic.Anthropic object at 0x105682720>, _async_client=<anthropic.AsyncAnthropic object at 0x106a08fe0>), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x106aeef20>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x106aee520>, history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableWithMessageHistory(\n",
<<<<<<< HEAD
    "    GigaChat(credentials=\"<авторизационные_данные>\", verify_ssl_certs=False),\n",
=======
    "    model,\n",
>>>>>>> langchan/master
    "    get_session_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04daa921-a2d1-40f9-8cd1-ae4e9a4163a7",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### Словарь с полем, хранящим все сообщения на входе, сообщения на выходе"
=======
    "#### Dict with single key for all messages input, messages output"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27157f15-9fb0-4167-9870-f4d7f234b3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  input_messages: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBinding(bound=RunnableLambda(itemgetter('input_messages'))\n",
<<<<<<< HEAD
       "  | ChatAnthropic(model='claude-3-haiku-20240307', temperature=0.0, anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), _client=<anthropic.Anthropic object at 0x1077ff5b0>, _async_client=<anthropic.AsyncAnthropic object at 0x1321c71f0>), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x1473df6d0>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x1374c7be0>, input_messages_key='input_messages', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
=======
       "  | ChatAnthropic(model='claude-3-haiku-20240307', temperature=0.0, anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), _client=<anthropic.Anthropic object at 0x105682720>, _async_client=<anthropic.AsyncAnthropic object at 0x106a08fe0>), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x106aef560>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x106aee520>, input_messages_key='input_messages', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
>>>>>>> langchan/master
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "RunnableWithMessageHistory(\n",
<<<<<<< HEAD
    "    itemgetter(\"input_messages\")\n",
    "    | GigaChat(credentials=\"<авторизационные_данные>\", verify_ssl_certs=False),\n",
=======
    "    itemgetter(\"input_messages\") | model,\n",
>>>>>>> langchan/master
    "    get_session_history,\n",
    "    input_messages_key=\"input_messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "76799a13-d99a-4c4f-91f2-db699e40b8df",
   "metadata": {},
   "source": [
    "## Постоянное хранение\n",
    "\n",
    "Вам может потребоваться организовать постоянное хранение истории диалогов.\n",
    "Для `RunnableWithMessageHistory` не важно, как `get_session_history` получает историю сообщений — из файловой системы или как-то иначе.\n",
    "Пример ниже показывает как для этого можно использовать базу данных Redis.\n",
    "\n",
    ":::note\n",
    "\n",
    "Описание интеграций с другими провайдерами памяти ищите в [официальной документации LangChain](https://integrations.langchain.com/memory).\n",
    "\n",
    ":::"
=======
   "id": "418ca7af-9ed9-478c-8bca-cba0de2ca61e",
   "metadata": {},
   "source": [
    "## Persistent storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76799a13-d99a-4c4f-91f2-db699e40b8df",
   "metadata": {},
   "source": [
    "In many cases it is preferable to persist conversation histories. `RunnableWithMessageHistory` is agnostic as to how the `get_session_history` callable retrieves its chat message histories. See [here](https://github.com/langchain-ai/langserve/blob/main/examples/chat_with_persistence_and_user/server.py) for an example using a local filesystem. Below we demonstrate how one could use Redis. Check out the [memory integrations](https://integrations.langchain.com/memory) page for implementations of chat message histories using other providers."
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca45e5-35d9-4603-9ca9-6ac0ce0e35cd",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Подготовка к работе\n",
    "\n",
    "Установите Redis с помощью менеджера пакетов:"
=======
    "### Setup\n",
    "\n",
    "We'll need to install Redis if it's not installed already:"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 14,
>>>>>>> langchan/master
   "id": "477d04b3-c2b6-4ba5-962f-492c0d625cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ec9e0-7b1c-4c6f-b570-e61d520b47c6",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Запустите локальные сервер Redis Stack, если у вас нет равзвернутого сервера, к которому можно подключиться:\n",
    "\n",
=======
    "Start a local Redis Stack server if we don't have an existing Redis deployment to connect to:\n",
>>>>>>> langchan/master
    "```bash\n",
    "docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 15,
>>>>>>> langchan/master
   "id": "cd6a250e-17fe-4368-a39d-1fe6b2cbde68",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_URL = \"redis://localhost:6379/0\""
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "f9d81796-ce61-484c-89e2-6c567d5e54ef",
   "metadata": {},
   "source": [
    "Для использования Redis достаточно определить новую вызываемую функцию, которая будет возвращать экземпляр `RedisChatMessageHistory`:"
=======
   "id": "36f43b87-655c-4f64-aa7b-bd8c1955d8e5",
   "metadata": {},
   "source": [
    "### [LangSmith](https://docs.smith.langchain.com)\n",
    "\n",
    "LangSmith is especially useful for something like message history injection, where it can be hard to otherwise understand what the inputs are to various parts of the chain.\n",
    "\n",
    "Note that LangSmith is not needed, but it is helpful.\n",
    "If you do want to use LangSmith, after you sign up at the link above, make sure to uncoment the below and set your environment variables to start logging traces:"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 16,
   "id": "2afc1556-8da1-4499-ba11-983b66c58b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d81796-ce61-484c-89e2-6c567d5e54ef",
   "metadata": {},
   "source": [
    "Updating the message history implementation just requires us to define a new callable, this time returning an instance of `RedisChatMessageHistory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
>>>>>>> langchan/master
   "id": "ca7c64d8-e138-4ef8-9734-f82076c47d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "\n",
    "def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "    return RedisChatMessageHistory(session_id, url=REDIS_URL)\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eefdec-9901-4650-b64c-d3c097ed5f4d",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Вызывать цепочку можно так же, как и раньше:"
=======
    "We can invoke as before:"
>>>>>>> langchan/master
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "a85bcc22-ca4c-4ad5-9440-f94be7318f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"математика\", \"input\": \"Что такое косинус?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29abd3-751f-41ce-a1b0-53f6b565e79d",
   "metadata": {},
=======
   "execution_count": 18,
   "id": "a85bcc22-ca4c-4ad5-9440-f94be7318f3e",
   "metadata": {},
>>>>>>> langchan/master
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "AIMessage(content='The inverse of cosine is the arccosine function, denoted as acos or cos^-1, which gives the angle corresponding to a given cosine value.')"
      ]
     },
     "execution_count": 12,
=======
       "AIMessage(content='Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse of a right triangle.', response_metadata={'id': 'msg_01DwU2BD8KPLoXeZ6bZPqxxJ', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 164, 'output_tokens': 31}}, id='run-c2a443c4-79b1-4b07-bb42-5e9112e5bbfc-0')"
      ]
     },
     "execution_count": 18,
>>>>>>> langchan/master
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
<<<<<<< HEAD
    "    {\"ability\": \"math\", \"input\": \"Какая у него обратная функция?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",
    ")"
   ]
=======
    "    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab29abd3-751f-41ce-a1b0-53f6b565e79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The inverse of cosine is called arccosine or inverse cosine.', response_metadata={'id': 'msg_01XYH5iCUokxV1UDhUa8xzna', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 202, 'output_tokens': 19}}, id='run-97dda3a2-01e3-42e5-8241-f948e7535ffc-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"input\": \"What's its inverse\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foobar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d1feb-b4bb-4624-961c-7db2e1180df7",
   "metadata": {},
   "source": [
    ":::tip\n",
    "\n",
    "[Langsmith trace](https://smith.langchain.com/public/bd73e122-6ec1-48b2-82df-e6483dc9cb63/r)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5115e-64a1-4ad5-b676-8afd4ef6093e",
   "metadata": {},
   "source": [
    "Looking at the Langsmith trace for the second call, we can see that when constructing the prompt, a \"history\" variable has been injected which is a list of two messages (our first input and first output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd510b68",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You have now learned one way to manage message history for a runnable.\n",
    "\n",
    "To learn more, see the other how-to guides on runnables in this section."
   ]
>>>>>>> langchan/master
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.6"
=======
   "version": "3.12.3"
>>>>>>> langchan/master
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
