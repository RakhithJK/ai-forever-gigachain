{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5151afed",
   "metadata": {},
   "source": [
    "# Ответы на вопросы с помощью RAG\n",
    "\n",
    "Большие языковые модели (LLM) позволяют разрабатывать развитые вопросо-ответные приложения, которые могут отвечать на вопросы на основе заданных данных.\n",
    "В основе таких систем лежит методика известная как «генерация с расширенным поиском» (retrieval-augmented generation или RAG).\n",
    "\n",
    "## Что такое RAG?\n",
    "\n",
    "RAG - это методика, которая позволяет расширить знания LLM дополнительными данными.\n",
    "\n",
    "Большие языковые модели имеют представление о различных темах, но их знания ограничены общими данными, доступными на момент обучения модели.\n",
    "Если вам нужно, чтобы модель была в курсе какой-то специфической информации или информации, которая появилась после ее обучения, вам понадобится предоставить модели соответствующие данные.\n",
    "Предоставление таких данных и применение их в промпте это и есть RAG — генерация с расширенным поиском.\n",
    "\n",
    "GigaChain предоставляет компоненты для разработки вопрос-ответных приолжени и поддержки RAG-методики в целом.\n",
    "\n",
    ":::note\n",
    "\n",
    "В этом разделе приводится пример разработки вопрос-ответетного приложения, которое использует неструктурированные данные.\n",
    "Другие примеры использования RAG включают:\n",
    "\n",
    "* [Ответы на основе SQL-данных](/docs/use_cases/sql/);\n",
    "* [Ответы на основе кода](/docs/use_cases/code_understanding) (например, Python).\n",
    "\n",
    ":::\n",
    "\n",
    "## Архитектура RAG\n",
    "\n",
    "В общем случае приложение с RAG включает два компонента:\n",
    "\n",
    "* Конвейер для загрузки данных из источника и их индексирование. Как правило индексирование работает в автономном режиме.\n",
    "* Цепочка RAG, которая в реальном времени обрабатывает запрос пользователя, извлекает соответствующие данные из индекса и передает их модели.\n",
    "\n",
    "Процесс генерации ответа на основе необработанных данных можно представить следующим образом:\n",
    "\n",
    "### Индексирование\n",
    "\n",
    "1. Загрузка документов с помощью [DocumentLoaders](/docs/modules/data_connection/document_loaders/).\n",
    "2. Использование [разделителей текста](/docs/modules/data_connection/document_transformers/) для разбивки `Documents` на мелкие фрагменты. Это удобно как для индексации данных, так и для передачи их в модель, поскольку большие фрагменты труднее искать и использовать в рамках контекста модели.\n",
    "3. Хранение данных с помощью [векторных хранилищ](/docs/modules/data_connection/vectorstores/) и создание [эмбеддингов](/docs/modules/data_connection/text_embedding/) для поиска. \n",
    "\n",
    "![index_diagram](../../../static/img/rag_indexing.png)\n",
    "\n",
    "### Извлечение данных и генерация\n",
    "\n",
    "1. Извлечение релевантных данных с помощью [ретривера](/docs/modules/data_connection/retrievers/) на основе запроса пользователя.\n",
    "2. Передача промпта, включающего вопрос пользователя и извлеченные данные, в [чат-модель](/docs/modules/model_io/chat) / [LLM](/docs/modules/model_io/llms/) для генерации ответа.\n",
    "\n",
    "![retrieval_diagram](../../../static/img/rag_retrieval_generation.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
