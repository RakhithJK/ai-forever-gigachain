{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befa7fd1",
   "metadata": {},
   "source": [
    "# Начало работы\n",
    "\n",
    "LCEL облегчает создание сложных цепочек на основе базовых компонентов и по умолчанию поддерживает такие функции как потоковая передача, параллелизм и журналирование.\n",
    "\n",
    ":::note\n",
    "\n",
    "Для работы с GigaChat вам понадобится получить авторизационные данные.\n",
    "Подробную инструкцию о том как это сделать вы найдете в официальной документации:\n",
    "\n",
    "* [Получение авторизационных данных для физических лиц](https://developers.sber.ru/docs/ru/gigachat/individuals-quickstart#shag-1-sozdayte-proekt-giga-chat-api)\n",
    "* [Получение авторизационных данных для ИП и юридических лиц](https://developers.sber.ru/docs/ru/gigachat/legal-quickstart#shag-3-poluchite-avtorizatsionnye-dannye)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9acd2e",
   "metadata": {},
   "source": [
    "## Базовый пример: промпт + модель + парсер вывода {#basic-example}\n",
    "\n",
    "Наиболее распространенный сценарий — объединение в цепочку шаблона промпта и модели.\n",
    "Посмотрим, как это работает на примере цепочки, которая генерирует шутку на заданную тему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  gigachain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d54f72",
   "metadata": {},
   "source": [
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs openaiParams={`model=\"gpt-4\"`} />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eed8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | output: false\n",
    "# | echo: false\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466b65b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Почему облака не любят понедельники? Потому что они всегда идут после субботы и воскресенья.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Расскажи шутку про {topic}\")\n",
    "model = GigaChat(credentials=\"<авторизационные_данные>\", verify_ssl_certs=False)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"облако\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c502c5-85ee-4f36-aaf4-d6e350b7792f",
   "metadata": {},
   "source": [
    "Обратите внимание на строку, в которой различные компоненты объединяются в цепочку с помощью LCEL:\n",
    "\n",
    "```\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "Символ `|` выполняет ту же функцию, что и [оператор конвейера в Unix](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BD%D0%B2%D0%B5%D0%B9%D0%B5%D1%80_(Unix)) — связывает вместе различные компоненты и подает вывод одного компонента на вход в следующий компонент.\n",
    "\n",
    "В этой цепочке данные, введенные пользователем, передаются в шаблон промпта, затем вывод шаблона передается на вход в модель, а затем результат работы модели передается в парсер выходных данных.\n",
    "Рассмотрим каждый компонент в отдельности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b77fa",
   "metadata": {},
   "source": [
    "### 1. Промпт\n",
    "\n",
    "Объект `prompt` - экземпляр `BasePromptTemplate`.\n",
    "Он принимает на вход словарь с переменными шаблона и возвращает `PromptValue`.\n",
    "`PromptValue` - это обертка вокруг готового промпта, которую можно передать в `LLM` (принимает строку в качестве входных данных) или в `ChatModel` (принимает на вход последовательность сообщений).\n",
    "`PromptValue` поддерживает разные типы языковых моделей, поскольку содержит логику как для создания экземпляров `BaseMessage`, так и для создания строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8656990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Расскажи шутку про облако')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"облако\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6034488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Расскажи шутку про облако')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60565463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Расскажи шутку про облако'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f0f76",
   "metadata": {},
   "source": [
    "### 2. Модель\n",
    "\n",
    "Готовый экземпляр `PromptValue` передается в `model`.\n",
    "В примере в качестве модели (`model`) используется класс `ChatModel`, который возвращает экземпляр класса `BaseMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cf5f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Почему облака не любят понедельники? Потому что они всегда идут после субботы и воскресенья.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e7db8",
   "metadata": {},
   "source": [
    "Если бы в `model` использовался класс `LLM`, то на выходе была бы строка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8feb05da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Почему облако не может перейти дорогу?\\n\\nОтвет: Потому что оно белое и пушистое!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = GigaChat(credentials=\"<авторизационные_данные>\", verify_ssl_certs=False)\n",
    "llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91847478",
   "metadata": {},
   "source": [
    "### 3. Парсер вывода\n",
    "\n",
    "И, наконец, выходные данные из модельи `model` передаются в `output_parser` — экземпляр класса `BaseOutputParser`, который принимает на вход строку или экземпляр `BaseMessage`.\n",
    "Объект `StrOutputParser` отвечает за преобразование любых входных данных в строку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "533e59a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Почему облака не любят понедельники? Потому что они всегда идут после субботы и воскресенья.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851e842",
   "metadata": {},
   "source": [
    "### 4. Итоговый процесс\n",
    "\n",
    "Получившуюся цепочку можно описать шагами:\n",
    "\n",
    "1. Ввод пользователя передается как `{\"topic\": \"облако\"}`.\n",
    "2. Компонент `prompt` принимает данные от пользователя, которые, после создания промпта с помощью темы `topic`, используются для создания `PromptValue`.\n",
    "3. Компонент `model` принимает сгенерированный промпт и передает его в GigaChat. Модель возвращает результат генераци в виде экземпляра `ChatMessage`.\n",
    "4. Компонент `output_parser` принимает `ChatMessage`, и преобразует это в Python-строку, которая возвращается из метода `invoke`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4873109",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    A(Input: topic=Облако) --> |Dict| B(PromptTemplate)\n",
    "    B -->|PromptValue| C(ChatModel)    \n",
    "    C -->|ChatMessage| D(StrOutputParser)\n",
    "    D --> |String| F(Результат)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe63534d",
   "metadata": {},
   "source": [
    ":::note\n",
    "\n",
    "Для просмотра выходных данных любого из компонентов, простестируйте сокращенную версию цепочки, например, `prompt` или `prompt | model`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11089b6f-23f8-474f-97ec-8cae8d0ca6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"topic\": \"облако\"}\n",
    "\n",
    "prompt.invoke(input)\n",
    "# > ChatPromptValue(messages=[HumanMessage(content='Расскажи шутку про облако')])\n",
    "\n",
    "(prompt | model).invoke(input)\n",
    "# > AIMessage(content='Почему облака не любят понедельники? Потому что они всегда идут после субботы и воскресенья.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d3b9d-e400-4c9b-9188-f29dac73e6bb",
   "metadata": {},
   "source": [
    "## Пример поиска с RAG\n",
    "\n",
    "Следующий пример демонстрирует цепочку с RAG (retrieval-augmented generation), с помощью которой можно добавить контекст при ответе на вопросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662426e8-4316-41dc-8312-9b58edc7e0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Василий работал в Сбере.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Для работы примера установите docarray tiktoken:\n",
    "# pip install langchain docarray tiktoken\n",
    "\n",
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "from langchain_community.vectorstores.docarray import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"Василий работал в Сбере\", \"медведи любят есть мед\"],\n",
    "    embedding=GigaChatEmbeddings(\n",
    "        credentials=\"<авторизационные_данные>\", verify_ssl_certs=False\n",
    "    ),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Отвечай на вопрос только на основе контекста:\n",
    "{context}\n",
    "\n",
    "Вопрос: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = GigaChat(credentials=\"<авторизационные_данные>\", verify_ssl_certs=False)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"где работал Василий?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0999140-6001-423b-970b-adf1dfdb4dec",
   "metadata": {},
   "source": [
    "Цепочка в примере выглядит так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b88e9bb-f04a-4a56-87ec-19a0e6350763",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e929e15-40a5-4569-8969-384f636cab87",
   "metadata": {},
   "source": [
    "Чтобы понять как работает цепочка, обратите внимание, что шаблон промпта содержит переменные `question` (вопрос пользователя) `context` (контекст вопроса), значения которых используются в промпте.\n",
    "Перед созданием промпта из `vectorstore` нужно получить документы, которые модель будет использовать в качестве контекста при ответе на вопрос.\n",
    "\n",
    "Для этого, на этапе предварительной подготовки цепочки, добавлен ретривер, который на основе запроса извлекает документы из векторного хранилища `vectorstore`.\n",
    "Ретривер — исполняемый компонент, который можно объединять с другими компонентами или запускать отдельно:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7319ef6-613b-4638-ad7d-4a2183702c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Василий работал в Сбере'),\n",
       " Document(page_content='медведи любят есть мед')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"где работал Василий?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6833844-f1c4-444c-a3d2-31b3c6b31d46",
   "metadata": {},
   "source": [
    "После этого, с помощью `RunnableParallel` на основе полученных документов и вопроса пользователя подготавливается промпт, который будет передан в модель.\n",
    "При этом для поиска документов используется ретривер, для получения вопроса пользователя `RunnablePassthrough`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbca26b-d6b9-4c24-806c-1ec8fdaab4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c721c1-048b-4a64-9d78-df54fe465992",
   "metadata": {},
   "source": [
    "Готовая цепочка будет выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d5115a7-7b8e-458b-b936-26cc87ee81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f5f74-b387-48a0-bedd-1fae202cd10a",
   "metadata": {},
   "source": [
    "Этапы работы цепочки:\n",
    "\n",
    "1. Вначале создается экземпляр `RunnableParallel` с двумя записями: вопросом и контекстом. Запись `context` содержит документы для поиска по контексту, которые получил ретривер. Запись `question` содержит вопрос пользователя. Для передачи вопроса использвуется экземпляр `RunnablePassthrough`, с помощью которого копируется запись.\n",
    "2. Словарь с записями, созданный на предыдщуем этапе, передается в компонент `prompt`. Компонент принимает вопрос пользователя (`question`) и контекст (`context`), извлеченный из документа, использует полученные данные для создания промпта и возвращает `PromptValue`.\n",
    "3. Компонент `model` принимает сгенерированный промпт и передает его в GigaChat. Модель возвращает результат генераци в виде экземпляра `ChatMessage`.\n",
    "4. Компонент `output_parser` принимает `ChatMessage`, и преобразует это в Python-строку, которая возвращается из метода `invoke`.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A(Вопрос) --> B(RunnableParallel)\n",
    "    B -->|Вопрос| C(Ретривер)\n",
    "    B -->|Вопрос| D(RunnablePassThrough)\n",
    "    C -->|context=извлеченные документы| E(PromptTemplate)\n",
    "    D -->|question=Djghjc| E\n",
    "    E -->|PromptValue| F(ChatModel)    \n",
    "    F -->|ChatMessage| G(StrOutputParser)\n",
    "    G --> |String| H(Ответ модели)\n",
    "```\n",
    "\n",
    "## Смотрите также\n",
    "\n",
    "* [Получение авторизационных данных для физических лиц](https://developers.sber.ru/docs/ru/gigachat/individuals-quickstart#shag-1-sozdayte-proekt-giga-chat-api)\n",
    "* [Получение авторизационных данных для ИП и юридических лиц](https://developers.sber.ru/docs/ru/gigachat/legal-quickstart#shag-3-poluchite-avtorizatsionnye-dannye)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
