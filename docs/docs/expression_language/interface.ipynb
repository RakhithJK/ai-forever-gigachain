{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9acd2e",
   "metadata": {},
   "source": [
    "# Интерфейс\n",
    "\n",
    "Для упрощения создания собственных цепочек в LCEL релизован протокол [Runnable](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable).\n",
    "Протокол стандартизирует определение и вызов цепочек и поддерживается в большинстве компонентов.\n",
    "Стандартные методы интерфеса Runnable:\n",
    "\n",
    "* [`stream`](#stream) — потоковая передача частей ответа;\n",
    "* [`invoke`](#invoke) — вызов цепочки с входными данными;\n",
    "* [`batch`](#batch) — вызов цепочки со списком входных данных.\n",
    "\n",
    "Асинхронные версии стандартных методов:\n",
    "\n",
    "- [`astream`](#async-stream) — асинхронная поотковая передача частей ответа;\n",
    "- [`ainvoke`](#async-invoke) — асинхронный вызов цепочки с входными данными;\n",
    "- [`abatch`](#async-batch) — вызов цепочки со списком входных данных;\n",
    "- [`astream_log`](#async-stream-intermediate-steps) — наряду с потоковой передачей ответа передает промежуточные шаги по мере их возникновения;\n",
    "- [`astream_events`](#async-stream-events) — **beta** потоковая передача событий по мере их возникновения в цепочке. Метод доступен начиная с `langchain-core` версии 0.1.14.\n",
    "\n",
    "Типы входных и выходных данных отличаются в зависимости от компонента:\n",
    "\n",
    "| Компонент | Тип ввода | Тип вывода |\n",
    "| --- | --- | --- |\n",
    "| Prompt | Словарь | PromptValue |\n",
    "| ChatModel | Одиночная строка, список сообщений чата или PromptValue | ChatMessage |\n",
    "| LLM | Одиночная строка, список сообщений чата или PromptValue | Строка |\n",
    "| OutputParser | Вывод LLM или ChatModel | Зависит от парсера |\n",
    "| Retriever | Одиночная строка | Список документов |\n",
    "| Tool | Одиночная строка или словарь, в зависимости от инструмента | Зависит от инструмента |\n",
    "\n",
    "\n",
    "Все экземпляры Runnable предоставляют доступ к схемам входных и выходных данных для проверки ввода и вывода:\n",
    "\n",
    "* [`input_schema`](#input-schema) — модель Pydantic, сгенерированная на основе структуры Runnable;\n",
    "* [`output_schema`](#output-schema) — модель Pydantic, сгенерированная на основе структуры Runnable.\n",
    "\n",
    "Давайте рассмотрим эти методы. Для этого мы создадим очень простой шаблон промпта + модель чата.\n",
    "\n",
    "Рассмотрим эти методы на примере работы простой цепочки, которая состоит из шаблона промпта и модели (*PromptTemplate + ChatModel*»)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57768739",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  gigachain gigachat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466b65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.gigachat import GigaChat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = GigaChat(credentials=\"<атворизационные_данные>\", verify_ssl_certs=False)\n",
    "prompt = ChatPromptTemplate.from_template(\"расскажи шутку про {topic}\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cccdf0b-2d89-4f74-9530-bf499610e9a5",
   "metadata": {},
   "source": [
    "## Схема входных данных\n",
    "\n",
    "Описание входных данных, которые принимает экземпляр Runnable.\n",
    "Представляет собой модель Pydantic, которая динамически генерируется на основе структуры экземпляра Runnable.\n",
    "Чтобы получить представление данных в формате JSONSchema, вызовите метод `.schema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e146d4-60da-40a2-9026-b5dfee106a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Схема входных данных цепочки соответствует схеме ее первой части — промпта.\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad130546-4c14-4f6c-95af-c56ea19b12ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d34744-d6db-4fdf-a0d6-261522b7f251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'GigaChatInput',\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/definitions/StringPromptValue'},\n",
       "  {'$ref': '#/definitions/ChatPromptValueConcrete'},\n",
       "  {'type': 'array',\n",
       "   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "     {'$ref': '#/definitions/HumanMessage'},\n",
       "     {'$ref': '#/definitions/ChatMessage'},\n",
       "     {'$ref': '#/definitions/SystemMessage'},\n",
       "     {'$ref': '#/definitions/FunctionMessage'},\n",
       "     {'$ref': '#/definitions/ToolMessage'}]}}],\n",
       " 'definitions': {'StringPromptValue': {'title': 'StringPromptValue',\n",
       "   'description': 'String prompt value.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'Message from an AI.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'Message from a human.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'Message for passing the result of executing a function back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id']},\n",
       "  'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',\n",
       "   'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'type': 'object',\n",
       "   'properties': {'messages': {'title': 'Messages',\n",
       "     'type': 'array',\n",
       "     'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "       {'$ref': '#/definitions/HumanMessage'},\n",
       "       {'$ref': '#/definitions/ChatMessage'},\n",
       "       {'$ref': '#/definitions/SystemMessage'},\n",
       "       {'$ref': '#/definitions/FunctionMessage'},\n",
       "       {'$ref': '#/definitions/ToolMessage'}]}},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages']}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059a5dc-d544-4add-85bd-78a3f2b78b9a",
   "metadata": {},
   "source": [
    "## Схема выходных данных\n",
    "\n",
    "Описание выходных данных, которые возвращает экземпляр Runnable.\n",
    "Представляет собой модель Pydantic, которая динамически генерируется на основе структуры экземпляра Runnable.\n",
    "Чтобы получить представление данных в формате JSONSchema, вызовите метод `.schema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e41fd3-77d8-4911-af6a-d4d3aad5f77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'GigaChatOutput',\n",
       " 'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "  {'$ref': '#/definitions/HumanMessage'},\n",
       "  {'$ref': '#/definitions/ChatMessage'},\n",
       "  {'$ref': '#/definitions/SystemMessage'},\n",
       "  {'$ref': '#/definitions/FunctionMessage'},\n",
       "  {'$ref': '#/definitions/ToolMessage'}],\n",
       " 'definitions': {'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'Message from an AI.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'Message from a human.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'Message for passing the result of executing a function back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id']}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Схема выходных данных цепочки — это схема вывода ее последней части. В данном случае это модель, которая возвращает сообщение чата.\n",
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2b2b2",
   "metadata": {},
   "source": [
    "## Потоковая передача"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bea9639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видны облака из дыма и пара, \n",
      "Идут они мимо, идут туда, \n",
      "Где небо с землёю цветут чередой, \n",
      "Идут они к югу, идут на восток, \n",
      "На север идут и на запад идут, \n",
      "И каждый идёт по своей кривой."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"облака\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1c782",
   "metadata": {},
   "source": [
    "## Вызов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "470e483f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Висит на заборе, колышется… Что это? — Это не облако, это — бельё!', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=27, total_tokens=45), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"облака\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0c279",
   "metadata": {},
   "source": [
    "## Пакетная обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9685de67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Висит на заборе, колышется ветром,\\nРыбак на него мочится.\\nВот такое у нас сегодня облако.', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=35, total_tokens=53), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='Вижу, тучи собрались, значит, скоро будет дождь.', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=16, total_tokens=34), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"облака\"}, {\"topic\": \"тучи\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434ab15",
   "metadata": {},
   "source": [
    "Используйте параметр `max_concurrency`, чтобы задать количество одновременных запросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a08522f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Висит на заборе, колышется ветром,\\nРыбак на него мочится.\\nВот такое у нас сегодня облако.', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=35, total_tokens=53), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='Вижу, вижу, собираются тучи, \\nТолько гром не грянул бы!\\nНадо выпить пропускную рюмку,\\nЧтоб её перебить!', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=41, total_tokens=59), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"облака\"}, {\"topic\": \"тучи\"}], config={\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960cbfe",
   "metadata": {},
   "source": [
    "## Асинхронная потоковая передача"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea35eee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Облако с ложкой стоит над тарелкой супа и приговаривает: «Облака-облака, а я вот ем!»"
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"облака\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb3324",
   "metadata": {},
   "source": [
    "## Асинхронный вызов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef8c9b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Висит на заборе, колышется… Что это? Это не облако, это мысль.', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=23, total_tokens=41), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.ainvoke({\"topic\": \"облака\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da288d5",
   "metadata": {},
   "source": [
    "## Асинхронная пакетная обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eba2a103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Облако с ложкой стоит над тарелкой супа и приговаривает: «Облака-облака, а я вот ем!»', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=35, total_tokens=53), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.abatch([{\"topic\": \"облака\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d365e5-dc14-4bb7-aa6a-7762c3af16a4",
   "metadata": {},
   "source": [
    "## Асинхронная потоковая передача событий (beta)\n",
    "\n",
    "Потоковая передача событий находится на стадии бета-версии.\n",
    "\n",
    "Для корректной работы с API `astream_events`:\n",
    "\n",
    "* Используйте `async` по всему коду (включая асинхронные инструменты и т. д.).\n",
    "* При определении собственных функций и Runnable-интерфейсов передавайте обратные вызовы.\n",
    "* При использовании Runnable-интерфейсов без LCEL, убедитесь, что при работе с LLM вы вызываете `.astream()`, а не `.ainvoke`, чтобы принудительно включать потоковую пердачу токенов.\n",
    "\n",
    "### Справка по событиям\n",
    "\n",
    "В таблице представлены некоторые события, которые могут сгенерировать различные Runnable-объекты.\n",
    "Примеры некоторых их Runnable-объектов приведены после таблицы.\n",
    "\n",
    "При потоковой передаче входных данных для Runnable-объекта, данные не будут доступны до завершения всего потока. Такое поведение приводит к тому, что входные данные доступны на соответствующем событии `end`, а не на событии `start`.\n",
    "\n",
    "\n",
    "| Событие                  | Имя             | Фрагмент                        | Ввод                                         | Вывод                                          |\n",
    "|--------------------------|-----------------|---------------------------------|----------------------------------------------|------------------------------------------------|\n",
    "| on_chat_model_start      | [имя модели]    |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                |\n",
    "| on_chat_model_stream     | [имя модели]    | AIMessageChunk(content=\"hello\") |                                              |                                                |\n",
    "| on_chat_model_end        | [имя модели]    |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...}   |\n",
    "| on_llm_start             | [имя модели]    |                                 | {'input': 'hello'}                           |                                                |\n",
    "| on_llm_stream            | [имя модели]    | 'Привет'                        |                                              |                                                |\n",
    "| on_llm_end               | [имя модели]    |                                 | 'Hello human!'                           |\n",
    "| on_chain_start           | format_docs     |                                 |                                              |                                                |\n",
    "| on_chain_stream          | format_docs     | \"привет мир!, прощай мир!\"      |                                              |                                                |\n",
    "| on_chain_end             | format_docs     |                                 | [Document(...)]                              | \"hello world!, goodbye world!\"                 |\n",
    "| on_tool_start            | some_tool       |                                 | {\"x\": 1, \"y\": \"2\"}                           |                                                |\n",
    "| on_tool_stream           | some_tool       | {\"x\": 1, \"y\": \"2\"}              |                                              |                                                |\n",
    "| on_tool_end              | some_tool       |                                 |                                              | {\"x\": 1, \"y\": \"2\"}                             |\n",
    "| on_retriever_start       | [имя ретривера] |                                 | {\"query\": \"hello\"}                           |                                                |\n",
    "| on_retriever_chunk       | [имя ретривера] | {документы: [...]}              |                                              |                                                |\n",
    "| on_retriever_end         | [имя ретривера] |                                 | {\"query\": \"hello\"}                           | {documents: [...]}                             |\n",
    "| on_prompt_start          | [имя шаблона]   |                                 | {\"question\": \"hello\"}                         |                                                |\n",
    "| on_prompt_end            | [имя шаблона]   |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(сообщения: [SystemMessage, ...])|\n",
    "\n",
    "Примеры объектов, связанные с событиями, представленными в таблице:\n",
    "\n",
    "`format_docs`:\n",
    "\n",
    "```python\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    '''Format the docs.'''\n",
    "    return \", \".join([doc.page_content for doc in docs])\n",
    "\n",
    "format_docs = RunnableLambda(format_docs)\n",
    "```\n",
    "\n",
    "`some_tool`:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def some_tool(x: int, y: str) -> dict:\n",
    "    '''Some_tool.'''\n",
    "    return {\"x\": x, \"y\": y}\n",
    "```\n",
    "\n",
    "`prompt`:\n",
    "\n",
    "```python\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
    ").with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108cf792-a372-4626-bbef-9d7be23dde33",
   "metadata": {},
   "source": [
    "Определим новую цепочку, чтобы более наглядно показать работу интерфейса `astream_events`, а затем и `astream_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92eeb4da-0aae-457b-bd8f-8c35a024d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Отвечай на вопрос только на основе контекста:\n",
    "{context}\n",
    "\n",
    "Вопрос: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"Василий работал в Сбербанке\"],\n",
    "    embedding=GigaChatEmbeddings(\n",
    "        credentials=\"<авторизационные_данные>\", verify_ssl_certs=False\n",
    "    ),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model.with_config(run_name=\"my_llm\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1167e8f2-cab7-45b4-8922-7518b58a7d8d",
   "metadata": {},
   "source": [
    "Используем `astream_events`, чтобы получить события от ретривера и LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742d723-5b00-4a44-961e-dd4a3ec6d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in retrieval_chain.astream_events(\n",
    "    \"где работал Василий?\", version=\"v1\", include_names=[\"Docs\", \"my_llm\"]\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"|\")\n",
    "    elif kind in {\"on_chat_model_start\"}:\n",
    "        print()\n",
    "        print(\"Потоковая передача от LLM:\")\n",
    "    elif kind in {\"on_chat_model_end\"}:\n",
    "        print()\n",
    "        print(\"Завершение потоковой передачи от LLM.\")\n",
    "    elif kind == \"on_retriever_end\":\n",
    "        print(\"--\")\n",
    "        print(\"Полученные документы:\")\n",
    "        print(event[\"data\"][\"output\"][\"documents\"])\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Ended tool: {event['name']}\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cef104",
   "metadata": {},
   "source": [
    "## Асинхронная потоковая передача промежуточных шагов\n",
    "\n",
    "Во всех Runnable-объектах реализован метод `.astream_log()`, который используется для потоковой передачи всех или части промежуточных шагов вашей цепочки по мере их возникновения.\n",
    "\n",
    "Метод можно использовать для отображения процесса выполнения пользователю, обработки промежуточных результатов или для отладки цепочки.\n",
    "\n",
    "Вы можете использовать потоковую передачу всех шагов (по умолчанию) или фильтровать шаги по имени, тегам или метаданным.\n",
    "\n",
    "Метод возвращает [операции JSONPatch](https://jsonpatch.com), которые при применении в том же порядке, в котором они поступили, позволяют собрать RunState.\n",
    "\n",
    "```python\n",
    "class LogEntry(TypedDict):\n",
    "    id: str\n",
    "    \"\"\"ID of the sub-run.\"\"\"\n",
    "    name: str\n",
    "    \"\"\"Name of the object being run.\"\"\"\n",
    "    type: str\n",
    "    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"\n",
    "    tags: List[str]\n",
    "    \"\"\"List of tags for the run.\"\"\"\n",
    "    metadata: Dict[str, Any]\n",
    "    \"\"\"Key-value pairs of metadata for the run.\"\"\"\n",
    "    start_time: str\n",
    "    \"\"\"ISO-8601 timestamp of when the run started.\"\"\"\n",
    "\n",
    "    streamed_output_str: List[str]\n",
    "    \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"\n",
    "    final_output: Optional[Any]\n",
    "    \"\"\"Final output of this run.\n",
    "    Only available after the run has finished successfully.\"\"\"\n",
    "    end_time: Optional[str]\n",
    "    \"\"\"ISO-8601 timestamp of when the run ended.\n",
    "    Only available after the run has finished.\"\"\"\n",
    "\n",
    "\n",
    "class RunState(TypedDict):\n",
    "    id: str\n",
    "    \"\"\"ID of the run.\"\"\"\n",
    "    streamed_output: List[Any]\n",
    "    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"\n",
    "    final_output: Optional[Any]\n",
    "    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.\n",
    "    Only available after the run has finished successfully.\"\"\"\n",
    "\n",
    "    logs: Dict[str, LogEntry]\n",
    "    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will\n",
    "    contain only the runs that matched the filters.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146a5df-25be-4fa2-a7e4-df8ebe55a35e",
   "metadata": {},
   "source": [
    "### Потоковая передача фрагментов JSONPatch\n",
    "\n",
    "Вы можете использовать потоковую передачю `JSONPatch` через HTTP-сервер, после чего применяя операции на клиенте восстанавливать RunState.\n",
    "Используйте [GigaServe](https://github.com/ai-forever/gigaserve) для создания веб-сервера на основе любого Runnable-объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21c9019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': 'd8997bd2-c0c5-4818-ac75-7001754a656a',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'c780af42-a72e-472a-bef6-cac749136303',\n",
      "            'metadata': {},\n",
      "            'name': 'Docs',\n",
      "            'start_time': '2024-03-28T12:09:01.497+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['map:key:context', 'FAISS', 'GigaChatEmbeddings'],\n",
      "            'type': 'retriever'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs/final_output',\n",
      "  'value': {'documents': [Document(page_content='Василий работал в Сбербанке')]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Docs/end_time',\n",
      "  'value': '2024-03-28T12:09:02.180+00:00'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/streamed_output/-',\n",
      "  'value': 'Василий работал в Сбербанке.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Василий работал в Сбербанке.'})\n"
     ]
    }
   ],
   "source": [
    "async for chunk in retrieval_chain.astream_log(\n",
    "    \"где работал Василий?\", include_names=[\"Docs\"]\n",
    "):\n",
    "    print(\"-\" * 40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19570f36-7126-4fe2-b209-0cc6178b4582",
   "metadata": {},
   "source": [
    "### Потоковая передача инкрементов RunState\n",
    "\n",
    "Для получения инкрементов `RunState` достаточно передать аргумент `diff=False`.\n",
    "Чем больше будет повторяющихся частей, тем более объемным будет вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c26b731-b4eb-4967-a42a-dec813249ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': None,\n",
      " 'id': '730fdb7b-e372-4e1b-a72a-d46179877ef1',\n",
      " 'logs': {},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': [],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': None,\n",
      " 'id': '730fdb7b-e372-4e1b-a72a-d46179877ef1',\n",
      " 'logs': {'Docs': {'end_time': None,\n",
      "                   'final_output': None,\n",
      "                   'id': 'bd46efbf-b646-4a7b-914a-21222832bc9d',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-03-28T12:09:19.793+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'GigaChatEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': [],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': None,\n",
      " 'id': '730fdb7b-e372-4e1b-a72a-d46179877ef1',\n",
      " 'logs': {'Docs': {'end_time': '2024-03-28T12:09:20.409+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='Василий работал в Сбербанке')]},\n",
      "                   'id': 'bd46efbf-b646-4a7b-914a-21222832bc9d',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-03-28T12:09:19.793+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'GigaChatEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': [],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Василий работал в Сбербанке.',\n",
      " 'id': '730fdb7b-e372-4e1b-a72a-d46179877ef1',\n",
      " 'logs': {'Docs': {'end_time': '2024-03-28T12:09:20.409+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='Василий работал в Сбербанке')]},\n",
      "                   'id': 'bd46efbf-b646-4a7b-914a-21222832bc9d',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-03-28T12:09:19.793+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'GigaChatEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['Василий работал в Сбербанке.'],\n",
      " 'type': 'chain'})\n"
     ]
    }
   ],
   "source": [
    "async for chunk in retrieval_chain.astream_log(\n",
    "    \"где работал Василий?\", include_names=[\"Docs\"], diff=False\n",
    "):\n",
    "    print(\"-\" * 70)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006f1aa",
   "metadata": {},
   "source": [
    "## Параллелизм\n",
    "\n",
    "Рассмотрим поддержку параллелизма в LCEL.\n",
    "Так, при использовании экземпляра `RunnableParallel`, который обычно записывается в формате словаря, каждый элемент выполняется праллельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a1c409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain1 = ChatPromptTemplate.from_template(\"расскажи шутку про {topic}\") | model\n",
    "chain2 = (\n",
    "    ChatPromptTemplate.from_template(\"напиши короткий стих на 2 строчки про {topic}\")\n",
    "    | model\n",
    ")\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08044c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.92 ms, sys: 2.76 ms, total: 11.7 ms\n",
      "Wall time: 1.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Вот какая шутка у меня получилась: \\n\\n— Тучка, тучка, а мне не до дождя!..', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=29, total_tokens=47), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain1.invoke({\"topic\": \"облако\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22c56804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.52 ms, sys: 2.48 ms, total: 11 ms\n",
      "Wall time: 1.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Вот такое стихотворение у меня получилось:\\n\\nПлывёт облако по небу,\\nА я лежать бы мог тут целый век.', response_metadata={'token_usage': Usage(prompt_tokens=24, completion_tokens=32, total_tokens=56), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain2.invoke({\"topic\": \"облако\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fff4cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 ms, sys: 5.06 ms, total: 33.3 ms\n",
      "Wall time: 4.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content='Вот какая шутка у меня получилась: \\n\\n— Тучка, тучка, а почему у тебя такие большие ноги?\\n\\n— А это чтобы по воде ходить!\\n\\n— А почему у тебя такие большие руки?\\n\\n— Это чтобы людей обнимать!\\n\\n— А почему у тебя такой большой нос?\\n\\n— А это чтобы в темноте с другими тучами не столкнуться!\\n\\n— А почему у тебя такое лицо бессмысленное?\\n\\n— Ну, это я ещё не развернулась…', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=115, total_tokens=133), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'}),\n",
       " 'poem': AIMessage(content='Вот такое стихотворение у меня получилось:\\n\\nПлывёт облако по небу,\\nА я лежать бы мог тут целый век.', response_metadata={'token_usage': Usage(prompt_tokens=24, completion_tokens=32, total_tokens=56), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined.invoke({\"topic\": \"облако\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80164216-0abd-439b-8407-409539e104b6",
   "metadata": {},
   "source": [
    "### Параллелизм при пакетной обработке\n",
    "\n",
    "Параллелизм можно использовать при работе с различными экземплярами Runnsble.\n",
    "Например, при пакетной обработке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f67d2268-c766-441b-8d64-57b8219ccb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 ms, sys: 5.31 ms, total: 27.7 ms\n",
      "Wall time: 1.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Облако на облаке, белый барашек,\\nУкололся шипом — и завял цветочек!', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=26, total_tokens=44), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='Вижу, тучи собрались, сейчас что-то будет… Либо салют, либо дождь.', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=24, total_tokens=42), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain1.batch([{\"topic\": \"облака\"}, {\"topic\": \"тучи\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83c8d511-9563-403e-9c06-cae986cf5dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 ms, sys: 4.51 ms, total: 22.1 ms\n",
      "Wall time: 1.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Облака плывут по небу,\\nОблака летят на юг.', response_metadata={'token_usage': Usage(prompt_tokens=24, completion_tokens=20, total_tokens=44), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'}),\n",
       " AIMessage(content='Тучи, как люди, порой собираются вместе,\\nТучи, как люди, порой плачут, кричат и смеются.', response_metadata={'token_usage': Usage(prompt_tokens=24, completion_tokens=31, total_tokens=55), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain2.batch([{\"topic\": \"облака\"}, {\"topic\": \"тучи\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07a81230-8db8-4b96-bdcb-99ae1d171f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.2 ms, sys: 10.1 ms, total: 57.3 ms\n",
      "Wall time: 2.49 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'joke': AIMessage(content='Висит на заборе, колышется ветром,\\nРыбак из деревни глядит на неё весь день:\\n«Скорей бы выросла!» — думает с надеждой.\\nИ так тридцать лет… Ну и что тут ещё сказать?', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=60, total_tokens=78), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'}),\n",
       "  'poem': AIMessage(content='Облака плывут по небу,\\nОблака, куда же вы?', response_metadata={'token_usage': Usage(prompt_tokens=24, completion_tokens=19, total_tokens=43), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})},\n",
       " {'joke': AIMessage(content='Вижу, тучи собираются, значит, будет дождик.', response_metadata={'token_usage': Usage(prompt_tokens=18, completion_tokens=16, total_tokens=34), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'}),\n",
       "  'poem': AIMessage(content='Тучи, словно камни, висят над головой,\\nТучи не дают нам увидеть небо голубое.', response_metadata={'token_usage': Usage(prompt_tokens=24, completion_tokens=25, total_tokens=49), 'model_name': 'GigaChat-Pro-preview:2.2.25.3', 'finish_reason': 'stop'})}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined.batch([{\"topic\": \"облака\"}, {\"topic\": \"тучи\"}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
