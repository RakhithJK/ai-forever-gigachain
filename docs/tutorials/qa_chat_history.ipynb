{
   "cells": [
      {
         "cell_type": "raw",
         "id": "023635f2-71cf-43f2-a2e2-a7b4ced30a74",
         "metadata": {},
         "source": [
            "---\n",
            "sidebar_position: 2\n",
            "---"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604",
         "metadata": {},
         "source": [
            "# Создание разговорного приложения с RAG\n",
            "\n",
            "Во многих вопросно-ответных приложениях пользователям нужна возможность вести разговор с сохранением контекста.\n",
            "Для этого в приложении должна быть реализация памяти о прошлых вопросах и ответах, а также некоторая логика для использования их в текущих рассуждениях.\n",
            "\n",
            "Информация в разделе посвящена добавлению логики для использования сообщений, полученных в течение разговора.\n",
            "Подробнее об управлении историей чата — в разделе [Работа с историей сообщений](/docs/how_to/message_history).\n",
            "\n",
            "В разделе рассмотрены два подхода:\n",
            "\n",
            "1. Цепочки, в которых приложение всегда выполняет этап извлечения данных;\n",
            "2. Агенты, в которых LLM самостоятельно принимает решение о выполнении этапа извлечения (или нескольких шагов) и способе его выполнения.\n",
            "\n",
            "В качестве внешнего источника знаний использован тот же блог-пост [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) Лилиан Венг из раздела [Создание RAG-приложения](/docs/tutorials/rag)."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "487d8d79-5ee9-4aa4-9fdf-cd5f4303e099",
         "metadata": {},
         "source": [
            "## Подготовка к работе\n",
            "\n",
            "### Установка зависимостей\n",
            "\n",
            "В этом руководстве используются эмбеддинги OpenAI и векторное хранилище Chroma, но все, что показано здесь, работает с любыми [моделями эмбеддингов](/docs/concepts#embedding-models), [векторными хранилищами](/docs/concepts#vectorstores) или [ретриверами](/docs/concepts#retrievers).\n",
            "\n",
            "Вам понадобятся следующие пакеты:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "ede7fdc0-ef31-483d-bd67-32e4b5c5d527",
         "metadata": {},
         "outputs": [],
         "source": [
            "%%capture --no-stderr\n",
            "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma bs4"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "51ef48de-70b6-4f43-8e0b-ab9b84c9c02a",
         "metadata": {},
         "source": [
            "Установите переменную окружения `OPENAI_API_KEY`.\n",
            "Ее можно установить напрямую или загрузить из файла `.env`:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "143787ca-d8e6-4dc9-8281-4374f4d71720",
         "metadata": {},
         "outputs": [],
         "source": [
            "import getpass\n",
            "import os\n",
            "\n",
            "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
            "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
            "\n",
            "# import dotenv\n",
            "\n",
            "# dotenv.load_dotenv()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "1665e740-ce01-4f09-b9ed-516db0bd326f",
         "metadata": {},
         "source": [
            "<!--\n",
            "### LangSmith\n",
            "\n",
            "Многие приложения, которые вы создаете с LangChain, будут содержать несколько шагов с многократными вызовами LLM. По мере того как эти приложения становятся все более сложными, становится важно иметь возможность проверять, что именно происходит внутри вашей цепочки или агента. Лучший способ сделать это с помощью [LangSmith](https://smith.langchain.com).\n",
            "\n",
            "Обратите внимание, что LangSmith не обязателен, но полезен. Если вы хотите использовать LangSmith, после регистрации по указанной выше ссылке, убедитесь, что установили переменные окружения для начала логирования трассировок:\n",
            "\n",
            "```python\n",
            "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
            "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
            "```\n",
            "-->"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "fa6ba684-26cf-4860-904e-a4d51380c134",
         "metadata": {},
         "source": [
            "## Цепочки {#chains}\n",
            "\n",
            "В первую очередь рассмотрим приложение, разработанное в разделе [Создание RAG-приложения](/docs/tutorials/rag)."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "646840fb-5212-48ea-8bc7-ec7be5ec727e",
         "metadata": {},
         "source": [
            "```{=mdx}\n",
            "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
            "\n",
            "<ChatModelTabs customVarName=\"llm\" />\n",
            "```"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "id": "cb58f273-2111-4a9b-8932-9b64c95030c8",
         "metadata": {},
         "outputs": [],
         "source": [
            "# | output: false\n",
            "# | echo: false\n",
            "\n",
            "from langchain_openai import ChatOpenAI\n",
            "\n",
            "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "820244ae-74b4-4593-b392-822979dd91b8",
         "metadata": {},
         "outputs": [],
         "source": [
            "import bs4\n",
            "from langchain import hub\n",
            "from langchain.chains import create_retrieval_chain\n",
            "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
            "from langchain_chroma import Chroma\n",
            "from langchain_community.document_loaders import WebBaseLoader\n",
            "from langchain_core.prompts import ChatPromptTemplate\n",
            "from langchain_openai import OpenAIEmbeddings\n",
            "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
            "\n",
            "# 1. Загрузите, разделите на части и проиндексируйте содержимого блога, чтобы создать ретривер.\n",
            "loader = WebBaseLoader(\n",
            "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
            "    bs_kwargs=dict(\n",
            "        parse_only=bs4.SoupStrainer(\n",
            "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
            "        )\n",
            "    ),\n",
            ")\n",
            "docs = loader.load()\n",
            "\n",
            "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
            "splits = text_splitter.split_documents(docs)\n",
            "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
            "retriever = vectorstore.as_retriever()\n",
            "\n",
            "\n",
            "# 2. Включите ретривер в вопросно-ответную цепочку.\n",
            "system_prompt = (\n",
            "    \"You are an assistant for question-answering tasks. \"\n",
            "    \"Use the following pieces of retrieved context to answer \"\n",
            "    \"the question. If you don't know the answer, say that you \"\n",
            "    \"don't know. Use three sentences maximum and keep the \"\n",
            "    \"answer concise.\"\n",
            "    \"\\n\\n\"\n",
            "    \"{context}\"\n",
            ")\n",
            "\n",
            "prompt = ChatPromptTemplate.from_messages(\n",
            "    [\n",
            "        (\"system\", system_prompt),\n",
            "        (\"human\", \"{input}\"),\n",
            "    ]\n",
            ")\n",
            "\n",
            "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
            "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "bf55faaf-0d17-4b74-925d-c478b555f7b2",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "\"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model. This process helps in guiding the agent through the various subgoals required to achieve the overall task efficiently. Different techniques like Chain of Thought and Tree of Thoughts can be used to decompose tasks into step-by-step processes, enhancing performance and understanding of the model's thinking process.\""
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
            "response[\"answer\"]"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "187404c7-db47-49c5-be29-9ecb96dc9afa",
         "metadata": {},
         "source": [
            "Обратите внимание, что в примере использованы встроенные конструкторы цепочек `create_stuff_documents_chain` и `create_retrieval_chain`.\n",
            "Таким образом основными составляющими итогового решения являются:\n",
            "\n",
            "1. Ретривер.\n",
            "2. Промпт.\n",
            "3. LLM.\n",
            "\n",
            "Это упростит процесс включения истории разговора.\n",
            "\n",
            "### Добавление истории разговора\n",
            "\n",
            "Разработанная цепочка использует сам входной запрос для извлечения релевантного контекста.\n",
            "Но в разговорной обстановке запрос пользователя может требовать дополнительного контекста беседы для понимания.\n",
            "Например, представьте такой диалог:\n",
            "\n",
            "> Human: \"What is Task Decomposition?\"\n",
            ">\n",
            "> AI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\n",
            ">\n",
            "> Human: \"What are common ways of doing it?\"\n",
            "\n",
            "Для ответа на второй вопрос, система должна понимать, что \"it\" относится к \"Task Decomposition\".\n",
            "\n",
            "Чтобы добиться этого в текущем приложении нужно изменить две вещи:\n",
            "\n",
            "1. Доработать промпт, чтобы он мог использовать сообщения, полученные в течение разговора, в качестве входных данных.\n",
            "2. Добавить подцепочку, которая берет последний вопрос пользователя и перефразирует его с учетом истории разговора. Создание такой цепочки можно рассматривать как разработку нового ретривера, который учитывает историю сообщений. То есть, если в первой итерации приложение работало так:\n",
            "   - `запрос` -> `ретривер`\n",
            "     \n",
            "     То после доработки оно будет работать так:\n",
            "   \n",
            "   - `(запрос, история разговора)` -> `LLM` -> `переформулированный запрос` -> `ретривер`"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "776ae958-cbdc-4471-8669-c6087436f0b5",
         "metadata": {},
         "source": [
            "#### Контекстуализация вопроса\n",
            "\n",
            "Сначала вам нужно определить подцепочку, которая:\n",
            "\n",
            "1. Берет сообщения, полученные в течение разговора, и последний вопрос пользователя.\n",
            "2. Перефразирует вопрос, если он ссылается на какую-либо информацию в истории разговора.\n",
            "\n",
            "Для этого используйте промпт, который включает переменную `MessagesPlaceholder` под именем `chat_history`.\n",
            "Таким образом вы сможете передавать список сообщений в промпт, используя ключ `chat_history`.\n",
            "Эти сообщения будут вставлены после системного сообщения и перед сообщением пользователя, содержащим последний вопрос.\n",
            "\n",
            "Для решения этой задачи используйте вспомогательную функцию [create_history_aware_retriever](https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html)\n",
            "Она обрабатывает случаи, когда поле `chat_history` не содержит данных, и в противном случае применяет цепочку `prompt | llm | StrOutputParser() | retriever`.\n",
            "\n",
            "Функция `create_history_aware_retriever` создает цепочку, которая принимает ключи `input` и `chat_history` в качестве входных данных и имеет такую же схему вывода, как и ретривер."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "2b685428-8b82-4af1-be4f-7232c5d55b73",
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain.chains import create_history_aware_retriever\n",
            "from langchain_core.prompts import MessagesPlaceholder\n",
            "\n",
            "contextualize_q_system_prompt = (\n",
            "    \"Given a chat history and the latest user question \"\n",
            "    \"which might reference context in the chat history, \"\n",
            "    \"formulate a standalone question which can be understood \"\n",
            "    \"without the chat history. Do NOT answer the question, \"\n",
            "    \"just reformulate it if needed and otherwise return it as is.\"\n",
            ")\n",
            "\n",
            "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
            "    [\n",
            "        (\"system\", contextualize_q_system_prompt),\n",
            "        MessagesPlaceholder(\"chat_history\"),\n",
            "        (\"human\", \"{input}\"),\n",
            "    ]\n",
            ")\n",
            "history_aware_retriever = create_history_aware_retriever(\n",
            "    llm, retriever, contextualize_q_prompt\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "42a47168-4a1f-4e39-bd2d-d5b03609a243",
         "metadata": {},
         "source": [
            "Эта цепочка добавляет перефразированный входной запрос к ретриверу, чтобы при извлечении данных приложение учитывало контекст беседы.\n",
            "\n",
            "Теперь вы можете собрать итоговую вопросно-ответную цепочку.\n",
            "Для этого достаточно заменить ретривер на новый `history_aware_retriever`.\n",
            "\n",
            "Использовать функцию [create_stuff_documents_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html), чтобы создать цепочку`question_answer_chain` с входными ключами `context`, `chat_history` и `input`.\n",
            "Эта цепочка принимает извлеченный контекст вместе с историей разговора и запросом для генерации ответа.\n",
            "\n",
            "Создайте итоговую цепочку `rag_chain` с помощью функции [create_retrieval_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html).\n",
            "Эта цепочка последовательно применяет `history_aware_retriever` и `question_answer_chain`.\n",
            "При этом цепочка сохраняет промежуточные выходные данные, такие как извлеченный контекст.\n",
            "Она имеет входные ключи `input` и `chat_history`, и включает `input`, `chat_history`, `context` и `answer` в своем выводе."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "66f275f3-ddef-4678-b90d-ee64576878f9",
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain.chains import create_retrieval_chain\n",
            "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
            "\n",
            "qa_prompt = ChatPromptTemplate.from_messages(\n",
            "    [\n",
            "        (\"system\", system_prompt),\n",
            "        MessagesPlaceholder(\"chat_history\"),\n",
            "        (\"human\", \"{input}\"),\n",
            "    ]\n",
            ")\n",
            "\n",
            "\n",
            "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
            "\n",
            "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "1ba1ae56-7ecb-4563-b792-50a1a5042df3",
         "metadata": {},
         "source": [
            "Теперь попробуйте обратиться к приложению.\n",
            "Задайте вопрос, ответ на который требует дополнительного контекста.\n",
            "Поскольку цепочка включает входной параметр `\"chat_history\"`, историей разговора нужно управлять на стороне компонента, который вызывает цепочку.\n",
            "Для этого вы можете добавлять входные и выходные сообщения в список:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "0005810b-1b95-4666-a795-08d80e478b83",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Task decomposition can be achieved through various methods such as using techniques like Chain of Thought (CoT) or Tree of Thoughts to break down complex tasks into smaller steps. Common ways include prompting the model with simple instructions like \"Steps for XYZ\" or task-specific instructions like \"Write a story outline.\" Human inputs can also be used to guide the task decomposition process effectively.\n"
               ]
            }
         ],
         "source": [
            "from langchain_core.messages import AIMessage, HumanMessage\n",
            "\n",
            "chat_history = []\n",
            "\n",
            "question = \"What is Task Decomposition?\"\n",
            "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
            "chat_history.extend(\n",
            "    [\n",
            "        HumanMessage(content=question),\n",
            "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
            "    ]\n",
            ")\n",
            "\n",
            "second_question = \"What are common ways of doing it?\"\n",
            "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
            "\n",
            "print(ai_msg_2[\"answer\"])"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "53263a65-4de2-4dd8-9291-6a8169ab6f1d",
         "metadata": {},
         "source": [
            "<!--\n",
            ":::{.callout-tip}\n",
            "\n",
            "Посмотрите [трассировку LangSmith](https://smith.langchain.com/public/243301e4-4cc5-4e52-a6e7-8cfe9208398d/r) \n",
            "\n",
            ":::\n",
            "-->"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "53a662c2-f38b-45f9-95c4-66de15637614",
         "metadata": {},
         "source": [
            "#### Управление историей разговора с сохранением состояния\n",
            "\n",
            "Вы уже знаете как добавить логику приложения для использования истории разговора, но пока ее нужно обновлять вручную и передавать при каждом вызове.\n",
            "В реальном вопросно-ответном приложении вам понадобится как-то автоматизировать хранение, обновление и передачу истории разговора.\n",
            "\n",
            "Для этого можно использовать:\n",
            "\n",
            "- [BaseChatMessageHistory](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.memory). Класс, который хранит историю разговора.\n",
            "- [RunnableWithMessageHistory](/docs/how_to/message_history). Класс-обертка для цепочки LCEL и `BaseChatMessageHistory`, которая обрабатывает передачу истории разговора на вход и обновляет ее после каждого вызова.\n",
            "\n",
            "О том как использовать эти классы для создания цепочки диалога с сохранением состояния — в разделе [Работа с историей сообщений](/docs/how_to/message_history).\n",
            "\n",
            "Ниже приведен пример решения, при котором история разговора хранится в простом словаре.\n",
            "Для более надежного решения GigaChain предоставляет интеграции памяти с [Redis](/docs/integrations/memory/redis_chat_message_history/) и другими.\n",
            "\n",
            "Экземпляры самостоятельно `RunnableWithMessageHistory` управляют историей разговора.\n",
            "Они принимают конфигурацию с ключом (по умолчанию `\"session_id\"`), который указывает, что из истории разговора извлечь и добавить к входным данным, а что добавить к выходным данным той же истории разговоров.\n",
            "\n",
            "Пример:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "9c3fb176-8d6a-4dc7-8408-6a22c5f7cc72",
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_community.chat_message_histories import ChatMessageHistory\n",
            "from langchain_core.chat_history import BaseChatMessageHistory\n",
            "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
            "\n",
            "store = {}\n",
            "\n",
            "\n",
            "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
            "    if session_id not in store:\n",
            "        store[session_id] = ChatMessageHistory()\n",
            "    return store[session_id]\n",
            "\n",
            "\n",
            "conversational_rag_chain = RunnableWithMessageHistory(\n",
            "    rag_chain,\n",
            "    get_session_history,\n",
            "    input_messages_key=\"input\",\n",
            "    history_messages_key=\"chat_history\",\n",
            "    output_messages_key=\"answer\",\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "1046c92f-21b3-4214-907d-92878d8cba23",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable. Techniques like Chain of Thought (CoT) and Tree of Thoughts help models decompose hard tasks into multiple manageable subtasks. This process allows agents to plan ahead and tackle intricate tasks effectively.'"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "conversational_rag_chain.invoke(\n",
            "    {\"input\": \"What is Task Decomposition?\"},\n",
            "    config={\n",
            "        \"configurable\": {\"session_id\": \"abc123\"}\n",
            "    },  # constructs a key \"abc123\" in `store`.\n",
            ")[\"answer\"]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "0e89c75f-7ad7-4331-a2fe-57579eb8f840",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Task decomposition can be achieved through various methods such as using Language Model (LLM) with simple prompting, task-specific instructions tailored to the specific task at hand, or incorporating human inputs to break down the task into smaller components. These approaches help in guiding agents to think step by step and decompose complex tasks into more manageable subgoals.'"
                  ]
               },
               "execution_count": 13,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "conversational_rag_chain.invoke(\n",
            "    {\"input\": \"What are common ways of doing it?\"},\n",
            "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
            ")[\"answer\"]"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "3ab59258-84bc-4904-880e-2ebfebbca563",
         "metadata": {},
         "source": [
            "Историю разговора можно просмотреть в словаре `store`:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "7686b874-3a85-499f-82b5-28a85c4c768c",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "User: What is Task Decomposition?\n",
                  "\n",
                  "AI: Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable. Techniques like Chain of Thought (CoT) and Tree of Thoughts help models decompose hard tasks into multiple manageable subtasks. This process allows agents to plan ahead and tackle intricate tasks effectively.\n",
                  "\n",
                  "User: What are common ways of doing it?\n",
                  "\n",
                  "AI: Task decomposition can be achieved through various methods such as using Language Model (LLM) with simple prompting, task-specific instructions tailored to the specific task at hand, or incorporating human inputs to break down the task into smaller components. These approaches help in guiding agents to think step by step and decompose complex tasks into more manageable subgoals.\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "for message in store[\"abc123\"].messages:\n",
            "    if isinstance(message, AIMessage):\n",
            "        prefix = \"AI\"\n",
            "    else:\n",
            "        prefix = \"User\"\n",
            "\n",
            "    print(f\"{prefix}: {message.content}\\n\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0ab1ded4-76d9-453f-9b9b-db9a4560c737",
         "metadata": {},
         "source": [
            "### Итоговое решение\n",
            "\n",
            "Для удобства все шаги представлены в одной ячейке кода:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "id": "71c32048-1a41-465f-a9e2-c4affc332fd9",
         "metadata": {},
         "outputs": [],
         "source": [
            "import bs4\n",
            "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
            "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
            "from langchain_chroma import Chroma\n",
            "from langchain_community.chat_message_histories import ChatMessageHistory\n",
            "from langchain_community.document_loaders import WebBaseLoader\n",
            "from langchain_core.chat_history import BaseChatMessageHistory\n",
            "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
            "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
            "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
            "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
            "\n",
            "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
            "\n",
            "\n",
            "### Создание ретривера ###\n",
            "loader = WebBaseLoader(\n",
            "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
            "    bs_kwargs=dict(\n",
            "        parse_only=bs4.SoupStrainer(\n",
            "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
            "        )\n",
            "    ),\n",
            ")\n",
            "docs = loader.load()\n",
            "\n",
            "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
            "splits = text_splitter.split_documents(docs)\n",
            "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
            "retriever = vectorstore.as_retriever()\n",
            "\n",
            "\n",
            "### Контекстуализация вопроса ###\n",
            "contextualize_q_system_prompt = (\n",
            "    \"Given a chat history and the latest user question \"\n",
            "    \"which might reference context in the chat history, \"\n",
            "    \"formulate a standalone question which can be understood \"\n",
            "    \"without the chat history. Do NOT answer the question, \"\n",
            "    \"just reformulate it if needed and otherwise return it as is.\"\n",
            ")\n",
            "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
            "    [\n",
            "        (\"system\", contextualize_q_system_prompt),\n",
            "        MessagesPlaceholder(\"chat_history\"),\n",
            "        (\"human\", \"{input}\"),\n",
            "    ]\n",
            ")\n",
            "history_aware_retriever = create_history_aware_retriever(\n",
            "    llm, retriever, contextualize_q_prompt\n",
            ")\n",
            "\n",
            "\n",
            "### Ответ на вопрос ###\n",
            "system_prompt = (\n",
            "    \"You are an assistant for question-answering tasks. \"\n",
            "    \"Use the following pieces of retrieved context to answer \"\n",
            "    \"the question. If you don't know the answer, say that you \"\n",
            "    \"don't know. Use three sentences maximum and keep the \"\n",
            "    \"answer concise.\"\n",
            "    \"\\n\\n\"\n",
            "    \"{context}\"\n",
            ")\n",
            "qa_prompt = ChatPromptTemplate.from_messages(\n",
            "    [\n",
            "        (\"system\", system_prompt),\n",
            "        MessagesPlaceholder(\"chat_history\"),\n",
            "        (\"human\", \"{input}\"),\n",
            "    ]\n",
            ")\n",
            "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
            "\n",
            "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
            "\n",
            "\n",
            "### Управление историей разговора с сохранением состояния ###\n",
            "store = {}\n",
            "\n",
            "\n",
            "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
            "    if session_id not in store:\n",
            "        store[session_id] = ChatMessageHistory()\n",
            "    return store[session_id]\n",
            "\n",
            "\n",
            "conversational_rag_chain = RunnableWithMessageHistory(\n",
            "    rag_chain,\n",
            "    get_session_history,\n",
            "    input_messages_key=\"input\",\n",
            "    history_messages_key=\"chat_history\",\n",
            "    output_messages_key=\"answer\",\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "id": "6d0a7a73-d151-47d9-9e99-b4f3291c0322",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks to facilitate problem-solving. Different methods like Chain of Thought and Tree of Thoughts can be employed to decompose tasks effectively.'"
                  ]
               },
               "execution_count": 16,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "conversational_rag_chain.invoke(\n",
            "    {\"input\": \"What is Task Decomposition?\"},\n",
            "    config={\n",
            "        \"configurable\": {\"session_id\": \"abc123\"}\n",
            "    },  # создает ключ \"abc123\" в `store`.\n",
            ")[\"answer\"]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "id": "17021822-896a-4513-a17d-1d20b1c5381c",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Task decomposition can be achieved through various methods such as using prompting techniques like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", providing task-specific instructions like \"Write a story outline,\" or incorporating human inputs to break down complex tasks into smaller components. These approaches help in organizing thoughts and planning ahead for successful task completion.'"
                  ]
               },
               "execution_count": 17,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "conversational_rag_chain.invoke(\n",
            "    {\"input\": \"What are common ways of doing it?\"},\n",
            "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
            ")[\"answer\"]"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "861da8ed-d890-4fdc-a3bf-30433db61e0d",
         "metadata": {},
         "source": [
            "## Агенты {#agents}\n",
            "\n",
            "Агенты используют способность LLM к рассуждению для принятия решений в процессе выполнения.\n",
            "Использование агентов позволяет делегировать часть процесса извлечения данных.\n",
            "Хотя поведение агентов менее предсказуемо, чем у цепочек, они предлагают ряд преимуществ:\n",
            "\n",
            "- агенты самостоятельно генерируют ввод для ретривера. То есть вам не нужно явно добавлять контекстуализацию, как показано в примере выше;\n",
            "- Агенты могут выполнять несколько шагов извлечения данных для создания запроса или игнорировать этап извлечения, например, в ответ на общее приветствие пользователя.\n",
            "\n",
            "### Инструмент для извлечения\n",
            "\n",
            "Агенты могут использовать *инструменты* и управлять их выполнением.\n",
            "В примере ниже показано как преобразовать ретривер в инструмент GigaChain, который будет использовать агент:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "id": "809cc747-2135-40a2-8e73-e4556343ee64",
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain.tools.retriever import create_retriever_tool\n",
            "\n",
            "tool = create_retriever_tool(\n",
            "    retriever,\n",
            "    \"blog_post_retriever\",\n",
            "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
            ")\n",
            "tools = [tool]"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "07dcb968-ed9a-458a-85e1-528cd28c6965",
         "metadata": {},
         "source": [
            "Инструменты являются экземплярами [Runnable](/docs/concepts#langchain-expression-language) и реализуют стандартный интерфейс:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "id": "931c4fe3-c603-4efb-9b37-5f7cbbb1cbbd",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'"
                  ]
               },
               "execution_count": 19,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tool.invoke(\"task decomposition\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f77e0217-28be-4b8b-b4c4-9cc4ed5ec201",
         "metadata": {},
         "source": [
            "### Конструктор агента\n",
            "\n",
            "После определения инструментов и LLM вы можете создать агента.\n",
            "Для этого используйте [GigaGraph](/docs/concepts/#langgraph).\n",
            "В представленных примерах используется высокоуровневый интерфейс для создания агента.\n",
            "При этом преимущество GigaGraph в том, что высокоуровневый интерфейс поддерживается низкоуровневым API, который позволяет точно контролировать логику агента если нужно."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "id": "1726d151-4653-4c72-a187-a14840add526",
         "metadata": {},
         "outputs": [],
         "source": [
            "from langgraph.prebuilt import create_react_agent\n",
            "\n",
            "agent_executor = create_react_agent(llm, tools)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6d5152ca-1c3b-4f58-bb28-f31c0be7ba66",
         "metadata": {},
         "source": [
            "Попробуйте обратиться к агенту.\n",
            "\n",
            ":::note\n",
            "\n",
            "На данный момент агент не умеет работать с состояниями, вам все еще нужно добавить память.\n",
            "\n",
            ":::"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "id": "170403a2-c914-41db-85d8-a2c381da112d",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Error in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID 1a50f4da-34a7-44af-8cbb-c67c90c9619e, but expected {'tool'} run.\")\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_1ZkTWsLYIlKZ1uMyIQGUuyJx', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 68, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-dddbe2d2-2355-4ca5-9961-1ceb39d78cf9-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_1ZkTWsLYIlKZ1uMyIQGUuyJx'}])]}}\n",
                  "----\n",
                  "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', name='blog_post_retriever', tool_call_id='call_1ZkTWsLYIlKZ1uMyIQGUuyJx')]}}\n",
                  "----\n",
                  "{'agent': {'messages': [AIMessage(content='Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps in managing and solving difficult tasks by dividing them into more manageable components. One common method of task decomposition is the Chain of Thought (CoT) technique, where models are instructed to think step by step to decompose hard tasks into smaller steps. Another extension of CoT is the Tree of Thoughts, which explores multiple reasoning possibilities at each step and generates multiple thoughts per step, creating a tree structure. Task decomposition can be facilitated by using simple prompts, task-specific instructions, or human inputs.', response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 636, 'total_tokens': 755}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4a701854-97f2-4ec2-b6e1-73410911fa72-0')]}}\n",
                  "----\n"
               ]
            }
         ],
         "source": [
            "query = \"What is Task Decomposition?\"\n",
            "\n",
            "for s in agent_executor.stream(\n",
            "    {\"messages\": [HumanMessage(content=query)]},\n",
            "):\n",
            "    print(s)\n",
            "    print(\"----\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "1df703b1-aad6-48fb-b6fa-703e32ea88b9",
         "metadata": {},
         "source": [
            "В GigaGraph есть встроенная поддержка сохранения состояния, поэтому вам не нужно использовать ChatMessageHistory. \n",
            "Вместо этого вы можете передать контроллер состояния напрямую своему агенту GigaGraph."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "id": "04a3a664-3c3f-4cd1-9995-26662a52da7c",
         "metadata": {},
         "outputs": [],
         "source": [
            "from langgraph.checkpoint.sqlite import SqliteSaver\n",
            "\n",
            "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
            "\n",
            "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "02026f78-338e-4d18-9f05-131e1dd59197",
         "metadata": {},
         "source": [
            "Этого достаточно для создания разговорного агента с RAG.\n",
            "\n",
            "Посмотрите как будет работать агент.\n",
            "Заметьте, что если вы введете запрос, который не требует этапа извлечения данных, агент его не выполнит:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "id": "d6d70833-b958-4cd7-9e27-29c1c08bb1b8",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'agent': {'messages': [AIMessage(content='Hello Bob! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 67, 'total_tokens': 78}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-022806f0-eb26-4c87-9132-ed2fcc6c21ea-0')]}}\n",
                  "----\n"
               ]
            }
         ],
         "source": [
            "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
            "\n",
            "for s in agent_executor.stream(\n",
            "    {\"messages\": [HumanMessage(content=\"Hi! I'm bob\")]}, config=config\n",
            "):\n",
            "    print(s)\n",
            "    print(\"----\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "a7928865-3dd6-4d36-abc6-2a30de770d09",
         "metadata": {},
         "source": [
            "Если вы введете запрос, который требует этапа извлечения данных, агент сгенерирует входные данные для вызова инструмента:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "id": "e2c570ae-dd91-402c-8693-ae746de63b16",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_DdAAJJgGIQOZQgKVE4duDyML', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 91, 'total_tokens': 110}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-acc3c903-4f6f-48dd-8b36-f6f3b80d0856-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_DdAAJJgGIQOZQgKVE4duDyML'}])]}}\n",
                  "----\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Error in LangChainTracer.on_tool_end callback: TracerException(\"Found chain run at ID 9a7ba580-ec91-412d-9649-1b5cbf5ae7bc, but expected {'tool'} run.\")\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', name='blog_post_retriever', tool_call_id='call_DdAAJJgGIQOZQgKVE4duDyML')]}}\n",
                  "----\n"
               ]
            }
         ],
         "source": [
            "query = \"What is Task Decomposition?\"\n",
            "\n",
            "for s in agent_executor.stream(\n",
            "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
            "):\n",
            "    print(s)\n",
            "    print(\"----\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "26eaae33-3c4e-49fc-9fc6-db8967e25579",
         "metadata": {},
         "source": [
            "Выше агент, вместо того чтобы вставлять запрос дословно в инструмент, удалил ненужные слова, такие как \"what\" и \"is\".\n",
            "\n",
            "Этот же принцип позволяет агенту использовать контекст разговора, если нужно:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "570d8c68-136e-4ba5-969a-03ba195f6118",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KvoiamnLfGEzMeEMlV3u0TJ7', 'function': {'arguments': '{\"query\":\"common ways of task decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 930, 'total_tokens': 951}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-dd842071-6dbd-4b68-8657-892eaca58638-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'common ways of task decomposition'}, 'id': 'call_KvoiamnLfGEzMeEMlV3u0TJ7'}])]}}\n",
                  "----\n",
                  "{'action': {'messages': [ToolMessage(content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.', name='blog_post_retriever', id='c749bb8e-c8e0-4fa3-bc11-3e2e0651880b', tool_call_id='call_KvoiamnLfGEzMeEMlV3u0TJ7')]}}\n",
                  "----\n",
                  "{'agent': {'messages': [AIMessage(content='According to the blog post, common ways of task decomposition include:\\n\\n1. Using language models with simple prompting like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n2. Utilizing task-specific instructions, for example, using \"Write a story outline\" for writing a novel.\\n3. Involving human inputs in the task decomposition process.\\n\\nThese methods help in breaking down complex tasks into smaller and more manageable steps, facilitating better planning and execution of the overall task.', response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 1475, 'total_tokens': 1575}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-98b765b3-f1a6-4c9a-ad0f-2db7950b900f-0')]}}\n",
                  "----\n"
               ]
            }
         ],
         "source": [
            "query = \"What according to the blog post are common ways of doing it? redo the search\"\n",
            "\n",
            "for s in agent_executor.stream(\n",
            "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
            "):\n",
            "    print(s)\n",
            "    print(\"----\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "f2724616-c106-4e15-a61a-3077c535f692",
         "metadata": {},
         "source": [
            "Заметьте, что агент смог понять, что \"it\" в запросе относится к \"task decomposition\", и сгенерировал обоснованный поисковый запрос — в данном случае, \"common ways of task decomposition\"."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "1cf87847-23bb-4672-b41c-12ad9cf81ed4",
         "metadata": {},
         "source": [
            "### Итоговое решение\n",
            "\n",
            "Для удобства все шаги представлены в одной ячейке кода:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b1d2b4d4-e604-497d-873d-d345b808578e",
         "metadata": {},
         "outputs": [],
         "source": [
            "import bs4\n",
            "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
            "from langchain.tools.retriever import create_retriever_tool\n",
            "from langchain_chroma import Chroma\n",
            "from langchain_community.chat_message_histories import ChatMessageHistory\n",
            "from langchain_community.document_loaders import WebBaseLoader\n",
            "from langchain_core.chat_history import BaseChatMessageHistory\n",
            "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
            "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
            "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
            "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
            "from langgraph.checkpoint.sqlite import SqliteSaver\n",
            "from langgraph.prebuilt import create_react_agent\n",
            "\n",
            "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
            "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
            "\n",
            "\n",
            "### Создание ретривера ###\n",
            "loader = WebBaseLoader(\n",
            "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
            "    bs_kwargs=dict(\n",
            "        parse_only=bs4.SoupStrainer(\n",
            "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
            "        )\n",
            "    ),\n",
            ")\n",
            "docs = loader.load()\n",
            "\n",
            "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
            "splits = text_splitter.split_documents(docs)\n",
            "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
            "retriever = vectorstore.as_retriever()\n",
            "\n",
            "\n",
            "### Создание инструмента ретривера ###\n",
            "tool = create_retriever_tool(\n",
            "    retriever,\n",
            "    \"blog_post_retriever\",\n",
            "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
            ")\n",
            "tools = [tool]\n",
            "\n",
            "\n",
            "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "cd6bf4f4-74f4-419d-9e26-f0ed83cf05fa",
         "metadata": {},
         "source": [
            "## Смотрите также\n",
            "\n",
            "- Изучите различные типы ретриверов и методики извлечения в разделе [ретриверы](/docs/how_to/#retrievers).\n",
            "\n",
            "- Ознакомьтесь с подробным руководствам по работе с разговорной памятью в GigaChain в разделе [Работа с историей сообщений](/docs/how_to/message_history).\n",
            "\n",
            "- Ознакомьте подробнее с [агентами](/docs/tutorials/agents)."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.2"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
